{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE, f_regression\n",
    "from sklearn.linear_model import (LinearRegression, Ridge, Lasso, RandomizedLasso)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import (RandomForestRegressor, IsolationForest)\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import math\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "TRAIN_FILE_PATH = \"data/X_train.csv\"\n",
    "TARGET_FILE_PATH =  \"data/y_train.csv\"\n",
    "TEST_FILE_PATH = \"data/X_test.csv\"\n",
    "\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "\n",
    "\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define all function we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values with mean column values train and test set\n",
    "def fill_NaN(train, test):\n",
    "    print(\"Filling all nan values in the data with the mean\")\n",
    "    train_mean_values = train.mean()\n",
    "    train =  train.fillna(train_mean_values)\n",
    "    test = test.fillna(train_mean_values)\n",
    "    \n",
    "    return train,test\n",
    "\n",
    "def remove_outliers(X_train, Y_train):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    isoForest = IsolationForest(behaviour='new', max_samples=100, random_state=rng, contamination='auto')\n",
    "    outliers = isoForest.fit_predict(X_train)\n",
    "    print(\"Removed {} Outliers\".format((outliers < 0).sum()))\n",
    "    X_train[\"outliers\"] = outliers\n",
    "    X_train = X_train[X_train[\"outliers\"] > 0]\n",
    "    Y_train[\"outliers\"] = outliers\n",
    "    Y_train = Y_train[Y_train[\"outliers\"] > 0]\n",
    "    X_train.drop([\"outliers\"], axis=1, inplace = True)\n",
    "    Y_train.drop([\"outliers\"], axis=1, inplace = True)\n",
    "    return X_train, Y_train\n",
    "\n",
    "#Zero mean unit variance for train and test data\n",
    "def scale_data(train, test):    \n",
    "    scaler = StandardScaler().fit(train, Y_train)\n",
    "    train = scaler.transform(train)\n",
    "    test = scaler.transform(test)\n",
    "   \n",
    "    return train, test\n",
    "\n",
    "def coeff_determination(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load train and test set\n",
    "train_data = pd.read_csv(TRAIN_FILE_PATH)\n",
    "train_data.drop(train_data.columns[0], axis=1, inplace=True)\n",
    "\n",
    "Y_train = pd.read_csv(TARGET_FILE_PATH)\n",
    "Y_train.drop(Y_train.columns[0], axis=1, inplace = True)\n",
    "\n",
    "test_data =  pd.read_csv(TEST_FILE_PATH)\n",
    "id_test = test_data.columns[0]\n",
    "test_data.drop(test_data.columns[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle NAN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling all nan values in the data with the mean\n"
     ]
    }
   ],
   "source": [
    "train_data_mean, test_data_mean = fill_NaN(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle possible outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 3 Outliers\n"
     ]
    }
   ],
   "source": [
    "train_data_mean_no_outlier, Y_train_no_outlier = remove_outliers(train_data_mean, Y_train)\n",
    "Y_train.drop([\"outliers\"], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled = scale_data(train_data_mean_no_outlier, test_data_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract feature importance of Random Forest & find intersection with f_regression ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced to 179 features\n",
      "Filling all nan values in the data with the mean\n"
     ]
    }
   ],
   "source": [
    "n_features_fr = 150\n",
    "n_features_rf = 100\n",
    "\n",
    "features_scores = f_regression(X_train_scaled, Y_train_no_outlier)[0]\n",
    "y = list(features_scores)\n",
    "myarray = np.asarray(y)\n",
    "\n",
    "indices_fr = myarray.argsort()[-n_features_fr:][::-1]\n",
    "\n",
    "rng = np.random.RandomState(seed)\n",
    "rf = RandomForestRegressor(n_jobs=-1, n_estimators=50, random_state=rng)\n",
    "rf.fit(X_train_scaled,Y_train_no_outlier)\n",
    "\n",
    "scores = list(rf.feature_importances_)\n",
    "my_rf_features = np.asarray(scores)\n",
    "\n",
    "\n",
    "indices_rf = my_rf_features.argsort()[-n_features_rf:][::-1]\n",
    "\n",
    "indices = list(np.union1d(indices_rf, indices_fr))\n",
    "print(f\"Reduced to {len(indices)} features\")\n",
    "\n",
    "# reduce the train and test set\n",
    "X_train_subset = train_data_mean[train_data_mean.columns[indices]]\n",
    "X_test_subset = test_data_mean[train_data_mean.columns[indices]]\n",
    "\n",
    "X_train_subset, X_test_subset = fill_NaN(X_train_subset, X_test_subset)\n",
    "X_train_subset, X_test_subset = scale_data(X_train_subset, X_test_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fitting ...\n",
      "calculate predictions\n"
     ]
    }
   ],
   "source": [
    "Y = Y_train\n",
    "dropout = 0.1\n",
    "#print(Y)\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=len(indices), kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(rate = dropout))\n",
    "model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(rate = dropout))\n",
    "model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(rate = dropout))\n",
    "model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(rate = dropout))\n",
    "model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(rate = dropout))\n",
    "model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(rate = dropout))\n",
    "model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(rate = dropout))\n",
    "model.add(Dense(30, input_dim=72, kernel_regularizer = regularizers.l2(1), init='RandomUniform'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(rate = dropout))\n",
    "\n",
    "model.add(Dense(1, init='RandomUniform'))\n",
    "# Compile model\n",
    "optimizer = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=[coeff_determination])\n",
    "# Fit the model\n",
    "print(\"Start fitting ...\")\n",
    "model.fit(x=X_train_subset, y=Y, epochs=80, verbose=0, validation_split=0.1, shuffle=True, steps_per_epoch=50, initial_epoch=0, validation_steps=5)\n",
    "# calculate predictions\n",
    "print(\"calculate predictions\")\n",
    "predictions = model.predict(X_test_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create finalsubmission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare predictions to be wirtten to csv\n",
    "filename = \"final.csv\"\n",
    "test_data =  pd.read_csv(TEST_FILE_PATH)\n",
    "test_data[\"y\"] = predictions\n",
    "test_data[[\"id\", \"y\"]].to_csv(\"submissions/\"+filename, index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aml-3)",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
