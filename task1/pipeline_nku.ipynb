{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, BaggingRegressor, AdaBoostRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from fancyimpute import KNN, NuclearNormMinimization, SoftImpute, IterativeImputer, BiScaler, MatrixFactorization\n",
    "\n",
    "%aimport util.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = util.data.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Missing Values (Remove NaNs)\n",
    "\n",
    "use one of the methods to fill the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_incomplete = train_data.drop([\"id\", \"y\"], axis=1).values\n",
    "y = train_data[\"y\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 6\n",
    "\n",
    "# Use nearest rows which have a feature to fill in each row's missing features\n",
    "X = KNN(k=k).fit_transform(X_incomplete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### singular values thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of solving the nuclear norm objective directly, instead\n",
    "# induce sparsity using singular value thresholding\n",
    "X_incomplete_normalized = BiScaler().fit_transform(X_incomplete)\n",
    "X = SoftImpute().fit_transform(X_incomplete_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = MatrixFactorization(learning_rate= 0.001, rank=40).fit_transform(X_incomplete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean_values = train_data.mean()\n",
    "train_data_mean =  train_data.fillna(train_mean_values)\n",
    "X = train_data_mean.drop([\"id\", \"y\"], axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization / Standardization\n",
    "\n",
    "use one of the methods provided to normalize the data (expects no NaNs)\n",
    "\n",
    "[Compare the effect of different scalers on data with outliers](http://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 - Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = normalize(X, axis=0, norm='l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 - Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = normalize(X, axis=0, norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scalar\n",
    "removes mean and divides by std (**sensitive to outliers** => probably not a good idea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Robust Scaler\n",
    "Scale features using statistics that are robust to outliers. [scikit doc](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = RobustScaler(quantile_range=(10, 90)).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection and Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RandomForestRegressor(n_jobs=-1, n_estimators=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = ExtraTreesRegressor(n_jobs=-1, n_estimators=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Feature Selection with Recursive Feature Elimination with Cross Validation (RFECV)\n",
    "\n",
    "n_rm_features_per_iteration = 4\n",
    "cv_k = 3\n",
    "\n",
    "\n",
    "\n",
    "rfecv = RFECV(estimator, step=n_rm_features_per_iteration, cv=cv_k, scoring='r2', verbose=1)\n",
    "\n",
    "rfecv.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal number of features : %d\" % rfecv.n_features_) #47\n",
    "print(f\"Validation Score: {max(rfecv.grid_scores_)}\")\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, n_rm_features_per_iteration * len(rfecv.grid_scores_) + 1, n_rm_features_per_iteration), rfecv.grid_scores_)\n",
    "plt.show()\n",
    "\n",
    "selected_feature_cols = train_data.columns.values[1:-1][rfecv.support_]\n",
    "\n",
    "print('Selected Features: ')\n",
    "print(selected_feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestRegressor\n",
    "\n",
    "selected_feature_cols = ['x0', 'x42', 'x80', 'x82', 'x89', 'x94', 'x96', 'x120', 'x137', 'x178', 'x185',\n",
    " 'x186', 'x192', 'x224', 'x234', 'x237', 'x264', 'x273', 'x280', 'x291', 'x300',\n",
    " 'x309', 'x320', 'x328', 'x331', 'x333', 'x340', 'x349', 'x391', 'x400', 'x424',\n",
    " 'x426', 'x449', 'x450', 'x470', 'x479', 'x499', 'x520', 'x529', 'x536', 'x547',\n",
    " 'x555', 'x559', 'x604', 'x618', 'x644', 'x651', 'x658', 'x664', 'x673', 'x685',\n",
    " 'x686', 'x687', 'x722', 'x730', 'x739', 'x743', 'x746', 'x751', 'x800', 'x810',\n",
    " 'x871', 'x882']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extras Tree Regressor\n",
    "\n",
    "selected_feature_cols_old = ['x0', 'x7', 'x60', 'x66', 'x80', 'x82', 'x88', 'x89', 'x94', 'x96', 'x117', 'x135',\n",
    " 'x137', 'x142', 'x178', 'x185', 'x192', 'x200', 'x210', 'x224', 'x229', 'x230',\n",
    " 'x234', 'x251', 'x273', 'x291', 'x297', 'x300', 'x309', 'x312', 'x333', 'x340',\n",
    " 'x349', 'x363', 'x370', 'x374', 'x388', 'x391', 'x424', 'x426', 'x428', 'x449',\n",
    " 'x450', 'x453', 'x470', 'x479', 'x490', 'x499', 'x504', 'x529', 'x547', 'x555',\n",
    " 'x559', 'x560', 'x586', 'x591', 'x599', 'x604', 'x608', 'x609', 'x613', 'x618',\n",
    " 'x622', 'x632', 'x636', 'x641', 'x643', 'x644', 'x651', 'x652', 'x664', 'x665',\n",
    " 'x666', 'x673', 'x685', 'x686', 'x687', 'x716', 'x722', 'x730', 'x734', 'x746',\n",
    " 'x751', 'x763', 'x789', 'x800', 'x803', 'x810', 'x838', 'x840', 'x844', 'x853',\n",
    " 'x870', 'x871', 'x882']\n",
    "\n",
    "selected_feature_cols = ['x0', 'x80', 'x82', 'x89', 'x96', 'x185', 'x192', 'x200', 'x224', 'x229', 'x273',\n",
    " 'x291', 'x309', 'x333', 'x340', 'x349', 'x370', 'x374', 'x391', 'x426', 'x457',\n",
    " 'x470', 'x479', 'x482', 'x499', 'x520', 'x529', 'x547', 'x555', 'x591', 'x599',\n",
    " 'x604', 'x613', 'x651', 'x664', 'x673', 'x685', 'x686', 'x687', 'x716', 'x722',\n",
    " 'x730', 'x746', 'x751', 'x803', 'x810', 'x823', 'x838', 'x853', 'x870', 'x882']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "k = 20\n",
    "\n",
    "train_mean_values = train_data.mean()\n",
    "train_data_mean =  train_data.fillna(train_mean_values)\n",
    "\n",
    "X_sel = train_data_mean[selected_feature_cols].values\n",
    "\n",
    "\n",
    "#X_sel_incomplete= train_data[selected_feature_cols].values\n",
    "#X_sel =  KNN(k=k).fit_transform(X_sel_incomplete)\n",
    "#X_sel = MinMaxScaler(feature_range=(0, 1)).fit_transform(X_sel)\n",
    "X_sel = RobustScaler(quantile_range=(10, 90)).fit_transform(X_sel)\n",
    "#X_sel = StandardScaler().fit_transform(X_sel)\n",
    "y_sel = train_data[\"y\"].values\n",
    "\n",
    "\n",
    "#pca = PCA(n_components=10)#\n",
    "#pca.fit_transform(X_sel)\n",
    "#eigenvalues = pca.explained_variance_\n",
    "#plt.plot(eigenvalues[0:20])\n",
    "#y_sel = y\n",
    "\n",
    "estimator = MLPRegressor(#learning_rate='constant', \n",
    "                        #hidden_layer_sizes=(100),\n",
    "                         activation='logistic', \n",
    "                        # learning_rate_init=0.0001, \n",
    "                         max_iter=15000, \n",
    "                         early_stopping =True,\n",
    "                         validation_fraction=0.1,\n",
    "                         tol=0.0000000000001,\n",
    "                            #alpha=0.0001,\n",
    "                         #n_iter_no_change=10,\n",
    "                         verbose=True)\n",
    "\n",
    "estimator = ExtraTreesRegressor(n_jobs=-1, n_estimators=20)\n",
    "\n",
    "#estimator = RandomForestRegressor(n_jobs=-1, n_estimators=60)\n",
    "\n",
    "estimator.fit(X_sel,y_sel)\n",
    "#score = estimator.score(X_sel,y_sel)\n",
    "\n",
    "score = cross_val_score((estimator), X_sel, y_sel, scoring='r2', cv=12)\n",
    "print(score)\n",
    "print(np.mean(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(y_sel.shape[0]), y_sel)\n",
    "\n",
    "estimator.fit(X_sel,y_sel)\n",
    "\n",
    "#X_test_sel_incomplete= test_data[selected_feature_cols].values\n",
    "#X_test_sel =  KNN(k=k).fit_transform(X_test_sel_incomplete)\n",
    "\n",
    "\n",
    "test_mean_values = test_data.mean()\n",
    "test_data_mean =  test_data.fillna(test_mean_values)\n",
    "X_test_sel = test_data_mean[selected_feature_cols].values\n",
    "X_test_sel = RobustScaler(quantile_range=(10, 90)).fit_transform(X_test_sel)\n",
    "\n",
    "y_pred = estimator.predict(X_test_sel)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(y_pred))\n",
    "print(max(y_pred))\n",
    "\n",
    "x = range(len(y_pred))\n",
    "plt.scatter(x, np.sort(y_pred))\n",
    "\n",
    "temp = train_data.sort_values(by=['y']).reset_index(drop=True)\n",
    "x = range(len(temp[\"y\"]))\n",
    "plt.scatter(x, temp[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sel_incomplete= train_data[selected_feature_cols].values\n",
    "\n",
    "train_mean_values = train_data.mean()\n",
    "train_data_mean =  train_data.fillna(train_mean_values)\n",
    "X_sel = train_data_mean[selected_feature_cols].values\n",
    "\n",
    "\n",
    "#k = 10\n",
    "X_sel =  KNN(k=k).fit_transform(X_sel_incomplete)\n",
    "\n",
    "y_sel = train_data[\"y\"].values\n",
    "\n",
    "X_sel = RobustScaler(quantile_range=(10, 90)).fit_transform(X_sel)\n",
    "\n",
    "score = cross_val_score(RandomForestRegressor(n_jobs=-1, n_estimators=200), X_sel, y_sel, scoring='r2', cv=12)\n",
    "print(score.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svr_model(gamma, C, epsilon):\n",
    "    score = cross_val_score(\n",
    "                SVR(gamma=gamma, C=C, epsilon=epsilon, kernel='poly'), \n",
    "                X, y, scoring='r2').mean()\n",
    "    #score = np.array(score)\n",
    "    return score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Optimization\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "bo = BayesianOptimization(svr_model,{'gamma': (0.01, 5.0), 'C': (0.1, 100), 'epsilon': (0.0001, 1)})\n",
    "\n",
    "# Once we are satisfied with the initialization conditions\n",
    "# we let the algorithm do its magic by calling the maximize()\n",
    "# method.\n",
    "bo.maximize(init_points=5, n_iter=15, kappa=10)\n",
    "\n",
    "# The output values can be accessed with self.res\n",
    "print(bo.res['max'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Official Metric\n",
    "score = r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"y\"] = y_pred\n",
    "util.data.write_submission(test_data, \"nku\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
