{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE, f_regression\n",
    "from sklearn.linear_model import (LinearRegression, Ridge, Lasso, RandomizedLasso)\n",
    "from sklearn.preprocessing import (MinMaxScaler, RobustScaler)\n",
    "from sklearn.ensemble import (RandomForestRegressor, IsolationForest)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from fancyimpute import KNN,  SoftImpute, IterativeImputer, BiScaler, MatrixFactorization\n",
    "import util.data\n",
    "rng = np.random.RandomState(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: \n",
      "  Amount of features: 888\n",
      "  Amount of observations: 1212\n",
      "  Min age: 42.0 Max age: 96.0\n",
      "\n",
      "Test Data: \n",
      "  Amount of observations: 776\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = util.data.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use various ways to fill the nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 1/1212 with 66 missing, elapsed time: 7.865\n",
      "Imputing row 101/1212 with 60 missing, elapsed time: 7.944\n",
      "Imputing row 201/1212 with 68 missing, elapsed time: 8.027\n",
      "Imputing row 301/1212 with 68 missing, elapsed time: 8.117\n",
      "Imputing row 401/1212 with 63 missing, elapsed time: 8.201\n",
      "Imputing row 501/1212 with 66 missing, elapsed time: 8.279\n",
      "Imputing row 601/1212 with 77 missing, elapsed time: 8.364\n",
      "Imputing row 701/1212 with 65 missing, elapsed time: 8.445\n",
      "Imputing row 801/1212 with 61 missing, elapsed time: 8.529\n",
      "Imputing row 901/1212 with 73 missing, elapsed time: 8.620\n",
      "Imputing row 1001/1212 with 58 missing, elapsed time: 8.702\n",
      "Imputing row 1101/1212 with 73 missing, elapsed time: 8.784\n",
      "Imputing row 1201/1212 with 53 missing, elapsed time: 8.871\n",
      "[BiScaler] Initial log residual value = 81.892772\n",
      "[BiScaler] Iter 1: log residual = 1.651141, log improvement ratio=80.241632\n",
      "[BiScaler] Iter 2: log residual = 0.995476, log improvement ratio=0.655665\n",
      "[BiScaler] Iter 3: log residual = -0.085638, log improvement ratio=1.081114\n",
      "[BiScaler] Iter 4: log residual = -1.372989, log improvement ratio=1.287350\n",
      "[BiScaler] Iter 5: log residual = -2.743947, log improvement ratio=1.370959\n",
      "[BiScaler] Iter 6: log residual = -4.150768, log improvement ratio=1.406821\n",
      "[BiScaler] Iter 7: log residual = -5.573414, log improvement ratio=1.422645\n",
      "[BiScaler] Iter 8: log residual = -7.003151, log improvement ratio=1.429738\n",
      "[BiScaler] Iter 9: log residual = -8.436101, log improvement ratio=1.432950\n",
      "[BiScaler] Iter 10: log residual = -9.870519, log improvement ratio=1.434418\n",
      "[BiScaler] Iter 11: log residual = -11.305612, log improvement ratio=1.435093\n",
      "[BiScaler] Iter 12: log residual = -12.741019, log improvement ratio=1.435406\n",
      "[BiScaler] Iter 13: log residual = -14.176571, log improvement ratio=1.435552\n",
      "[BiScaler] Iter 14: log residual = -15.612192, log improvement ratio=1.435621\n",
      "[BiScaler] Iter 15: log residual = -17.047845, log improvement ratio=1.435653\n",
      "[BiScaler] Iter 16: log residual = -18.483513, log improvement ratio=1.435668\n",
      "[BiScaler] Iter 17: log residual = -19.919188, log improvement ratio=1.435675\n",
      "[BiScaler] Iter 18: log residual = -21.354870, log improvement ratio=1.435682\n",
      "[BiScaler] Iter 19: log residual = -22.790559, log improvement ratio=1.435688\n",
      "[BiScaler] Iter 20: log residual = -24.226222, log improvement ratio=1.435663\n",
      "[BiScaler] Iter 21: log residual = -25.661899, log improvement ratio=1.435678\n",
      "[BiScaler] Iter 22: log residual = -27.097594, log improvement ratio=1.435694\n",
      "[BiScaler] Iter 23: log residual = -28.533258, log improvement ratio=1.435664\n",
      "[BiScaler] Iter 24: log residual = -29.968733, log improvement ratio=1.435475\n",
      "[BiScaler] Iter 25: log residual = -31.403494, log improvement ratio=1.434761\n",
      "[BiScaler] Iter 26: log residual = -32.833458, log improvement ratio=1.429964\n",
      "[BiScaler] Iter 27: log residual = -34.252805, log improvement ratio=1.419347\n",
      "[BiScaler] Iter 28: log residual = -35.551127, log improvement ratio=1.298323\n",
      "[BiScaler] Iter 29: log residual = -36.763116, log improvement ratio=1.211989\n",
      "[BiScaler] Iter 30: log residual = -37.687069, log improvement ratio=0.923953\n",
      "[BiScaler] Iter 31: log residual = -37.606081, log improvement ratio=-0.080988\n",
      "[SoftImpute] Max Singular Value of X_init = 361.212204\n",
      "[SoftImpute] Iter 1: observed MAE=0.161246 rank=787\n",
      "[SoftImpute] Iter 2: observed MAE=0.160640 rank=759\n",
      "[SoftImpute] Iter 3: observed MAE=0.159980 rank=746\n",
      "[SoftImpute] Iter 4: observed MAE=0.159629 rank=740\n",
      "[SoftImpute] Iter 5: observed MAE=0.159443 rank=736\n",
      "[SoftImpute] Iter 6: observed MAE=0.159328 rank=734\n",
      "[SoftImpute] Iter 7: observed MAE=0.159247 rank=731\n",
      "[SoftImpute] Iter 8: observed MAE=0.159188 rank=730\n",
      "[SoftImpute] Iter 9: observed MAE=0.159144 rank=730\n",
      "[SoftImpute] Iter 10: observed MAE=0.159110 rank=729\n",
      "[SoftImpute] Iter 11: observed MAE=0.159083 rank=728\n",
      "[SoftImpute] Iter 12: observed MAE=0.159061 rank=727\n",
      "[SoftImpute] Iter 13: observed MAE=0.159044 rank=727\n",
      "[SoftImpute] Iter 14: observed MAE=0.159030 rank=727\n",
      "[SoftImpute] Iter 15: observed MAE=0.159020 rank=727\n",
      "[SoftImpute] Iter 16: observed MAE=0.159011 rank=727\n",
      "[SoftImpute] Iter 17: observed MAE=0.159004 rank=726\n",
      "[SoftImpute] Iter 18: observed MAE=0.158998 rank=725\n",
      "[SoftImpute] Iter 19: observed MAE=0.158993 rank=725\n",
      "[SoftImpute] Iter 20: observed MAE=0.158989 rank=725\n",
      "[SoftImpute] Iter 21: observed MAE=0.158986 rank=725\n",
      "[SoftImpute] Iter 22: observed MAE=0.158983 rank=725\n",
      "[SoftImpute] Iter 23: observed MAE=0.158981 rank=725\n",
      "[SoftImpute] Iter 24: observed MAE=0.158979 rank=725\n",
      "[SoftImpute] Iter 25: observed MAE=0.158978 rank=725\n",
      "[SoftImpute] Iter 26: observed MAE=0.158977 rank=725\n",
      "[SoftImpute] Iter 27: observed MAE=0.158976 rank=725\n",
      "[SoftImpute] Iter 28: observed MAE=0.158975 rank=725\n",
      "[SoftImpute] Iter 29: observed MAE=0.158974 rank=725\n",
      "[SoftImpute] Stopped after iteration 29 for lambda=7.224244\n",
      "Train on 898130 samples, validate on 99793 samples\n",
      "Epoch 1/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0371 - val_loss: 1.0320\n",
      "Epoch 2/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0358 - val_loss: 1.0316\n",
      "Epoch 3/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0349 - val_loss: 1.0313\n",
      "Epoch 4/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0340 - val_loss: 1.0309\n",
      "Epoch 5/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0331 - val_loss: 1.0306\n",
      "Epoch 6/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0323 - val_loss: 1.0303\n",
      "Epoch 7/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0314 - val_loss: 1.0299\n",
      "Epoch 8/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0305 - val_loss: 1.0296\n",
      "Epoch 9/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0296 - val_loss: 1.0292\n",
      "Epoch 10/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0287 - val_loss: 1.0289\n",
      "Epoch 11/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0278 - val_loss: 1.0285\n",
      "Epoch 12/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0268 - val_loss: 1.0282\n",
      "Epoch 13/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0259 - val_loss: 1.0278\n",
      "Epoch 14/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0250 - val_loss: 1.0274\n",
      "Epoch 15/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0240 - val_loss: 1.0270\n",
      "Epoch 16/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0231 - val_loss: 1.0266\n",
      "Epoch 17/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0221 - val_loss: 1.0262\n",
      "Epoch 18/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0211 - val_loss: 1.0258\n",
      "Epoch 19/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0202 - val_loss: 1.0254\n",
      "Epoch 20/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0192 - val_loss: 1.0250\n",
      "Epoch 21/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0183 - val_loss: 1.0246\n",
      "Epoch 22/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0173 - val_loss: 1.0241\n",
      "Epoch 23/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0163 - val_loss: 1.0237\n",
      "Epoch 24/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0153 - val_loss: 1.0232\n",
      "Epoch 25/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0144 - val_loss: 1.0228\n",
      "Epoch 26/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0134 - val_loss: 1.0223\n",
      "Epoch 27/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0124 - val_loss: 1.0218\n",
      "Epoch 28/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0114 - val_loss: 1.0213\n",
      "Epoch 29/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0104 - val_loss: 1.0208\n",
      "Epoch 30/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0094 - val_loss: 1.0202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0084 - val_loss: 1.0197\n",
      "Epoch 32/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0073 - val_loss: 1.0191\n",
      "Epoch 33/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0063 - val_loss: 1.0185\n",
      "Epoch 34/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0052 - val_loss: 1.0179\n",
      "Epoch 35/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0042 - val_loss: 1.0173\n",
      "Epoch 36/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0031 - val_loss: 1.0166\n",
      "Epoch 37/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0020 - val_loss: 1.0160\n",
      "Epoch 38/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 1.0009 - val_loss: 1.0153\n",
      "Epoch 39/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9997 - val_loss: 1.0146\n",
      "Epoch 40/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9986 - val_loss: 1.0139\n",
      "Epoch 41/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9974 - val_loss: 1.0131\n",
      "Epoch 42/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9962 - val_loss: 1.0123\n",
      "Epoch 43/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9950 - val_loss: 1.0115\n",
      "Epoch 44/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9938 - val_loss: 1.0107\n",
      "Epoch 45/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9926 - val_loss: 1.0099\n",
      "Epoch 46/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9913 - val_loss: 1.0090\n",
      "Epoch 47/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9900 - val_loss: 1.0081\n",
      "Epoch 48/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9887 - val_loss: 1.0071\n",
      "Epoch 49/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9873 - val_loss: 1.0062\n",
      "Epoch 50/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9860 - val_loss: 1.0052\n",
      "Epoch 51/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9846 - val_loss: 1.0042\n",
      "Epoch 52/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9832 - val_loss: 1.0032\n",
      "Epoch 53/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9817 - val_loss: 1.0021\n",
      "Epoch 54/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9803 - val_loss: 1.0010\n",
      "Epoch 55/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9788 - val_loss: 0.9999\n",
      "Epoch 56/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9773 - val_loss: 0.9988\n",
      "Epoch 57/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9758 - val_loss: 0.9976\n",
      "Epoch 58/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9742 - val_loss: 0.9964\n",
      "Epoch 59/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9727 - val_loss: 0.9952\n",
      "Epoch 60/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9711 - val_loss: 0.9940\n",
      "Epoch 61/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9695 - val_loss: 0.9928\n",
      "Epoch 62/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9679 - val_loss: 0.9915\n",
      "Epoch 63/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9662 - val_loss: 0.9902\n",
      "Epoch 64/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9646 - val_loss: 0.9890\n",
      "Epoch 65/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9629 - val_loss: 0.9877\n",
      "Epoch 66/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9612 - val_loss: 0.9863\n",
      "Epoch 67/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9595 - val_loss: 0.9850\n",
      "Epoch 68/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9578 - val_loss: 0.9837\n",
      "Epoch 69/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9561 - val_loss: 0.9823\n",
      "Epoch 70/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9544 - val_loss: 0.9810\n",
      "Epoch 71/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9527 - val_loss: 0.9796\n",
      "Epoch 72/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9509 - val_loss: 0.9783\n",
      "Epoch 73/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9492 - val_loss: 0.9769\n",
      "Epoch 74/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9475 - val_loss: 0.9756\n",
      "Epoch 75/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9457 - val_loss: 0.9742\n",
      "Epoch 76/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9440 - val_loss: 0.9729\n",
      "Epoch 77/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9423 - val_loss: 0.9715\n",
      "Epoch 78/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9406 - val_loss: 0.9702\n",
      "Epoch 79/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9388 - val_loss: 0.9689\n",
      "Epoch 80/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9371 - val_loss: 0.9675\n",
      "Epoch 81/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9354 - val_loss: 0.9663\n",
      "Epoch 82/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9338 - val_loss: 0.9650\n",
      "Epoch 83/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9321 - val_loss: 0.9637\n",
      "Epoch 84/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9304 - val_loss: 0.9624\n",
      "Epoch 85/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9288 - val_loss: 0.9612\n",
      "Epoch 86/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9271 - val_loss: 0.9600\n",
      "Epoch 87/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9255 - val_loss: 0.9587\n",
      "Epoch 88/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9239 - val_loss: 0.9576\n",
      "Epoch 89/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9223 - val_loss: 0.9564\n",
      "Epoch 90/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9208 - val_loss: 0.9552\n",
      "Epoch 91/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9192 - val_loss: 0.9541\n",
      "Epoch 92/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9177 - val_loss: 0.9530\n",
      "Epoch 93/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9162 - val_loss: 0.9519\n",
      "Epoch 94/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9147 - val_loss: 0.9508\n",
      "Epoch 95/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9132 - val_loss: 0.9498\n",
      "Epoch 96/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9117 - val_loss: 0.9488\n",
      "Epoch 97/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9103 - val_loss: 0.9478\n",
      "Epoch 98/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9088 - val_loss: 0.9468\n",
      "Epoch 99/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9074 - val_loss: 0.9458\n",
      "Epoch 100/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9060 - val_loss: 0.9449\n",
      "Epoch 101/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9046 - val_loss: 0.9439\n",
      "Epoch 102/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9033 - val_loss: 0.9430\n",
      "Epoch 103/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9019 - val_loss: 0.9422\n",
      "Epoch 104/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.9006 - val_loss: 0.9413\n",
      "Epoch 105/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8993 - val_loss: 0.9404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8980 - val_loss: 0.9396\n",
      "Epoch 107/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8967 - val_loss: 0.9388\n",
      "Epoch 108/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8954 - val_loss: 0.9380\n",
      "Epoch 109/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8942 - val_loss: 0.9372\n",
      "Epoch 110/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8929 - val_loss: 0.9365\n",
      "Epoch 111/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8917 - val_loss: 0.9357\n",
      "Epoch 112/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8905 - val_loss: 0.9350\n",
      "Epoch 113/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8893 - val_loss: 0.9343\n",
      "Epoch 114/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8881 - val_loss: 0.9336\n",
      "Epoch 115/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8869 - val_loss: 0.9329\n",
      "Epoch 116/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8857 - val_loss: 0.9322\n",
      "Epoch 117/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8846 - val_loss: 0.9316\n",
      "Epoch 118/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8834 - val_loss: 0.9309\n",
      "Epoch 119/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8823 - val_loss: 0.9303\n",
      "Epoch 120/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8812 - val_loss: 0.9297\n",
      "Epoch 121/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8801 - val_loss: 0.9291\n",
      "Epoch 122/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8789 - val_loss: 0.9285\n",
      "Epoch 123/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8778 - val_loss: 0.9279\n",
      "Epoch 124/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8768 - val_loss: 0.9273\n",
      "Epoch 125/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8757 - val_loss: 0.9268\n",
      "Epoch 126/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8746 - val_loss: 0.9262\n",
      "Epoch 127/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8735 - val_loss: 0.9257\n",
      "Epoch 128/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8725 - val_loss: 0.9252\n",
      "Epoch 129/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8714 - val_loss: 0.9247\n",
      "Epoch 130/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8704 - val_loss: 0.9242\n",
      "Epoch 131/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8694 - val_loss: 0.9237\n",
      "Epoch 132/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8683 - val_loss: 0.9232\n",
      "Epoch 133/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8673 - val_loss: 0.9227\n",
      "Epoch 134/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8663 - val_loss: 0.9223\n",
      "Epoch 135/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8653 - val_loss: 0.9218\n",
      "Epoch 136/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8643 - val_loss: 0.9214\n",
      "Epoch 137/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8633 - val_loss: 0.9210\n",
      "Epoch 138/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8624 - val_loss: 0.9205\n",
      "Epoch 139/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8614 - val_loss: 0.9201\n",
      "Epoch 140/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8604 - val_loss: 0.9197\n",
      "Epoch 141/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8595 - val_loss: 0.9193\n",
      "Epoch 142/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8585 - val_loss: 0.9190\n",
      "Epoch 143/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8576 - val_loss: 0.9186\n",
      "Epoch 144/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8567 - val_loss: 0.9182\n",
      "Epoch 145/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8557 - val_loss: 0.9179\n",
      "Epoch 146/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8548 - val_loss: 0.9176\n",
      "Epoch 147/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8539 - val_loss: 0.9172\n",
      "Epoch 148/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8530 - val_loss: 0.9169\n",
      "Epoch 149/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8521 - val_loss: 0.9166\n",
      "Epoch 150/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8512 - val_loss: 0.9163\n",
      "Epoch 151/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8503 - val_loss: 0.9160\n",
      "Epoch 152/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8495 - val_loss: 0.9157\n",
      "Epoch 153/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8486 - val_loss: 0.9155\n",
      "Epoch 154/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8477 - val_loss: 0.9152\n",
      "Epoch 155/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8469 - val_loss: 0.9150\n",
      "Epoch 156/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8460 - val_loss: 0.9147\n",
      "Epoch 157/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8452 - val_loss: 0.9145\n",
      "Epoch 158/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8444 - val_loss: 0.9143\n",
      "Epoch 159/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8436 - val_loss: 0.9141\n",
      "Epoch 160/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8427 - val_loss: 0.9139\n",
      "Epoch 161/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8419 - val_loss: 0.9137\n",
      "Epoch 162/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8411 - val_loss: 0.9135\n",
      "Epoch 163/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8403 - val_loss: 0.9133\n",
      "Epoch 164/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8396 - val_loss: 0.9132\n",
      "Epoch 165/10000\n",
      "898130/898130 [==============================] - 1s 1us/step - loss: 0.8388 - val_loss: 0.9130\n"
     ]
    }
   ],
   "source": [
    "X_incomplete = train_data.drop([\"id\", \"y\"], axis=1).values\n",
    "X = {}\n",
    "\n",
    "\n",
    "k = 12\n",
    "# Use nearest rows which have a feature to fill in each row's missing features\n",
    "X[\"KNN\"] = KNN(k=k).fit_transform(X_incomplete)\n",
    "\n",
    "\n",
    "# Instead of solving the nuclear norm objective directly, instead\n",
    "# induce sparsity using singular value thresholding\n",
    "X_incomplete_normalized = BiScaler().fit_transform(X_incomplete)\n",
    "X[\"SoftImpute\"] = SoftImpute().fit_transform(X_incomplete_normalized)\n",
    "\n",
    "X[\"MatrixFactorization\"] = MatrixFactorization(learning_rate= 0.001, rank=50).fit_transform(X_incomplete)\n",
    "\n",
    "\n",
    "train_mean_values = train_data.mean()\n",
    "train_data_mean =  train_data.fillna(train_mean_values)\n",
    "X[\"Mean\"] = train_data_mean.drop([\"id\", \"y\"], axis=1).values\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = {}\n",
    "y = train_data[\"y\"].values\n",
    "Y[\"KNN\"] = y\n",
    "Y[\"SoftImpute\"] = y\n",
    "Y[\"MatrixFactorization\"] = y\n",
    "Y[\"Mean\"] = y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'KNN': array([ 1, -1,  1, ...,  1,  1,  1]), 'SoftImpute': array([1, 1, 1, ..., 1, 1, 1]), 'MatrixFactorization': array([1, 1, 1, ..., 1, 1, 1]), 'Mean': array([1, 1, 1, ..., 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "# fit the model compute the otliers\n",
    "outliers = {}\n",
    "isoForest = IsolationForest(behaviour='new', max_samples=100, random_state=rng, contamination='auto')\n",
    "for key in X:\n",
    "    outliers[key] = isoForest.fit_predict(X[key])\n",
    "print(outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the outlier\n",
    "for key in X:\n",
    "    X[key] = X[key][np.where(outliers[key] > 0)]\n",
    "    Y[key] = Y[key][np.where(outliers[key] > 0)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in X:\n",
    "    transformer = RobustScaler().fit(X[key])\n",
    "    X[key] = transformer.transform(X[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the feature ranking for each imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary to store our rankings\n",
    "ranks = {}\n",
    "for key in X:\n",
    "    ranks[key] = {}\n",
    "\n",
    "# Create our function which stores the feature rankings to the ranks dictionary\n",
    "def ranking(ranks, names, order=1):\n",
    "    minmax = MinMaxScaler()\n",
    "    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n",
    "    ranks = map(lambda x: round(x,2), ranks)\n",
    "    return dict(zip(names, ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ax/miniconda3/envs/aml/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class RandomizedLasso is deprecated; The class RandomizedLasso is deprecated in 0.19 and will be removed in 0.21.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing for KNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ax/miniconda3/envs/aml/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing for SoftImpute\n",
      "Computing for MatrixFactorization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ax/miniconda3/envs/aml/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing for Mean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ax/miniconda3/envs/aml/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "colnames = train_data.drop([\"id\", \"y\"], axis=1).columns\n",
    "\n",
    "rlasso = RandomizedLasso(alpha=0.04)\n",
    "lr = LinearRegression(normalize=True)\n",
    "ridge = Ridge(alpha = 7)\n",
    "lasso = Lasso(alpha=.05)\n",
    "rf = RandomForestRegressor(n_jobs=-1, n_estimators=50)\n",
    "\n",
    "\n",
    "for key in X:\n",
    "    print(\"Computing for {}\".format(key))\n",
    "    rlasso.fit(X[key], Y[key])\n",
    "    ranks[key][\"rlasso/Stability\"] = ranking(np.abs(rlasso.scores_), colnames)\n",
    "    print(\"rLasso done\")\n",
    "    # Construct our Linear Regression model\n",
    "\n",
    "    lr.fit(X[key], Y[key])\n",
    "    ranks[key][\"LinReg\"] = ranking(np.abs(lr.coef_), colnames)\n",
    "    print(\"LinReg done\")\n",
    "    \n",
    "    #stop the search when only the last feature is left\n",
    "    rfe = RFE(lr, n_features_to_select=1 )\n",
    "    rfe.fit(X[key], Y[key])\n",
    "    ranks[key][\"RFE\"] = ranking(list(map(float, rfe.ranking_)), colnames, order=-1)\n",
    "    print(\"RFE done\")\n",
    "    \n",
    "    ridge.fit(X[key],Y[key])\n",
    "    ranks[key]['Ridge'] = ranking(np.abs(ridge.coef_), colnames)\n",
    "    print(\"Ridge done\")\n",
    "    \n",
    "    lasso.fit(X[key], Y[key])\n",
    "    ranks[key][\"Lasso\"] = ranking(np.abs(lasso.coef_), colnames)\n",
    "    print(\"Lasso done\")\n",
    "    \n",
    "    rf.fit(X[key],Y[key])\n",
    "    ranks[key][\"RF\"] = ranking(rf.feature_importances_, colnames);\n",
    "    print(\"RF done\")\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dictionary to store the mean value calculated from all the scores\n",
    "r = {}\n",
    "meanplot = {}\n",
    "for key in X:\n",
    "    r[key] = {}    \n",
    "    for name in colnames:\n",
    "        r[key][name] = round(np.mean([ranks[key][method][name] for method in ranks[key].keys()]), 2)\n",
    " \n",
    "    methods = sorted(ranks[key].keys())\n",
    "    ranks[key][\"Mean\"] = r[key]\n",
    "    methods.append(\"Mean\")\n",
    "    \n",
    "    # Put the mean scores into a Pandas dataframe\n",
    "    meanplot[key] = pd.DataFrame(list(r[key].items()), columns= ['Feature','Mean Ranking'])\n",
    "\n",
    "    # Sort the dataframe\n",
    "    meanplot[key] = meanplot[key].sort_values('Mean Ranking', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the first 50 features\n",
    "n_features_to_use = 50\n",
    "combined_feature_index = set()\n",
    "X_reduced_dim = {}\n",
    "for key in meanplot:\n",
    "    feature_list = list(meanplot[key].head(n_features_to_use)[\"Feature\"].values) \n",
    "    combined_feature_index.update(feature_list)\n",
    "    X_reduced_dim[key] = pd.DataFrame(X[key], columns=colnames)\n",
    "    X_reduced_dim[key] = X_reduced_dim[key][feature_list]\n",
    "    X_reduced_dim[key][\"y\"] = Y[key]\n",
    "    X_reduced_dim[key].to_csv(\"preprocessed/\" + key + \".csv\", index=False)\n",
    "    #X_reduced_dim[key] = X\n",
    "    #train_subset = train_data_mean[feature_list + [\"id\", \"y\"]]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ax/miniconda3/envs/aml/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, input_dim=50, kernel_initializer=\"RandomUniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/ax/miniconda3/envs/aml/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, kernel_initializer=\"RandomUniform\")`\n",
      "  \"\"\"\n",
      "/home/ax/miniconda3/envs/aml/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, kernel_initializer=\"RandomUniform\")`\n",
      "  import sys\n",
      "/home/ax/miniconda3/envs/aml/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"RandomUniform\")`\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(n_features_to_use, input_dim=50, init='RandomUniform'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(50, init='RandomUniform'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(100, init='RandomUniform'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(1, init='RandomUniform', activation='sigmoid'))\n",
    "# Compile model\n",
    "optimizer = Adam(lr=0.000005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer=optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load test data and compute same preprocessin steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 1/776 with 63 missing, elapsed time: 3.032\n",
      "Imputing row 101/776 with 54 missing, elapsed time: 3.100\n",
      "Imputing row 201/776 with 63 missing, elapsed time: 3.167\n",
      "Imputing row 301/776 with 54 missing, elapsed time: 3.241\n",
      "Imputing row 401/776 with 65 missing, elapsed time: 3.326\n",
      "Imputing row 501/776 with 53 missing, elapsed time: 3.406\n",
      "Imputing row 601/776 with 71 missing, elapsed time: 3.491\n",
      "Imputing row 701/776 with 70 missing, elapsed time: 3.559\n",
      "[BiScaler] Initial log residual value = 81.909061\n",
      "[BiScaler] Iter 1: log residual = 0.999783, log improvement ratio=80.909279\n",
      "[BiScaler] Iter 2: log residual = -0.076421, log improvement ratio=1.076204\n",
      "[BiScaler] Iter 3: log residual = -1.423854, log improvement ratio=1.347433\n",
      "[BiScaler] Iter 4: log residual = -2.871885, log improvement ratio=1.448031\n",
      "[BiScaler] Iter 5: log residual = -4.353028, log improvement ratio=1.481143\n",
      "[BiScaler] Iter 6: log residual = -5.845094, log improvement ratio=1.492065\n",
      "[BiScaler] Iter 7: log residual = -7.340710, log improvement ratio=1.495616\n",
      "[BiScaler] Iter 8: log residual = -8.837465, log improvement ratio=1.496755\n",
      "[BiScaler] Iter 9: log residual = -10.334581, log improvement ratio=1.497116\n",
      "[BiScaler] Iter 10: log residual = -11.831810, log improvement ratio=1.497228\n",
      "[BiScaler] Iter 11: log residual = -13.329072, log improvement ratio=1.497262\n",
      "[BiScaler] Iter 12: log residual = -14.826344, log improvement ratio=1.497272\n",
      "[BiScaler] Iter 13: log residual = -16.323619, log improvement ratio=1.497275\n",
      "[BiScaler] Iter 14: log residual = -17.820895, log improvement ratio=1.497276\n",
      "[BiScaler] Iter 15: log residual = -19.318170, log improvement ratio=1.497276\n",
      "[BiScaler] Iter 16: log residual = -20.815446, log improvement ratio=1.497275\n",
      "[BiScaler] Iter 17: log residual = -22.312719, log improvement ratio=1.497273\n",
      "[BiScaler] Iter 18: log residual = -23.809995, log improvement ratio=1.497276\n",
      "[BiScaler] Iter 19: log residual = -25.307267, log improvement ratio=1.497272\n",
      "[BiScaler] Iter 20: log residual = -26.804517, log improvement ratio=1.497251\n",
      "[BiScaler] Iter 21: log residual = -28.301876, log improvement ratio=1.497359\n",
      "[BiScaler] Iter 22: log residual = -29.798721, log improvement ratio=1.496845\n",
      "[BiScaler] Iter 23: log residual = -31.295302, log improvement ratio=1.496581\n",
      "[BiScaler] Iter 24: log residual = -32.790717, log improvement ratio=1.495415\n",
      "[BiScaler] Iter 25: log residual = -34.267813, log improvement ratio=1.477096\n",
      "[BiScaler] Iter 26: log residual = -35.635644, log improvement ratio=1.367831\n",
      "[BiScaler] Iter 27: log residual = -36.886975, log improvement ratio=1.251331\n",
      "[BiScaler] Iter 28: log residual = -37.583628, log improvement ratio=0.696653\n",
      "[BiScaler] Iter 29: log residual = -38.307649, log improvement ratio=0.724021\n",
      "[BiScaler] Iter 30: log residual = -37.641868, log improvement ratio=-0.665781\n",
      "[SoftImpute] Max Singular Value of X_init = 279.891987\n",
      "[SoftImpute] Iter 1: observed MAE=0.147189 rank=679\n",
      "[SoftImpute] Iter 2: observed MAE=0.147136 rank=667\n",
      "[SoftImpute] Iter 3: observed MAE=0.146896 rank=658\n",
      "[SoftImpute] Iter 4: observed MAE=0.146639 rank=654\n",
      "[SoftImpute] Iter 5: observed MAE=0.146435 rank=650\n",
      "[SoftImpute] Iter 6: observed MAE=0.146280 rank=648\n",
      "[SoftImpute] Iter 7: observed MAE=0.146160 rank=644\n",
      "[SoftImpute] Iter 8: observed MAE=0.146064 rank=643\n",
      "[SoftImpute] Iter 9: observed MAE=0.145990 rank=642\n",
      "[SoftImpute] Iter 10: observed MAE=0.145931 rank=641\n",
      "[SoftImpute] Iter 11: observed MAE=0.145884 rank=640\n",
      "[SoftImpute] Iter 12: observed MAE=0.145846 rank=639\n",
      "[SoftImpute] Iter 13: observed MAE=0.145816 rank=639\n",
      "[SoftImpute] Iter 14: observed MAE=0.145791 rank=638\n",
      "[SoftImpute] Iter 15: observed MAE=0.145770 rank=637\n",
      "[SoftImpute] Iter 16: observed MAE=0.145753 rank=637\n",
      "[SoftImpute] Iter 17: observed MAE=0.145740 rank=637\n",
      "[SoftImpute] Iter 18: observed MAE=0.145729 rank=637\n",
      "[SoftImpute] Iter 19: observed MAE=0.145720 rank=636\n",
      "[SoftImpute] Iter 20: observed MAE=0.145712 rank=636\n",
      "[SoftImpute] Iter 21: observed MAE=0.145706 rank=636\n",
      "[SoftImpute] Iter 22: observed MAE=0.145701 rank=636\n",
      "[SoftImpute] Iter 23: observed MAE=0.145697 rank=636\n",
      "[SoftImpute] Iter 24: observed MAE=0.145693 rank=636\n",
      "[SoftImpute] Iter 25: observed MAE=0.145690 rank=636\n",
      "[SoftImpute] Iter 26: observed MAE=0.145688 rank=636\n",
      "[SoftImpute] Iter 27: observed MAE=0.145685 rank=636\n",
      "[SoftImpute] Iter 28: observed MAE=0.145684 rank=636\n",
      "[SoftImpute] Iter 29: observed MAE=0.145682 rank=636\n",
      "[SoftImpute] Iter 30: observed MAE=0.145681 rank=636\n",
      "[SoftImpute] Iter 31: observed MAE=0.145680 rank=636\n",
      "[SoftImpute] Iter 32: observed MAE=0.145679 rank=636\n",
      "[SoftImpute] Iter 33: observed MAE=0.145678 rank=636\n",
      "[SoftImpute] Stopped after iteration 33 for lambda=5.597840\n",
      "Train on 577737 samples, validate on 64193 samples\n",
      "Epoch 1/10000\n",
      "577737/577737 [==============================] - 1s 2us/step - loss: 1.0316 - val_loss: 1.0375\n",
      "Epoch 2/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0303 - val_loss: 1.0372\n",
      "Epoch 3/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0293 - val_loss: 1.0369\n",
      "Epoch 4/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0284 - val_loss: 1.0366\n",
      "Epoch 5/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0275 - val_loss: 1.0364\n",
      "Epoch 6/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0266 - val_loss: 1.0361\n",
      "Epoch 7/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0257 - val_loss: 1.0358\n",
      "Epoch 8/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0248 - val_loss: 1.0356\n",
      "Epoch 9/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0239 - val_loss: 1.0353\n",
      "Epoch 10/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0229 - val_loss: 1.0350\n",
      "Epoch 11/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0219 - val_loss: 1.0347\n",
      "Epoch 12/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0210 - val_loss: 1.0344\n",
      "Epoch 13/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0200 - val_loss: 1.0341\n",
      "Epoch 14/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0190 - val_loss: 1.0338\n",
      "Epoch 15/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0180 - val_loss: 1.0335\n",
      "Epoch 16/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0171 - val_loss: 1.0332\n",
      "Epoch 17/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0161 - val_loss: 1.0329\n",
      "Epoch 18/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0151 - val_loss: 1.0326\n",
      "Epoch 19/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0141 - val_loss: 1.0322\n",
      "Epoch 20/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0131 - val_loss: 1.0319\n",
      "Epoch 21/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0121 - val_loss: 1.0316\n",
      "Epoch 22/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0111 - val_loss: 1.0312\n",
      "Epoch 23/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0101 - val_loss: 1.0309\n",
      "Epoch 24/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0091 - val_loss: 1.0305\n",
      "Epoch 25/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0081 - val_loss: 1.0302\n",
      "Epoch 26/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0071 - val_loss: 1.0298\n",
      "Epoch 27/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0061 - val_loss: 1.0294\n",
      "Epoch 28/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0051 - val_loss: 1.0290\n",
      "Epoch 29/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0041 - val_loss: 1.0286\n",
      "Epoch 30/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0031 - val_loss: 1.0282\n",
      "Epoch 31/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0020 - val_loss: 1.0277\n",
      "Epoch 32/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0010 - val_loss: 1.0273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 1.0000 - val_loss: 1.0268\n",
      "Epoch 34/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9989 - val_loss: 1.0263\n",
      "Epoch 35/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9979 - val_loss: 1.0258\n",
      "Epoch 36/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9968 - val_loss: 1.0253\n",
      "Epoch 37/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9957 - val_loss: 1.0248\n",
      "Epoch 38/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9946 - val_loss: 1.0242\n",
      "Epoch 39/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9935 - val_loss: 1.0236\n",
      "Epoch 40/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9924 - val_loss: 1.0230\n",
      "Epoch 41/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9913 - val_loss: 1.0224\n",
      "Epoch 42/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9901 - val_loss: 1.0218\n",
      "Epoch 43/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9890 - val_loss: 1.0211\n",
      "Epoch 44/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9878 - val_loss: 1.0204\n",
      "Epoch 45/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9866 - val_loss: 1.0197\n",
      "Epoch 46/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9853 - val_loss: 1.0190\n",
      "Epoch 47/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9841 - val_loss: 1.0182\n",
      "Epoch 48/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9828 - val_loss: 1.0174\n",
      "Epoch 49/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9815 - val_loss: 1.0166\n",
      "Epoch 50/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9802 - val_loss: 1.0158\n",
      "Epoch 51/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9789 - val_loss: 1.0149\n",
      "Epoch 52/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9775 - val_loss: 1.0140\n",
      "Epoch 53/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9762 - val_loss: 1.0131\n",
      "Epoch 54/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9748 - val_loss: 1.0122\n",
      "Epoch 55/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9733 - val_loss: 1.0112\n",
      "Epoch 56/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9719 - val_loss: 1.0102\n",
      "Epoch 57/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9704 - val_loss: 1.0092\n",
      "Epoch 58/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9689 - val_loss: 1.0081\n",
      "Epoch 59/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9674 - val_loss: 1.0071\n",
      "Epoch 60/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9659 - val_loss: 1.0060\n",
      "Epoch 61/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9643 - val_loss: 1.0048\n",
      "Epoch 62/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9627 - val_loss: 1.0037\n",
      "Epoch 63/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9611 - val_loss: 1.0025\n",
      "Epoch 64/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9595 - val_loss: 1.0013\n",
      "Epoch 65/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9579 - val_loss: 1.0001\n",
      "Epoch 66/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9562 - val_loss: 0.9989\n",
      "Epoch 67/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9545 - val_loss: 0.9977\n",
      "Epoch 68/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9528 - val_loss: 0.9964\n",
      "Epoch 69/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9511 - val_loss: 0.9951\n",
      "Epoch 70/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9494 - val_loss: 0.9938\n",
      "Epoch 71/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9477 - val_loss: 0.9925\n",
      "Epoch 72/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9459 - val_loss: 0.9912\n",
      "Epoch 73/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9442 - val_loss: 0.9899\n",
      "Epoch 74/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9424 - val_loss: 0.9886\n",
      "Epoch 75/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9406 - val_loss: 0.9872\n",
      "Epoch 76/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9388 - val_loss: 0.9859\n",
      "Epoch 77/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9370 - val_loss: 0.9846\n",
      "Epoch 78/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9353 - val_loss: 0.9832\n",
      "Epoch 79/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9335 - val_loss: 0.9819\n",
      "Epoch 80/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9317 - val_loss: 0.9805\n",
      "Epoch 81/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9299 - val_loss: 0.9792\n",
      "Epoch 82/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9281 - val_loss: 0.9779\n",
      "Epoch 83/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9263 - val_loss: 0.9765\n",
      "Epoch 84/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9246 - val_loss: 0.9752\n",
      "Epoch 85/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9228 - val_loss: 0.9739\n",
      "Epoch 86/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9210 - val_loss: 0.9726\n",
      "Epoch 87/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9193 - val_loss: 0.9713\n",
      "Epoch 88/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9175 - val_loss: 0.9700\n",
      "Epoch 89/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9158 - val_loss: 0.9688\n",
      "Epoch 90/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9141 - val_loss: 0.9675\n",
      "Epoch 91/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9124 - val_loss: 0.9663\n",
      "Epoch 92/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9107 - val_loss: 0.9651\n",
      "Epoch 93/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9090 - val_loss: 0.9639\n",
      "Epoch 94/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9074 - val_loss: 0.9627\n",
      "Epoch 95/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9057 - val_loss: 0.9616\n",
      "Epoch 96/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9041 - val_loss: 0.9604\n",
      "Epoch 97/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9025 - val_loss: 0.9593\n",
      "Epoch 98/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.9009 - val_loss: 0.9582\n",
      "Epoch 99/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8993 - val_loss: 0.9571\n",
      "Epoch 100/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8978 - val_loss: 0.9561\n",
      "Epoch 101/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8963 - val_loss: 0.9551\n",
      "Epoch 102/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8947 - val_loss: 0.9541\n",
      "Epoch 103/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8933 - val_loss: 0.9531\n",
      "Epoch 104/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8918 - val_loss: 0.9521\n",
      "Epoch 105/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8903 - val_loss: 0.9512\n",
      "Epoch 106/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8889 - val_loss: 0.9503\n",
      "Epoch 107/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8875 - val_loss: 0.9494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8861 - val_loss: 0.9485\n",
      "Epoch 109/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8847 - val_loss: 0.9477\n",
      "Epoch 110/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8834 - val_loss: 0.9469\n",
      "Epoch 111/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8820 - val_loss: 0.9461\n",
      "Epoch 112/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8807 - val_loss: 0.9453\n",
      "Epoch 113/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8794 - val_loss: 0.9446\n",
      "Epoch 114/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8781 - val_loss: 0.9439\n",
      "Epoch 115/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8769 - val_loss: 0.9431\n",
      "Epoch 116/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8756 - val_loss: 0.9425\n",
      "Epoch 117/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8744 - val_loss: 0.9418\n",
      "Epoch 118/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8732 - val_loss: 0.9412\n",
      "Epoch 119/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8720 - val_loss: 0.9405\n",
      "Epoch 120/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8708 - val_loss: 0.9399\n",
      "Epoch 121/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8697 - val_loss: 0.9394\n",
      "Epoch 122/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8685 - val_loss: 0.9388\n",
      "Epoch 123/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8674 - val_loss: 0.9383\n",
      "Epoch 124/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8663 - val_loss: 0.9378\n",
      "Epoch 125/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8652 - val_loss: 0.9373\n",
      "Epoch 126/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8641 - val_loss: 0.9368\n",
      "Epoch 127/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8630 - val_loss: 0.9363\n",
      "Epoch 128/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8619 - val_loss: 0.9359\n",
      "Epoch 129/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8609 - val_loss: 0.9354\n",
      "Epoch 130/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8599 - val_loss: 0.9350\n",
      "Epoch 131/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8588 - val_loss: 0.9346\n",
      "Epoch 132/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8578 - val_loss: 0.9342\n",
      "Epoch 133/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8568 - val_loss: 0.9339\n",
      "Epoch 134/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8559 - val_loss: 0.9335\n",
      "Epoch 135/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8549 - val_loss: 0.9332\n",
      "Epoch 136/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8539 - val_loss: 0.9329\n",
      "Epoch 137/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8530 - val_loss: 0.9326\n",
      "Epoch 138/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8520 - val_loss: 0.9323\n",
      "Epoch 139/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8511 - val_loss: 0.9321\n",
      "Epoch 140/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8502 - val_loss: 0.9318\n",
      "Epoch 141/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8493 - val_loss: 0.9316\n",
      "Epoch 142/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8484 - val_loss: 0.9313\n",
      "Epoch 143/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8475 - val_loss: 0.9311\n",
      "Epoch 144/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8466 - val_loss: 0.9309\n",
      "Epoch 145/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8458 - val_loss: 0.9307\n",
      "Epoch 146/10000\n",
      "577737/577737 [==============================] - 0s 1us/step - loss: 0.8449 - val_loss: 0.9306\n"
     ]
    }
   ],
   "source": [
    "X_test_incomplete = test_data.drop([\"id\"], axis=1).values\n",
    "X_test = {}\n",
    "X_test_reduced_dim = {}\n",
    "\n",
    "k = 12\n",
    "# Use nearest rows which have a feature to fill in each row's missing features\n",
    "X_test[\"KNN\"] = KNN(k=k).fit_transform(X_test_incomplete)\n",
    "X_test[\"SoftImpute\"] = SoftImpute().fit_transform(BiScaler().fit_transform(X_test_incomplete))\n",
    "X_test[\"MatrixFactorization\"] = MatrixFactorization(learning_rate= 0.001, rank=50).fit_transform(X_test_incomplete)\n",
    "# use the training mean values\n",
    "test_data_mean =  test_data.fillna(train_mean_values)\n",
    "X_test[\"Mean\"] = test_data_mean.drop([\"id\"], axis=1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALE THE DATA\n",
    "for key in X_test:\n",
    "    transformer = RobustScaler().fit(X_test[key])\n",
    "    X_test[key] = transformer.transform(X_test[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECTED THE IMPORTANTANT FEATURES\n",
    "for key in X_test:\n",
    "    X_test_reduced_dim[key] = pd.DataFrame(X_test[key], columns=colnames)\n",
    "    X_test_reduced_dim[key] = X_test_reduced_dim[key][feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ax/miniconda3/envs/aml/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1088 samples, validate on 121 samples\n",
      "Epoch 1/150\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 0.1990 - val_loss: 0.1964\n",
      "Epoch 2/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1985 - val_loss: 0.1958\n",
      "Epoch 3/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1979 - val_loss: 0.1951\n",
      "Epoch 4/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1972 - val_loss: 0.1943\n",
      "Epoch 5/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1963 - val_loss: 0.1933\n",
      "Epoch 6/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1952 - val_loss: 0.1921\n",
      "Epoch 7/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1939 - val_loss: 0.1907\n",
      "Epoch 8/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1923 - val_loss: 0.1889\n",
      "Epoch 9/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1904 - val_loss: 0.1867\n",
      "Epoch 10/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1881 - val_loss: 0.1841\n",
      "Epoch 11/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1853 - val_loss: 0.1810\n",
      "Epoch 12/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1823 - val_loss: 0.1776\n",
      "Epoch 13/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1788 - val_loss: 0.1736\n",
      "Epoch 14/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1748 - val_loss: 0.1690\n",
      "Epoch 15/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1704 - val_loss: 0.1638\n",
      "Epoch 16/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1655 - val_loss: 0.1584\n",
      "Epoch 17/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1601 - val_loss: 0.1528\n",
      "Epoch 18/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1544 - val_loss: 0.1469\n",
      "Epoch 19/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1483 - val_loss: 0.1411\n",
      "Epoch 20/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1419 - val_loss: 0.1351\n",
      "Epoch 21/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1352 - val_loss: 0.1287\n",
      "Epoch 22/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1283 - val_loss: 0.1225\n",
      "Epoch 23/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.1214 - val_loss: 0.1163\n",
      "Epoch 24/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1147 - val_loss: 0.1104\n",
      "Epoch 25/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1083 - val_loss: 0.1050\n",
      "Epoch 26/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1025 - val_loss: 0.1002\n",
      "Epoch 27/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0975 - val_loss: 0.0958\n",
      "Epoch 28/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0933 - val_loss: 0.0920\n",
      "Epoch 29/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0895 - val_loss: 0.0886\n",
      "Epoch 30/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0861 - val_loss: 0.0854\n",
      "Epoch 31/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0833 - val_loss: 0.0826\n",
      "Epoch 32/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0810 - val_loss: 0.0802\n",
      "Epoch 33/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0791 - val_loss: 0.0780\n",
      "Epoch 34/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0777 - val_loss: 0.0763\n",
      "Epoch 35/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0764 - val_loss: 0.0749\n",
      "Epoch 36/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0754 - val_loss: 0.0740\n",
      "Epoch 37/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0746 - val_loss: 0.0730\n",
      "Epoch 38/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0738 - val_loss: 0.0722\n",
      "Epoch 39/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0731 - val_loss: 0.0714\n",
      "Epoch 40/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0725 - val_loss: 0.0709\n",
      "Epoch 41/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0719 - val_loss: 0.0703\n",
      "Epoch 42/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0714 - val_loss: 0.0697\n",
      "Epoch 43/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0708 - val_loss: 0.0692\n",
      "Epoch 44/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0702 - val_loss: 0.0687\n",
      "Epoch 45/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0696 - val_loss: 0.0681\n",
      "Epoch 46/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0691 - val_loss: 0.0676\n",
      "Epoch 47/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0685 - val_loss: 0.0671\n",
      "Epoch 48/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0679 - val_loss: 0.0665\n",
      "Epoch 49/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0674 - val_loss: 0.0660\n",
      "Epoch 50/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0668 - val_loss: 0.0655\n",
      "Epoch 51/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0663 - val_loss: 0.0651\n",
      "Epoch 52/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0658 - val_loss: 0.0648\n",
      "Epoch 53/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0653 - val_loss: 0.0643\n",
      "Epoch 54/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0648 - val_loss: 0.0639\n",
      "Epoch 55/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0643 - val_loss: 0.0635\n",
      "Epoch 56/150\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0638 - val_loss: 0.0630\n",
      "Epoch 57/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0634 - val_loss: 0.0625\n",
      "Epoch 58/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0629 - val_loss: 0.0620\n",
      "Epoch 59/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0624 - val_loss: 0.0615\n",
      "Epoch 60/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0620 - val_loss: 0.0611\n",
      "Epoch 61/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0615 - val_loss: 0.0606\n",
      "Epoch 62/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 63/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0607 - val_loss: 0.0600\n",
      "Epoch 64/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0603 - val_loss: 0.0597\n",
      "Epoch 65/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0599 - val_loss: 0.0594\n",
      "Epoch 66/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0595 - val_loss: 0.0591\n",
      "Epoch 67/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0591 - val_loss: 0.0588\n",
      "Epoch 68/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0588 - val_loss: 0.0585\n",
      "Epoch 69/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0584 - val_loss: 0.0583\n",
      "Epoch 70/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0581 - val_loss: 0.0581\n",
      "Epoch 71/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0577 - val_loss: 0.0579\n",
      "Epoch 72/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0574 - val_loss: 0.0579\n",
      "Epoch 73/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0571 - val_loss: 0.0578\n",
      "Epoch 74/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0568 - val_loss: 0.0577\n",
      "Epoch 75/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0565 - val_loss: 0.0575\n",
      "Epoch 76/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0562 - val_loss: 0.0572\n",
      "Epoch 77/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0559 - val_loss: 0.0570\n",
      "Epoch 78/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0556 - val_loss: 0.0568\n",
      "Epoch 79/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0554 - val_loss: 0.0566\n",
      "Epoch 80/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0551 - val_loss: 0.0564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0549 - val_loss: 0.0563\n",
      "Epoch 82/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0546 - val_loss: 0.0562\n",
      "Epoch 83/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0544 - val_loss: 0.0560\n",
      "Epoch 84/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0542 - val_loss: 0.0559\n",
      "Epoch 85/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0540 - val_loss: 0.0558\n",
      "Epoch 86/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0538 - val_loss: 0.0557\n",
      "Epoch 87/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0536 - val_loss: 0.0556\n",
      "Epoch 88/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0534 - val_loss: 0.0555\n",
      "Epoch 89/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0532 - val_loss: 0.0554\n",
      "Epoch 90/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0530 - val_loss: 0.0553\n",
      "Epoch 91/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0528 - val_loss: 0.0552\n",
      "Epoch 92/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0526 - val_loss: 0.0551\n",
      "Epoch 93/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0524 - val_loss: 0.0551\n",
      "Epoch 94/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0522 - val_loss: 0.0550\n",
      "Epoch 95/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0521 - val_loss: 0.0550\n",
      "Epoch 96/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0519 - val_loss: 0.0549\n",
      "Epoch 97/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0549\n",
      "Epoch 98/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0516 - val_loss: 0.0548\n",
      "Epoch 99/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0548\n",
      "Epoch 100/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0513 - val_loss: 0.0547\n",
      "Epoch 101/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0512 - val_loss: 0.0547\n",
      "Epoch 102/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0510 - val_loss: 0.0547\n",
      "Epoch 103/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0509 - val_loss: 0.0546\n",
      "Epoch 104/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0507 - val_loss: 0.0546\n",
      "Epoch 105/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0506 - val_loss: 0.0546\n",
      "Epoch 106/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0504 - val_loss: 0.0546\n",
      "Epoch 107/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0503 - val_loss: 0.0546\n",
      "Epoch 108/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0501 - val_loss: 0.0546\n",
      "Epoch 109/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0500 - val_loss: 0.0546\n",
      "Epoch 110/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0498 - val_loss: 0.0546\n",
      "Epoch 111/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0496 - val_loss: 0.0546\n",
      "Epoch 112/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0495 - val_loss: 0.0546\n",
      "Epoch 113/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0493 - val_loss: 0.0546\n",
      "Epoch 114/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0492 - val_loss: 0.0546\n",
      "Epoch 115/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0490 - val_loss: 0.0545\n",
      "Epoch 116/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0489 - val_loss: 0.0545\n",
      "Epoch 117/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0488 - val_loss: 0.0545\n",
      "Epoch 118/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0486 - val_loss: 0.0545\n",
      "Epoch 119/150\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0485 - val_loss: 0.0545\n",
      "Epoch 120/150\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0483 - val_loss: 0.0545\n",
      "Epoch 121/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0482 - val_loss: 0.0545\n",
      "Epoch 122/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0480 - val_loss: 0.0545\n",
      "Epoch 123/150\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0479 - val_loss: 0.0545\n",
      "Epoch 124/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0478 - val_loss: 0.0545\n",
      "Epoch 125/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0476 - val_loss: 0.0546\n",
      "Epoch 126/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0475 - val_loss: 0.0546\n",
      "Epoch 127/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0474 - val_loss: 0.0546\n",
      "Epoch 128/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0472 - val_loss: 0.0546\n",
      "Epoch 129/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0471 - val_loss: 0.0546\n",
      "Epoch 130/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0470 - val_loss: 0.0547\n",
      "Epoch 131/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0469 - val_loss: 0.0547\n",
      "Epoch 132/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0467 - val_loss: 0.0547\n",
      "Epoch 133/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0466 - val_loss: 0.0547\n",
      "Epoch 134/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0465 - val_loss: 0.0547\n",
      "Epoch 135/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0464 - val_loss: 0.0547\n",
      "Epoch 136/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0462 - val_loss: 0.0547\n",
      "Epoch 137/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0461 - val_loss: 0.0548\n",
      "Epoch 138/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0460 - val_loss: 0.0548\n",
      "Epoch 139/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0459 - val_loss: 0.0548\n",
      "Epoch 140/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0458 - val_loss: 0.0548\n",
      "Epoch 141/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0456 - val_loss: 0.0548\n",
      "Epoch 142/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0455 - val_loss: 0.0549\n",
      "Epoch 143/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0454 - val_loss: 0.0549\n",
      "Epoch 144/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0453 - val_loss: 0.0549\n",
      "Epoch 145/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0452 - val_loss: 0.0549\n",
      "Epoch 146/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0451 - val_loss: 0.0549\n",
      "Epoch 147/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0450 - val_loss: 0.0549\n",
      "Epoch 148/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0449 - val_loss: 0.0550\n",
      "Epoch 149/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0447 - val_loss: 0.0550\n",
      "Epoch 150/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0446 - val_loss: 0.0550\n",
      "Train on 1090 samples, validate on 122 samples\n",
      "Epoch 1/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1058 - val_loss: 0.0932\n",
      "Epoch 2/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0849 - val_loss: 0.0776\n",
      "Epoch 3/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0760 - val_loss: 0.0705\n",
      "Epoch 4/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0715 - val_loss: 0.0666\n",
      "Epoch 5/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0688 - val_loss: 0.0643\n",
      "Epoch 6/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0670 - val_loss: 0.0626\n",
      "Epoch 7/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0656 - val_loss: 0.0613\n",
      "Epoch 8/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0645 - val_loss: 0.0602\n",
      "Epoch 9/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0636 - val_loss: 0.0592\n",
      "Epoch 10/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0628 - val_loss: 0.0585\n",
      "Epoch 11/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0621 - val_loss: 0.0578\n",
      "Epoch 12/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0614 - val_loss: 0.0571\n",
      "Epoch 13/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0608 - val_loss: 0.0565\n",
      "Epoch 14/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0603 - val_loss: 0.0559\n",
      "Epoch 15/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0597 - val_loss: 0.0554\n",
      "Epoch 16/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0592 - val_loss: 0.0551\n",
      "Epoch 17/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0588 - val_loss: 0.0547\n",
      "Epoch 18/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0583 - val_loss: 0.0543\n",
      "Epoch 19/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0579 - val_loss: 0.0539\n",
      "Epoch 20/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0575 - val_loss: 0.0536\n",
      "Epoch 21/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0571 - val_loss: 0.0533\n",
      "Epoch 22/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0567 - val_loss: 0.0530\n",
      "Epoch 23/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0564 - val_loss: 0.0528\n",
      "Epoch 24/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0560 - val_loss: 0.0527\n",
      "Epoch 25/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0557 - val_loss: 0.0526\n",
      "Epoch 26/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0554 - val_loss: 0.0524\n",
      "Epoch 27/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0551 - val_loss: 0.0523\n",
      "Epoch 28/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0548 - val_loss: 0.0523\n",
      "Epoch 29/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0546 - val_loss: 0.0522\n",
      "Epoch 30/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0543 - val_loss: 0.0522\n",
      "Epoch 31/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0540 - val_loss: 0.0522\n",
      "Epoch 32/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0537 - val_loss: 0.0521\n",
      "Epoch 33/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0535 - val_loss: 0.0521\n",
      "Epoch 34/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0532 - val_loss: 0.0521\n",
      "Epoch 35/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0529 - val_loss: 0.0520\n",
      "Epoch 36/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0527 - val_loss: 0.0520\n",
      "Epoch 37/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0525 - val_loss: 0.0519\n",
      "Epoch 38/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0522 - val_loss: 0.0519\n",
      "Epoch 39/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0520 - val_loss: 0.0518\n",
      "Epoch 40/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0517\n",
      "Epoch 41/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0516 - val_loss: 0.0517\n",
      "Epoch 42/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0514 - val_loss: 0.0516\n",
      "Epoch 43/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0512 - val_loss: 0.0516\n",
      "Epoch 44/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0510 - val_loss: 0.0515\n",
      "Epoch 45/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0508 - val_loss: 0.0514\n",
      "Epoch 46/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0505 - val_loss: 0.0514\n",
      "Epoch 47/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0503 - val_loss: 0.0514\n",
      "Epoch 48/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0501 - val_loss: 0.0514\n",
      "Epoch 49/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0499 - val_loss: 0.0514\n",
      "Epoch 50/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0497 - val_loss: 0.0514\n",
      "Epoch 51/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0495 - val_loss: 0.0513\n",
      "Epoch 52/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0493 - val_loss: 0.0512\n",
      "Epoch 53/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0491 - val_loss: 0.0511\n",
      "Epoch 54/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0489 - val_loss: 0.0511\n",
      "Epoch 55/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0487 - val_loss: 0.0510\n",
      "Epoch 56/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0485 - val_loss: 0.0510\n",
      "Epoch 57/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0483 - val_loss: 0.0510\n",
      "Epoch 58/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0481 - val_loss: 0.0509\n",
      "Epoch 59/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0479 - val_loss: 0.0508\n",
      "Epoch 60/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0477 - val_loss: 0.0508\n",
      "Epoch 61/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0475 - val_loss: 0.0507\n",
      "Epoch 62/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0473 - val_loss: 0.0507\n",
      "Epoch 63/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0471 - val_loss: 0.0507\n",
      "Epoch 64/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0469 - val_loss: 0.0506\n",
      "Epoch 65/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0467 - val_loss: 0.0506\n",
      "Epoch 66/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0466 - val_loss: 0.0505\n",
      "Epoch 67/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0464 - val_loss: 0.0504\n",
      "Epoch 68/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0462 - val_loss: 0.0504\n",
      "Epoch 69/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0460 - val_loss: 0.0504\n",
      "Epoch 70/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0458 - val_loss: 0.0503\n",
      "Epoch 71/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0456 - val_loss: 0.0502\n",
      "Epoch 72/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0455 - val_loss: 0.0502\n",
      "Epoch 73/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0453 - val_loss: 0.0502\n",
      "Epoch 74/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0451 - val_loss: 0.0502\n",
      "Epoch 75/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0449 - val_loss: 0.0502\n",
      "Epoch 76/150\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0447 - val_loss: 0.0502\n",
      "Epoch 77/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0446 - val_loss: 0.0502\n",
      "Epoch 78/150\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0444 - val_loss: 0.0502\n",
      "Epoch 79/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0442 - val_loss: 0.0502\n",
      "Epoch 80/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0440 - val_loss: 0.0502\n",
      "Epoch 81/150\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0439 - val_loss: 0.0502\n",
      "Epoch 82/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0437 - val_loss: 0.0502\n",
      "Epoch 83/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0436 - val_loss: 0.0502\n",
      "Epoch 84/150\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0434 - val_loss: 0.0501\n",
      "Epoch 85/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0433 - val_loss: 0.0501\n",
      "Epoch 86/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0431 - val_loss: 0.0501\n",
      "Epoch 87/150\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0430 - val_loss: 0.0501\n",
      "Epoch 88/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0429 - val_loss: 0.0500\n",
      "Epoch 89/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0427 - val_loss: 0.0500\n",
      "Epoch 90/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0426 - val_loss: 0.0500\n",
      "Epoch 91/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0424 - val_loss: 0.0500\n",
      "Epoch 92/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0423 - val_loss: 0.0499\n",
      "Epoch 93/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0422 - val_loss: 0.0499\n",
      "Epoch 94/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0421 - val_loss: 0.0499\n",
      "Epoch 95/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0419 - val_loss: 0.0499\n",
      "Epoch 96/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0418 - val_loss: 0.0499\n",
      "Epoch 97/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0417 - val_loss: 0.0499\n",
      "Epoch 98/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0415 - val_loss: 0.0499\n",
      "Epoch 99/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0414 - val_loss: 0.0499\n",
      "Epoch 100/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0413 - val_loss: 0.0499\n",
      "Epoch 101/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0412 - val_loss: 0.0499\n",
      "Epoch 102/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0411 - val_loss: 0.0498\n",
      "Epoch 103/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0409 - val_loss: 0.0498\n",
      "Epoch 104/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0408 - val_loss: 0.0498\n",
      "Epoch 105/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0407 - val_loss: 0.0498\n",
      "Epoch 106/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0406 - val_loss: 0.0498\n",
      "Epoch 107/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0405 - val_loss: 0.0498\n",
      "Epoch 108/150\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0404 - val_loss: 0.0498\n",
      "Epoch 109/150\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0403 - val_loss: 0.0498\n",
      "Epoch 110/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0402 - val_loss: 0.0498\n",
      "Epoch 111/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0401 - val_loss: 0.0498\n",
      "Epoch 112/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0400 - val_loss: 0.0498\n",
      "Epoch 113/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0399 - val_loss: 0.0499\n",
      "Epoch 114/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0398 - val_loss: 0.0499\n",
      "Epoch 115/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0397 - val_loss: 0.0499\n",
      "Epoch 116/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0396 - val_loss: 0.0499\n",
      "Epoch 117/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0395 - val_loss: 0.0500\n",
      "Epoch 118/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0394 - val_loss: 0.0500\n",
      "Epoch 119/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0393 - val_loss: 0.0501\n",
      "Epoch 120/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0392 - val_loss: 0.0501\n",
      "Epoch 121/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0391 - val_loss: 0.0501\n",
      "Epoch 122/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0390 - val_loss: 0.0502\n",
      "Epoch 123/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0389 - val_loss: 0.0502\n",
      "Epoch 124/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0389 - val_loss: 0.0502\n",
      "Epoch 125/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0388 - val_loss: 0.0502\n",
      "Epoch 126/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0387 - val_loss: 0.0502\n",
      "Epoch 127/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0386 - val_loss: 0.0503\n",
      "Epoch 128/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0386 - val_loss: 0.0503\n",
      "Epoch 129/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0385 - val_loss: 0.0503\n",
      "Epoch 130/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0384 - val_loss: 0.0503\n",
      "Epoch 131/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0383 - val_loss: 0.0504\n",
      "Epoch 132/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0383 - val_loss: 0.0504\n",
      "Epoch 133/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0382 - val_loss: 0.0504\n",
      "Epoch 134/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0381 - val_loss: 0.0504\n",
      "Epoch 135/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0381 - val_loss: 0.0504\n",
      "Epoch 136/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0380 - val_loss: 0.0505\n",
      "Epoch 137/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0379 - val_loss: 0.0505\n",
      "Epoch 138/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0379 - val_loss: 0.0505\n",
      "Epoch 139/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0378 - val_loss: 0.0505\n",
      "Epoch 140/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0378 - val_loss: 0.0505\n",
      "Epoch 141/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0377 - val_loss: 0.0506\n",
      "Epoch 142/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0376 - val_loss: 0.0506\n",
      "Epoch 143/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0376 - val_loss: 0.0506\n",
      "Epoch 144/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0375 - val_loss: 0.0506\n",
      "Epoch 145/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0375 - val_loss: 0.0506\n",
      "Epoch 146/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0374 - val_loss: 0.0506\n",
      "Epoch 147/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0373 - val_loss: 0.0506\n",
      "Epoch 148/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0373 - val_loss: 0.0506\n",
      "Epoch 149/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0372 - val_loss: 0.0506\n",
      "Epoch 150/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0372 - val_loss: 0.0507\n",
      "Train on 1087 samples, validate on 121 samples\n",
      "Epoch 1/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0859 - val_loss: 0.0804\n",
      "Epoch 2/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0723 - val_loss: 0.0728\n",
      "Epoch 3/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0691 - val_loss: 0.0697\n",
      "Epoch 4/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0669 - val_loss: 0.0675\n",
      "Epoch 5/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0652 - val_loss: 0.0658\n",
      "Epoch 6/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0639 - val_loss: 0.0644\n",
      "Epoch 7/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0628 - val_loss: 0.0633\n",
      "Epoch 8/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0619 - val_loss: 0.0623\n",
      "Epoch 9/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0611 - val_loss: 0.0614\n",
      "Epoch 10/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0604 - val_loss: 0.0606\n",
      "Epoch 11/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0597 - val_loss: 0.0599\n",
      "Epoch 12/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0591 - val_loss: 0.0593\n",
      "Epoch 13/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0586 - val_loss: 0.0587\n",
      "Epoch 14/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0580 - val_loss: 0.0583\n",
      "Epoch 15/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0575 - val_loss: 0.0579\n",
      "Epoch 16/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0570 - val_loss: 0.0575\n",
      "Epoch 17/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0565 - val_loss: 0.0571\n",
      "Epoch 18/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0561 - val_loss: 0.0569\n",
      "Epoch 19/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0556 - val_loss: 0.0567\n",
      "Epoch 20/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0552 - val_loss: 0.0564\n",
      "Epoch 21/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0548 - val_loss: 0.0562\n",
      "Epoch 22/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0544 - val_loss: 0.0559\n",
      "Epoch 23/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0541 - val_loss: 0.0557\n",
      "Epoch 24/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0537 - val_loss: 0.0555\n",
      "Epoch 25/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0534 - val_loss: 0.0554\n",
      "Epoch 26/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0531 - val_loss: 0.0553\n",
      "Epoch 27/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0528 - val_loss: 0.0552\n",
      "Epoch 28/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0524 - val_loss: 0.0550\n",
      "Epoch 29/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0521 - val_loss: 0.0548\n",
      "Epoch 30/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0546\n",
      "Epoch 31/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0515 - val_loss: 0.0545\n",
      "Epoch 32/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0513 - val_loss: 0.0544\n",
      "Epoch 33/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0510 - val_loss: 0.0544\n",
      "Epoch 34/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0507 - val_loss: 0.0543\n",
      "Epoch 35/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0504 - val_loss: 0.0542\n",
      "Epoch 36/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0501 - val_loss: 0.0540\n",
      "Epoch 37/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0499 - val_loss: 0.0539\n",
      "Epoch 38/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0496 - val_loss: 0.0539\n",
      "Epoch 39/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0494 - val_loss: 0.0539\n",
      "Epoch 40/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0491 - val_loss: 0.0539\n",
      "Epoch 41/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0489 - val_loss: 0.0539\n",
      "Epoch 42/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0486 - val_loss: 0.0540\n",
      "Epoch 43/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0484 - val_loss: 0.0540\n",
      "Epoch 44/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0481 - val_loss: 0.0540\n",
      "Epoch 45/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0479 - val_loss: 0.0539\n",
      "Epoch 46/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0477 - val_loss: 0.0538\n",
      "Epoch 47/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0475 - val_loss: 0.0537\n",
      "Epoch 48/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0473 - val_loss: 0.0537\n",
      "Epoch 49/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0472 - val_loss: 0.0536\n",
      "Epoch 50/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0470 - val_loss: 0.0535\n",
      "Epoch 51/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0468 - val_loss: 0.0535\n",
      "Epoch 52/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0467 - val_loss: 0.0534\n",
      "Epoch 53/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0465 - val_loss: 0.0533\n",
      "Epoch 54/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0463 - val_loss: 0.0533\n",
      "Epoch 55/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0461 - val_loss: 0.0532\n",
      "Epoch 56/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0460 - val_loss: 0.0531\n",
      "Epoch 57/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0458 - val_loss: 0.0530\n",
      "Epoch 58/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0457 - val_loss: 0.0530\n",
      "Epoch 59/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0455 - val_loss: 0.0529\n",
      "Epoch 60/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0454 - val_loss: 0.0529\n",
      "Epoch 61/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0452 - val_loss: 0.0528\n",
      "Epoch 62/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0450 - val_loss: 0.0528\n",
      "Epoch 63/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0449 - val_loss: 0.0527\n",
      "Epoch 64/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0447 - val_loss: 0.0526\n",
      "Epoch 65/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0446 - val_loss: 0.0525\n",
      "Epoch 66/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0444 - val_loss: 0.0525\n",
      "Epoch 67/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0443 - val_loss: 0.0524\n",
      "Epoch 68/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0441 - val_loss: 0.0523\n",
      "Epoch 69/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0440 - val_loss: 0.0523\n",
      "Epoch 70/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0438 - val_loss: 0.0522\n",
      "Epoch 71/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0437 - val_loss: 0.0521\n",
      "Epoch 72/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0435 - val_loss: 0.0521\n",
      "Epoch 73/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0434 - val_loss: 0.0520\n",
      "Epoch 74/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0433 - val_loss: 0.0519\n",
      "Epoch 75/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0431 - val_loss: 0.0519\n",
      "Epoch 76/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0430 - val_loss: 0.0518\n",
      "Epoch 77/150\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0428 - val_loss: 0.0517\n",
      "Epoch 78/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0427 - val_loss: 0.0517\n",
      "Epoch 79/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0426 - val_loss: 0.0516\n",
      "Epoch 80/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0425 - val_loss: 0.0515\n",
      "Epoch 81/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0423 - val_loss: 0.0514\n",
      "Epoch 82/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0422 - val_loss: 0.0514\n",
      "Epoch 83/150\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0421 - val_loss: 0.0513\n",
      "Epoch 84/150\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0420 - val_loss: 0.0513\n",
      "Epoch 85/150\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0419 - val_loss: 0.0512\n",
      "Epoch 86/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0418 - val_loss: 0.0511\n",
      "Epoch 87/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0416 - val_loss: 0.0511\n",
      "Epoch 88/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0415 - val_loss: 0.0511\n",
      "Epoch 89/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0414 - val_loss: 0.0511\n",
      "Epoch 90/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0413 - val_loss: 0.0510\n",
      "Epoch 91/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0412 - val_loss: 0.0510\n",
      "Epoch 92/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0411 - val_loss: 0.0510\n",
      "Epoch 93/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0410 - val_loss: 0.0510\n",
      "Epoch 94/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0408 - val_loss: 0.0510\n",
      "Epoch 95/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0407 - val_loss: 0.0510\n",
      "Epoch 96/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0406 - val_loss: 0.0510\n",
      "Epoch 97/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0405 - val_loss: 0.0510\n",
      "Epoch 98/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0404 - val_loss: 0.0510\n",
      "Epoch 99/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0403 - val_loss: 0.0510\n",
      "Epoch 100/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0402 - val_loss: 0.0510\n",
      "Epoch 101/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0401 - val_loss: 0.0509\n",
      "Epoch 102/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0400 - val_loss: 0.0509\n",
      "Epoch 103/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0399 - val_loss: 0.0509\n",
      "Epoch 104/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0397 - val_loss: 0.0509\n",
      "Epoch 105/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0396 - val_loss: 0.0509\n",
      "Epoch 106/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0395 - val_loss: 0.0509\n",
      "Epoch 107/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0395 - val_loss: 0.0509\n",
      "Epoch 108/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0394 - val_loss: 0.0509\n",
      "Epoch 109/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0393 - val_loss: 0.0509\n",
      "Epoch 110/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0392 - val_loss: 0.0509\n",
      "Epoch 111/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0391 - val_loss: 0.0509\n",
      "Epoch 112/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0390 - val_loss: 0.0509\n",
      "Epoch 113/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0389 - val_loss: 0.0508\n",
      "Epoch 114/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0388 - val_loss: 0.0508\n",
      "Epoch 115/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0387 - val_loss: 0.0508\n",
      "Epoch 116/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0386 - val_loss: 0.0508\n",
      "Epoch 117/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0385 - val_loss: 0.0508\n",
      "Epoch 118/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0384 - val_loss: 0.0508\n",
      "Epoch 119/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0384 - val_loss: 0.0508\n",
      "Epoch 120/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0383 - val_loss: 0.0508\n",
      "Epoch 121/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0382 - val_loss: 0.0508\n",
      "Epoch 122/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0381 - val_loss: 0.0508\n",
      "Epoch 123/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0380 - val_loss: 0.0508\n",
      "Epoch 124/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0379 - val_loss: 0.0508\n",
      "Epoch 125/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0378 - val_loss: 0.0508\n",
      "Epoch 126/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0377 - val_loss: 0.0508\n",
      "Epoch 127/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0377 - val_loss: 0.0509\n",
      "Epoch 128/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0376 - val_loss: 0.0509\n",
      "Epoch 129/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0375 - val_loss: 0.0509\n",
      "Epoch 130/150\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0374 - val_loss: 0.0509\n",
      "Epoch 131/150\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0373 - val_loss: 0.0509\n",
      "Epoch 132/150\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0372 - val_loss: 0.0510\n",
      "Epoch 133/150\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0372 - val_loss: 0.0510\n",
      "Epoch 134/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0371 - val_loss: 0.0510\n",
      "Epoch 135/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0370 - val_loss: 0.0510\n",
      "Epoch 136/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0369 - val_loss: 0.0511\n",
      "Epoch 137/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0369 - val_loss: 0.0511\n",
      "Epoch 138/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0368 - val_loss: 0.0511\n",
      "Epoch 139/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0367 - val_loss: 0.0511\n",
      "Epoch 140/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0366 - val_loss: 0.0512\n",
      "Epoch 141/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0365 - val_loss: 0.0512\n",
      "Epoch 142/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0365 - val_loss: 0.0512\n",
      "Epoch 143/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0364 - val_loss: 0.0513\n",
      "Epoch 144/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0363 - val_loss: 0.0513\n",
      "Epoch 145/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0362 - val_loss: 0.0513\n",
      "Epoch 146/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0362 - val_loss: 0.0513\n",
      "Epoch 147/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0361 - val_loss: 0.0514\n",
      "Epoch 148/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0360 - val_loss: 0.0514\n",
      "Epoch 149/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0359 - val_loss: 0.0514\n",
      "Epoch 150/150\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0359 - val_loss: 0.0514\n",
      "Train on 1084 samples, validate on 121 samples\n",
      "Epoch 1/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0978 - val_loss: 0.0829\n",
      "Epoch 2/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0868 - val_loss: 0.0769\n",
      "Epoch 3/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0814 - val_loss: 0.0731\n",
      "Epoch 4/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0777 - val_loss: 0.0700\n",
      "Epoch 5/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0746 - val_loss: 0.0674\n",
      "Epoch 6/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0721 - val_loss: 0.0653\n",
      "Epoch 7/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0699 - val_loss: 0.0635\n",
      "Epoch 8/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0681 - val_loss: 0.0622\n",
      "Epoch 9/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0666 - val_loss: 0.0612\n",
      "Epoch 10/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0652 - val_loss: 0.0606\n",
      "Epoch 11/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0640 - val_loss: 0.0600\n",
      "Epoch 12/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0630 - val_loss: 0.0594\n",
      "Epoch 13/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0620 - val_loss: 0.0589\n",
      "Epoch 14/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0612 - val_loss: 0.0584\n",
      "Epoch 15/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0603 - val_loss: 0.0578\n",
      "Epoch 16/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0596 - val_loss: 0.0572\n",
      "Epoch 17/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0588 - val_loss: 0.0567\n",
      "Epoch 18/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0581 - val_loss: 0.0563\n",
      "Epoch 19/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0574 - val_loss: 0.0559\n",
      "Epoch 20/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0568 - val_loss: 0.0556\n",
      "Epoch 21/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0562 - val_loss: 0.0553\n",
      "Epoch 22/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0556 - val_loss: 0.0551\n",
      "Epoch 23/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0550 - val_loss: 0.0549\n",
      "Epoch 24/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0545 - val_loss: 0.0547\n",
      "Epoch 25/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0540 - val_loss: 0.0546\n",
      "Epoch 26/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0534 - val_loss: 0.0545\n",
      "Epoch 27/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0529 - val_loss: 0.0543\n",
      "Epoch 28/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0525 - val_loss: 0.0541\n",
      "Epoch 29/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0520 - val_loss: 0.0540\n",
      "Epoch 30/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0515 - val_loss: 0.0538\n",
      "Epoch 31/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0511 - val_loss: 0.0536\n",
      "Epoch 32/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0507 - val_loss: 0.0535\n",
      "Epoch 33/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0503 - val_loss: 0.0534\n",
      "Epoch 34/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0499 - val_loss: 0.0533\n",
      "Epoch 35/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0495 - val_loss: 0.0532\n",
      "Epoch 36/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0492 - val_loss: 0.0531\n",
      "Epoch 37/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0488 - val_loss: 0.0530\n",
      "Epoch 38/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0485 - val_loss: 0.0529\n",
      "Epoch 39/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0482 - val_loss: 0.0529\n",
      "Epoch 40/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0479 - val_loss: 0.0528\n",
      "Epoch 41/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0475 - val_loss: 0.0527\n",
      "Epoch 42/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0472 - val_loss: 0.0526\n",
      "Epoch 43/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0470 - val_loss: 0.0525\n",
      "Epoch 44/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0467 - val_loss: 0.0524\n",
      "Epoch 45/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0464 - val_loss: 0.0523\n",
      "Epoch 46/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0461 - val_loss: 0.0522\n",
      "Epoch 47/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0458 - val_loss: 0.0521\n",
      "Epoch 48/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0456 - val_loss: 0.0520\n",
      "Epoch 49/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0453 - val_loss: 0.0519\n",
      "Epoch 50/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0450 - val_loss: 0.0519\n",
      "Epoch 51/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0447 - val_loss: 0.0518\n",
      "Epoch 52/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0445 - val_loss: 0.0517\n",
      "Epoch 53/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0442 - val_loss: 0.0516\n",
      "Epoch 54/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0440 - val_loss: 0.0515\n",
      "Epoch 55/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0437 - val_loss: 0.0514\n",
      "Epoch 56/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0435 - val_loss: 0.0513\n",
      "Epoch 57/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0433 - val_loss: 0.0512\n",
      "Epoch 58/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0430 - val_loss: 0.0512\n",
      "Epoch 59/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0428 - val_loss: 0.0511\n",
      "Epoch 60/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0426 - val_loss: 0.0510\n",
      "Epoch 61/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0424 - val_loss: 0.0510\n",
      "Epoch 62/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0422 - val_loss: 0.0509\n",
      "Epoch 63/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0420 - val_loss: 0.0509\n",
      "Epoch 64/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0418 - val_loss: 0.0509\n",
      "Epoch 65/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0416 - val_loss: 0.0508\n",
      "Epoch 66/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0414 - val_loss: 0.0508\n",
      "Epoch 67/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0412 - val_loss: 0.0508\n",
      "Epoch 68/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0411 - val_loss: 0.0508\n",
      "Epoch 69/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0409 - val_loss: 0.0508\n",
      "Epoch 70/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0407 - val_loss: 0.0508\n",
      "Epoch 71/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0406 - val_loss: 0.0508\n",
      "Epoch 72/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0404 - val_loss: 0.0508\n",
      "Epoch 73/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0403 - val_loss: 0.0508\n",
      "Epoch 74/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0401 - val_loss: 0.0508\n",
      "Epoch 75/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0400 - val_loss: 0.0508\n",
      "Epoch 76/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0398 - val_loss: 0.0508\n",
      "Epoch 77/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0397 - val_loss: 0.0508\n",
      "Epoch 78/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0395 - val_loss: 0.0508\n",
      "Epoch 79/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0394 - val_loss: 0.0507\n",
      "Epoch 80/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0392 - val_loss: 0.0507\n",
      "Epoch 81/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0391 - val_loss: 0.0507\n",
      "Epoch 82/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0390 - val_loss: 0.0508\n",
      "Epoch 83/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0388 - val_loss: 0.0508\n",
      "Epoch 84/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0387 - val_loss: 0.0508\n",
      "Epoch 85/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0385 - val_loss: 0.0508\n",
      "Epoch 86/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0384 - val_loss: 0.0508\n",
      "Epoch 87/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0382 - val_loss: 0.0509\n",
      "Epoch 88/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0381 - val_loss: 0.0509\n",
      "Epoch 89/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0380 - val_loss: 0.0509\n",
      "Epoch 90/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0378 - val_loss: 0.0510\n",
      "Epoch 91/150\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0377 - val_loss: 0.0510\n",
      "Epoch 92/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0375 - val_loss: 0.0510\n",
      "Epoch 93/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0374 - val_loss: 0.0511\n",
      "Epoch 94/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0372 - val_loss: 0.0511\n",
      "Epoch 95/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0371 - val_loss: 0.0512\n",
      "Epoch 96/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0370 - val_loss: 0.0512\n",
      "Epoch 97/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0368 - val_loss: 0.0513\n",
      "Epoch 98/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0367 - val_loss: 0.0513\n",
      "Epoch 99/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0365 - val_loss: 0.0513\n",
      "Epoch 100/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0364 - val_loss: 0.0513\n",
      "Epoch 101/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0363 - val_loss: 0.0513\n",
      "Epoch 102/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0362 - val_loss: 0.0513\n",
      "Epoch 103/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0360 - val_loss: 0.0514\n",
      "Epoch 104/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0359 - val_loss: 0.0514\n",
      "Epoch 105/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0358 - val_loss: 0.0514\n",
      "Epoch 106/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0356 - val_loss: 0.0515\n",
      "Epoch 107/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0355 - val_loss: 0.0515\n",
      "Epoch 108/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0354 - val_loss: 0.0515\n",
      "Epoch 109/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0353 - val_loss: 0.0515\n",
      "Epoch 110/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0351 - val_loss: 0.0515\n",
      "Epoch 111/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0350 - val_loss: 0.0516\n",
      "Epoch 112/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0349 - val_loss: 0.0516\n",
      "Epoch 113/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0348 - val_loss: 0.0517\n",
      "Epoch 114/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0347 - val_loss: 0.0517\n",
      "Epoch 115/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0345 - val_loss: 0.0517\n",
      "Epoch 116/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0344 - val_loss: 0.0517\n",
      "Epoch 117/150\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0343 - val_loss: 0.0517\n",
      "Epoch 118/150\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.0342 - val_loss: 0.0518\n",
      "Epoch 119/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0341 - val_loss: 0.0518\n",
      "Epoch 120/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0340 - val_loss: 0.0518\n",
      "Epoch 121/150\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0339 - val_loss: 0.0519\n",
      "Epoch 122/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0338 - val_loss: 0.0519\n",
      "Epoch 123/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0336 - val_loss: 0.0519\n",
      "Epoch 124/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0335 - val_loss: 0.0520\n",
      "Epoch 125/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0334 - val_loss: 0.0520\n",
      "Epoch 126/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0333 - val_loss: 0.0520\n",
      "Epoch 127/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0332 - val_loss: 0.0521\n",
      "Epoch 128/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0331 - val_loss: 0.0521\n",
      "Epoch 129/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0330 - val_loss: 0.0522\n",
      "Epoch 130/150\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0329 - val_loss: 0.0522\n",
      "Epoch 131/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0328 - val_loss: 0.0522\n",
      "Epoch 132/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0327 - val_loss: 0.0523\n",
      "Epoch 133/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0326 - val_loss: 0.0523\n",
      "Epoch 134/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0325 - val_loss: 0.0523\n",
      "Epoch 135/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0324 - val_loss: 0.0524\n",
      "Epoch 136/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0323 - val_loss: 0.0524\n",
      "Epoch 137/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0322 - val_loss: 0.0524\n",
      "Epoch 138/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0321 - val_loss: 0.0525\n",
      "Epoch 139/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0320 - val_loss: 0.0525\n",
      "Epoch 140/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0319 - val_loss: 0.0526\n",
      "Epoch 141/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0318 - val_loss: 0.0526\n",
      "Epoch 142/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0317 - val_loss: 0.0527\n",
      "Epoch 143/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0316 - val_loss: 0.0527\n",
      "Epoch 144/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0316 - val_loss: 0.0528\n",
      "Epoch 145/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0315 - val_loss: 0.0528\n",
      "Epoch 146/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0314 - val_loss: 0.0528\n",
      "Epoch 147/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0313 - val_loss: 0.0529\n",
      "Epoch 148/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0312 - val_loss: 0.0529\n",
      "Epoch 149/150\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0311 - val_loss: 0.0529\n",
      "Epoch 150/150\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0311 - val_loss: 0.0530\n"
     ]
    }
   ],
   "source": [
    "predictions = {}\n",
    "for key in X_reduced_dim:\n",
    "    model.fit(x=X_reduced_dim[key].drop([\"y\"], axis=1).as_matrix(), \n",
    "              y=X_reduced_dim[key][\"y\"].values / 100.0, epochs=150, \n",
    "              verbose=1, \n",
    "              validation_split=0.1, \n",
    "              shuffle=True, \n",
    "              steps_per_epoch=100, initial_epoch=0, validation_steps=10)\n",
    "    # calculate predictions\n",
    "    \n",
    "    predictions[key] = model.predict(X_test_reduced_dim[key].as_matrix()) * 100.0\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in predictions:\n",
    "    submission =  pd.DataFrame(data={\"id\": list(range(776)), \"y\": np.squeeze(list(predictions[key]))}) \n",
    "    submission.to_csv(\"submissions/{}_NN.csv\".format(key), index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 50\n",
      "building tree 2 of 50\n",
      "building tree 3 of 50\n",
      "building tree 4 of 50\n",
      "building tree 5 of 50\n",
      "building tree 6 of 50\n",
      "building tree 7 of 50\n",
      "building tree 8 of 50\n",
      "building tree 9 of 50\n",
      "building tree 10 of 50\n",
      "building tree 11 of 50\n",
      "building tree 12 of 50\n",
      "building tree 13 of 50\n",
      "building tree 14 of 50\n",
      "building tree 15 of 50\n",
      "building tree 16 of 50\n",
      "building tree 17 of 50\n",
      "building tree 18 of 50\n",
      "building tree 19 of 50\n",
      "building tree 20 of 50\n",
      "building tree 21 of 50\n",
      "building tree 22 of 50\n",
      "building tree 23 of 50\n",
      "building tree 24 of 50\n",
      "building tree 25 of 50\n",
      "building tree 26 of 50\n",
      "building tree 27 of 50\n",
      "building tree 28 of 50\n",
      "building tree 29 of 50\n",
      "building tree 30 of 50\n",
      "building tree 31 of 50\n",
      "building tree 32 of 50\n",
      "building tree 33 of 50\n",
      "building tree 34 of 50\n",
      "building tree 35 of 50\n",
      "building tree 36 of 50\n",
      "building tree 37 of 50\n",
      "building tree 38 of 50\n",
      "building tree 39 of 50\n",
      "building tree 40 of 50\n",
      "building tree 41 of 50\n",
      "building tree 42 of 50\n",
      "building tree 43 of 50building tree 44 of 50\n",
      "building tree 45 of 50\n",
      "building tree 46 of 50\n",
      "building tree 47 of 50\n",
      "building tree 48 of 50\n",
      "\n",
      "building tree 49 of 50\n",
      "building tree 50 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1,\n",
       "           oob_score=False, random_state=None, verbose=3, warm_start=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_subset = train_subset.drop([\"id\", \"y\"], axis=1).as_matrix()\n",
    "Y_subset = train_subset[\"y\"].values\n",
    "rf = RandomForestRegressor(n_jobs=-1, n_estimators=50, verbose=3)\n",
    "rf.fit(X_subset,Y_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# load the test set\n",
    "test_set =  pd.read_csv(TEST_FILE_PATH)\n",
    "test_set = target.fillna(train_mean_values)\n",
    "test_set_sub = test_set[feature_list]\n",
    "\n",
    "y_pred = rf.predict(test_set_sub.as_matrix())\n",
    "test_set[\"y\"] = y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission\n",
    "test_set[[\"id\", \"y\"]].to_csv(\"submissions/first.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aml-3)",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
