{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 17:14:51) \n",
      "[GCC 7.2.0]\n",
      "scipy: 1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# packages to load \n",
    "# Check the versions of libraries\n",
    "# Python version\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "print('Python: {}'.format(sys.version))\n",
    "\n",
    "import scipy\n",
    "print('scipy: {}'.format(scipy.__version__))\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# Importing metrics for evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from keras.callbacks import (TensorBoard, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint)\n",
    "\n",
    "import math\n",
    "from itertools import product\n",
    "TRAIN_FILE_PATH = \"data/X_train.csv\"\n",
    "TARGET_FILE_PATH =  \"data/y_train.csv\"\n",
    "TEST_FILE_PATH = \"data/X_test.csv\"\n",
    "\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "\n",
    "\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load train and test set\n",
    "\n",
    "train_data = pd.read_csv(TRAIN_FILE_PATH)\n",
    "train_data.drop(\"id\", axis=1, inplace=True)\n",
    "\n",
    "Y_train = pd.read_csv(TARGET_FILE_PATH)\n",
    "Y_train.drop(\"id\", axis=1, inplace = True)\n",
    "\n",
    "test_data =  pd.read_csv(TEST_FILE_PATH)\n",
    "id_test = test_data.columns[0]\n",
    "test_data.drop(\"id\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ================ FUNCTION DEFS ================ ##\n",
    "\n",
    "#Zero mean unit variance for train and test data\n",
    "def scale_data(train, test):\n",
    "    \n",
    "    print(\"Train shape: \", train.shape)\n",
    "    print(\"Test shape: \",test.shape)\n",
    "    \n",
    "    scaler = StandardScaler().fit(train, Y_train)\n",
    "    train = scaler.transform(train)\n",
    "    test = scaler.transform(test)\n",
    "   \n",
    "    return train, test\n",
    "\n",
    "def make_submission(filename, predictions):\n",
    "    test_data =  pd.read_csv(TEST_FILE_PATH)\n",
    "    test_data[\"y\"] = predictions\n",
    "    test_data[[\"id\", \"y\"]].to_csv(\"submissions/\"+filename, index= False)\n",
    "    \n",
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "@as_keras_metric\n",
    "def bmac_metric(Y_true, Y_pred):\n",
    "    return tf.metrics.mean_per_class_accuracy(Y_true, Y_pred, 3)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# define baseline model\n",
    "def baseline_model(weights=[], dropout=0.5, lambda_reg=2.0):\n",
    "    # create model\n",
    "    optimizer = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #model.add(LeakyReLU(alpha=0.1))\n",
    "    # model.add(Dropout(rate = dropout))\n",
    "\n",
    "    #model.add(Dense(1, kernel_regularizer = regularizers.l1(lambda_reg)))\n",
    "    \n",
    "    \n",
    "    nn = Sequential()\n",
    "    \n",
    "    nn.add(Dense(32, input_dim=1000, activation='relu', kernel_regularizer = regularizers.l2(lambda_reg)))\n",
    "    #nn.add(LeakyReLU(alpha=0.1))\n",
    "    #nn.add(Dropout(rate = dropout))\n",
    "    \n",
    "    nn.add(Dense(16, activation='relu'))\n",
    "    #nn.add(LeakyReLU(alpha=0.1))\n",
    "    #nn.add(Dropout(rate = dropout))\n",
    "    \n",
    "    nn.add(Dense(8, activation='relu'))\n",
    "    #nn.add(LeakyReLU(alpha=0.1))\n",
    "    #nn.add(Dropout(rate = dropout))\n",
    "    \n",
    "    nn.add(Dense(4,activation='relu'))\n",
    "    #nn.add(LeakyReLU(alpha=0.1))\n",
    "    #nn.add(Dropout(rate = dropout))\n",
    "    \n",
    "    nn.add(Dense(3, activation='softmax'))\n",
    "    # Compile model\n",
    "   \n",
    "\n",
    "    nn.compile(loss=\"categorical_crossentropy\", #weighted_categorical_crossentropy(weights), \n",
    "               optimizer=optimizer, \n",
    "               metrics=[bmac_metric])\n",
    "    nn.summary()\n",
    "    return nn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (4800, 1000)\n",
      "Test shape:  (4100, 1000)\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled, X_test_scaled = scale_data(train_data, test_data)\n",
    "# encode class values as integers\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoder.fit(Y_train.values)\n",
    "encoded_Y = encoder.transform(Y_train.values).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ax/miniconda3/envs/aml/lib/python3.6/site-packages/keras/callbacks.py:928: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` insted.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540\n",
      "3240\n",
      "540\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_41 (Dense)             (None, 32)                32032     \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 32,747\n",
      "Trainable params: 32,747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4320 samples, validate on 480 samples\n",
      "Epoch 1/70\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 7.8735 - bmac_metric: 0.3326 - val_loss: 1.0025 - val_bmac_metric: 0.3372\n",
      "Epoch 2/70\n",
      "100/100 [==============================] - 2s 21ms/step - loss: 2.2833 - bmac_metric: 0.3391 - val_loss: 1.3233 - val_bmac_metric: 0.3410\n",
      "Epoch 3/70\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 2.3026 - bmac_metric: 0.3408 - val_loss: 1.3051 - val_bmac_metric: 0.3397\n",
      "Epoch 4/70\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 5.3342 - bmac_metric: 0.3422 - val_loss: 0.7832 - val_bmac_metric: 0.3427\n",
      "Epoch 5/70\n",
      "100/100 [==============================] - 2s 21ms/step - loss: 2.4634 - bmac_metric: 0.3424 - val_loss: 2.8566 - val_bmac_metric: 0.3423\n",
      "Epoch 6/70\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 3.0112 - bmac_metric: 0.3417 - val_loss: 1.7026 - val_bmac_metric: 0.3417\n",
      "Epoch 7/70\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 2.7255 - bmac_metric: 0.3419 - val_loss: 2.1855 - val_bmac_metric: 0.3421\n",
      "Epoch 8/70\n",
      "100/100 [==============================] - 2s 21ms/step - loss: 3.5172 - bmac_metric: 0.3426 - val_loss: 1.4192 - val_bmac_metric: 0.3438\n",
      "Epoch 9/70\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 2.3611 - bmac_metric: 0.3433 - val_loss: 1.4972 - val_bmac_metric: 0.3432\n",
      "Epoch 10/70\n",
      "100/100 [==============================] - 2s 21ms/step - loss: 2.5628 - bmac_metric: 0.3429 - val_loss: 0.8857 - val_bmac_metric: 0.3427\n",
      "Epoch 11/70\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 2.3740 - bmac_metric: 0.3428 - val_loss: 0.7577 - val_bmac_metric: 0.3424\n",
      "Epoch 12/70\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 2.2692 - bmac_metric: 0.3422 - val_loss: 1.4619 - val_bmac_metric: 0.3420\n",
      "Epoch 13/70\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 2.1184 - bmac_metric: 0.3416 - val_loss: 0.7722 - val_bmac_metric: 0.3413\n",
      "Epoch 14/70\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 2.0302 - bmac_metric: 0.3411 - val_loss: 0.7237 - val_bmac_metric: 0.3408\n",
      "Epoch 15/70\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 2.0069 - bmac_metric: 0.3406 - val_loss: 0.8690 - val_bmac_metric: 0.3404\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 16/70\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 1.6985 - bmac_metric: 0.3401 - val_loss: 0.7103 - val_bmac_metric: 0.3399\n",
      "Epoch 17/70\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 1.6105 - bmac_metric: 0.3397 - val_loss: 0.7499 - val_bmac_metric: 0.3395\n",
      "Epoch 18/70\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 1.5521 - bmac_metric: 0.3394 - val_loss: 0.7389 - val_bmac_metric: 0.3392\n",
      "Epoch 19/70\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 1.4666 - bmac_metric: 0.3390 - val_loss: 0.9860 - val_bmac_metric: 0.3389\n",
      "Epoch 20/70\n",
      "100/100 [==============================] - 2s 21ms/step - loss: 1.4709 - bmac_metric: 0.3387 - val_loss: 0.9028 - val_bmac_metric: 0.3386\n",
      "Epoch 21/70\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 1.3277 - bmac_metric: 0.3385 - val_loss: 0.8126 - val_bmac_metric: 0.3384\n",
      "Epoch 22/70\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 1.2755 - bmac_metric: 0.3382 - val_loss: 0.8295 - val_bmac_metric: 0.3381\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 23/70\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 1.0504 - bmac_metric: 0.3380 - val_loss: 0.8268 - val_bmac_metric: 0.3379\n",
      "Epoch 24/70\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 1.0310 - bmac_metric: 0.3378 - val_loss: 0.8271 - val_bmac_metric: 0.3377\n",
      "Epoch 25/70\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 1.0157 - bmac_metric: 0.3376 - val_loss: 0.8276 - val_bmac_metric: 0.3376\n",
      "Epoch 26/70\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 1.0008 - bmac_metric: 0.3375 - val_loss: 0.8298 - val_bmac_metric: 0.3374\n",
      "Epoch 27/70\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 0.9863 - bmac_metric: 0.3373 - val_loss: 0.8312 - val_bmac_metric: 0.3372\n",
      "Epoch 28/70\n",
      "100/100 [==============================] - 2s 21ms/step - loss: 0.9720 - bmac_metric: 0.3372 - val_loss: 0.8350 - val_bmac_metric: 0.3371\n",
      "Epoch 29/70\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 0.9576 - bmac_metric: 0.3370 - val_loss: 0.8377 - val_bmac_metric: 0.3370\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 30/70\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 0.9495 - bmac_metric: 0.3369 - val_loss: 0.8377 - val_bmac_metric: 0.3369\n",
      "Epoch 31/70\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.9480 - bmac_metric: 0.3368 - val_loss: 0.8379 - val_bmac_metric: 0.3367\n",
      "Epoch 32/70\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.9465 - bmac_metric: 0.3367 - val_loss: 0.8382 - val_bmac_metric: 0.3366\n",
      "Epoch 33/70\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.9449 - bmac_metric: 0.3366 - val_loss: 0.8384 - val_bmac_metric: 0.3365\n",
      "Epoch 34/70\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.9434 - bmac_metric: 0.3365 - val_loss: 0.8385 - val_bmac_metric: 0.3364\n",
      "Epoch 35/70\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.9418 - bmac_metric: 0.3364 - val_loss: 0.8389 - val_bmac_metric: 0.3363\n",
      "Epoch 36/70\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.9401 - bmac_metric: 0.3363 - val_loss: 0.8390 - val_bmac_metric: 0.3363\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 37/70\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.9392 - bmac_metric: 0.3362 - val_loss: 0.8391 - val_bmac_metric: 0.3362\n",
      "Epoch 38/70\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.9390 - bmac_metric: 0.3361 - val_loss: 0.8391 - val_bmac_metric: 0.3361\n",
      "Epoch 39/70\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.9388 - bmac_metric: 0.3361 - val_loss: 0.8392 - val_bmac_metric: 0.3361\n",
      "Epoch 40/70\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.9386 - bmac_metric: 0.3361 - val_loss: 0.8392 - val_bmac_metric: 0.3361\n",
      "Epoch 41/70\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 0.9385 - bmac_metric: 0.3361 - val_loss: 0.8392 - val_bmac_metric: 0.3361\n",
      "Epoch 42/70\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 0.9383 - bmac_metric: 0.3361 - val_loss: 0.8392 - val_bmac_metric: 0.3361\n",
      "Epoch 43/70\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 0.9381 - bmac_metric: 0.3361 - val_loss: 0.8393 - val_bmac_metric: 0.3361\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "Epoch 44/70\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 0.9380 - bmac_metric: 0.3361 - val_loss: 0.8393 - val_bmac_metric: 0.3361\n",
      "Epoch 45/70\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 0.9380 - bmac_metric: 0.3361 - val_loss: 0.8393 - val_bmac_metric: 0.3361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/70\n",
      "100/100 [==============================] - 2s 21ms/step - loss: 0.9379 - bmac_metric: 0.3361 - val_loss: 0.8393 - val_bmac_metric: 0.3361\n",
      "Epoch 47/70\n",
      "100/100 [==============================] - 2s 21ms/step - loss: 0.9379 - bmac_metric: 0.3361 - val_loss: 0.8393 - val_bmac_metric: 0.3361\n",
      "Epoch 48/70\n",
      "100/100 [==============================] - 2s 21ms/step - loss: 0.9379 - bmac_metric: 0.3361 - val_loss: 0.8393 - val_bmac_metric: 0.3361\n",
      "Epoch 49/70\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 0.9379 - bmac_metric: 0.3361 - val_loss: 0.8393 - val_bmac_metric: 0.3361\n",
      "Epoch 50/70\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 0.9378 - bmac_metric: 0.3361 - val_loss: 0.8393 - val_bmac_metric: 0.3361\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\n",
      "Epoch 51/70\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 0.9378 - bmac_metric: 0.3361 - val_loss: 0.8393 - val_bmac_metric: 0.3361\n",
      "Epoch 52/70\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 0.9378 - bmac_metric: 0.3361 - val_loss: 0.8393 - val_bmac_metric: 0.3361\n",
      "Epoch 53/70\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 0.9378 - bmac_metric: 0.3361 - val_loss: 0.8393 - val_bmac_metric: 0.3361\n",
      "Epoch 54/70\n",
      " 86/100 [========================>.....] - ETA: 0s - loss: 0.9378 - bmac_metric: 0.3361"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ad4c553d5c82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m              \u001b[0;31m#earlyStop,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m              \u001b[0mmcp_save\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m              reduce_lr_loss])\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aml/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/aml/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/aml/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1187\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BMAC = []\n",
    "cMat = None\n",
    "\n",
    "# tf board call back\n",
    "tbCallBack = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True)\n",
    "earlyStop = EarlyStopping(monitor='val_bmac_metric', patience=10, verbose=1, mode='auto')\n",
    "mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_bmac_metric', mode='max')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_bmac_metric', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='max')\n",
    "\n",
    "num_epochs = 70\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "for train, test in kfold.split(X_train_scaled, Y_train.values):\n",
    "    X = X_train_scaled[train]\n",
    "    Y = encoded_Y[train]\n",
    "    X_test = X_train_scaled[test]\n",
    "    Y_valid = encoded_Y[test]\n",
    "    # compute the class weights\n",
    "    fac_0_2_class = 2\n",
    "    #class_weights = np.array([(np.sum(Y_train.values == 0) / X_train_scaled.shape[0]) * fac_0_2_class, \n",
    "    #                      np.sum(Y_train.values == 1) / X_train_scaled.shape[0], \n",
    "    #                      (np.sum(Y_train.values == 2) / X_train_scaled.shape[0]) * fac_0_2_class])\n",
    "    print(np.sum(Y_train.values[train] == 0))\n",
    "    print(np.sum(Y_train.values[train] == 1))\n",
    "    print(np.sum(Y_train.values[train] == 2))\n",
    "    model = baseline_model()\n",
    "    class_weight = {0: 6., 1: 1.,2: 6.}\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(x=X, y=Y, epochs=num_epochs, verbose=1, validation_data=(X_test, Y_valid), shuffle=True, \n",
    "              steps_per_epoch=100, initial_epoch=0, validation_steps=5,\n",
    "              class_weight=class_weight,\n",
    "             callbacks=[\n",
    "             #tbCallBack,\n",
    "             #earlyStop, \n",
    "             mcp_save, \n",
    "             reduce_lr_loss])\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.load_weights(filepath = '.mdl_wts.hdf5')\n",
    "    Y_pred = [[p] for p in  model.predict_classes(X_test)]\n",
    "    Y_true = Y_train.values[test]\n",
    "    cur_BMAC = balanced_accuracy_score(Y_true, Y_pred)\n",
    "    print(f\"Current BMAC score {cur_BMAC}\")\n",
    "    BMAC.append(cur_BMAC)\n",
    "    cMat = confusion_matrix(Y_true, Y_pred)\n",
    "    break # only do a single run\n",
    "    \n",
    "print(\"AVG: BMAC score: %.4f (+/- %.4f)\" % (np.mean(BMAC), np.std(BMAC)))\n",
    "plot_confusion_matrix(cMat, [\"0\", \"1\", \"2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_121 (Dense)            (None, 32)                32032     \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 32,747\n",
      "Trainable params: 32,747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4320 samples, validate on 480 samples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-6c2a83f7d172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m              \u001b[0;31m#earlyStop,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m              \u001b[0mmcp_save\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m              reduce_lr_loss])\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.mdl_wts.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aml/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/aml/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/aml/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mcallback_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1155\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1156\u001b[0m         callbacks.set_params({\n\u001b[1;32m   1157\u001b[0m             \u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aml/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mset_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aml/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mset_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             self.writer = tf.summary.FileWriter(self.log_dir,\n\u001b[0;32m--> 787\u001b[0;31m                                                 self.sess.graph)\n\u001b[0m\u001b[1;32m    788\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aml/lib/python3.6/site-packages/tensorflow/python/summary/writer/writer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logdir, graph, max_queue, flush_secs, graph_def, filename_suffix, session)\u001b[0m\n\u001b[1;32m    365\u001b[0m       event_writer = EventFileWriter(logdir, max_queue, flush_secs,\n\u001b[1;32m    366\u001b[0m                                      filename_suffix)\n\u001b[0;32m--> 367\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aml/lib/python3.6/site-packages/tensorflow/python/summary/writer/writer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, event_writer, graph, graph_def)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgraph_def\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0;31m# Calling it with both graph and graph_def for backward compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m       \u001b[0;31m# Also export the meta_graph_def in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m       \u001b[0;31m# graph may itself be a graph_def due to positional arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aml/lib/python3.6/site-packages/tensorflow/python/summary/writer/writer.py\u001b[0m in \u001b[0;36madd_graph\u001b[0;34m(self, graph, global_step, graph_def)\u001b[0m\n\u001b[1;32m    211\u001b[0m                       \"or the deprecated `GraphDef`\")\n\u001b[1;32m    212\u001b[0m     \u001b[0;31m# Finally, add the graph_def to the summary writer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_write_plugin_assets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aml/lib/python3.6/site-packages/tensorflow/python/summary/writer/writer.py\u001b[0m in \u001b[0;36m_add_graph_def\u001b[0;34m(self, graph_def, global_step)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_add_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0mgraph_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m     \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# use all data for training\n",
    "fac_0_2_class = 1\n",
    "num_epochs = 100\n",
    "class_weights = np.array([(np.sum(Y_train.values == 0) / X_train_scaled.shape[0]) * fac_0_2_class, \n",
    "                          np.sum(Y_train.values == 1) / X_train_scaled.shape[0], \n",
    "                          (np.sum(Y_train.values == 2) / X_train_scaled.shape[0]) * fac_0_2_class])\n",
    "\n",
    "model = baseline_model(class_weights)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model.fit(x=X_train_scaled, y=encoded_Y, epochs=num_epochs, verbose=1, validation_split=0.1, shuffle=True, \n",
    "          steps_per_epoch=100, initial_epoch=0, validation_steps=5, \n",
    "         callbacks=[\n",
    "             tbCallBack,\n",
    "             #earlyStop, \n",
    "             mcp_save, \n",
    "             reduce_lr_loss])\n",
    "\n",
    "model.load_weights(filepath = '.mdl_wts.hdf5')\n",
    "pred = model.predict_classes(X_test_scaled)\n",
    "make_submission('ax_SOFTMAX.csv', pred)\n",
    "print(\"Process finsihed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aml-3)",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
