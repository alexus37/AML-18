{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import natsort\n",
    "import random as rn\n",
    "import skvideo.io\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import (SVC, SVR)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import (AdaBoostRegressor, RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, BaggingRegressor)\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support function from the given repo for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support function from the given repo for the project\n",
    "def get_videos_from_folder(data_folder):\n",
    "    '''\n",
    "    get a list of video x wehre each video is a numpy array in the format [n_frames,width,height] \n",
    "    with uint8 elements.\n",
    "    argument: relative path to the data_folder from the source folder.\n",
    "    '''\n",
    "    data_folder = os.path.join(dir_path,data_folder)\n",
    "    x = []\n",
    "    file_names = []\n",
    "    \n",
    "    if os.path.isdir(data_folder):\n",
    "        for dirpath, dirnames, filenames in os.walk(data_folder):\n",
    "            filenames = natsort.natsorted(filenames,reverse=False)\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                statinfo = os.stat(file_path)\n",
    "                if statinfo.st_size != 0:\n",
    "                    video = skvideo.io.vread(file_path, outputdict={\"-pix_fmt\": \"gray\"})[:, :, :, 0]\n",
    "                    x.append(video)\n",
    "                    file_names.append(int(filename.split(\".\")[0]))\n",
    "\n",
    "    indices = sorted(range(len(file_names)), key=file_names.__getitem__)\n",
    "    x = np.take(x,indices)\n",
    "    return x\n",
    "\n",
    "def get_target_from_csv(csv_file):\n",
    "    '''\n",
    "    get a numpy array y of labels. the order follows the id of video. \n",
    "    argument: relative path to the csv_file from the source folder.\n",
    "    '''\n",
    "    csv_file = os.path.join(dir_path,csv_file)\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        label_reader = pd.read_csv(csvfile)\n",
    "        #print(\"Labels: \", label_reader['id'])\n",
    "        y = label_reader['y']\n",
    "        \n",
    "    y = np.array(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def save_solution(csv_file,prob_positive_class):\n",
    "    with open(csv_file, 'w') as csv:\n",
    "        df = pd.DataFrame.from_dict({'id':range(len(prob_positive_class)),'y': prob_positive_class})\n",
    "        df.to_csv(csv,index = False)\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def save_tf_record(x,file_name,y = None):\n",
    "    writer = tf.python_io.TFRecordWriter(file_name)\n",
    "    if y is None:\n",
    "        for video in x:\n",
    "            sys.stdout.flush()\n",
    "            feature = {'len': _int64_feature(video.shape[0]),\n",
    "                       'height': _int64_feature(video.shape[1]),\n",
    "                       'width': _int64_feature(video.shape[2]),\n",
    "                       'video': _bytes_feature(tf.compat.as_bytes(video.tostring()))}\n",
    "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "            writer.write(example.SerializeToString())\n",
    "    else:\n",
    "        for video,label in zip(x,y):\n",
    "            sys.stdout.flush()\n",
    "            feature = {'len': _int64_feature(video.shape[0]),\n",
    "                       'height': _int64_feature(video.shape[1]),\n",
    "                       'width': _int64_feature(video.shape[2]),\n",
    "                       'video': _bytes_feature(tf.compat.as_bytes(video.tostring())),\n",
    "                       'label': _int64_feature(label)}\n",
    "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "            writer.write(example.SerializeToString())\n",
    "    \n",
    "    writer.close()\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def prob_positive_class_from_prediction(pred):\n",
    "    return np.array([p['probabilities'][1] for p in pred])\n",
    "\n",
    "def decode(serialized_example):\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'len': tf.FixedLenFeature([], tf.int64),\n",
    "            'height': tf.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.FixedLenFeature([], tf.int64),\n",
    "            'label': tf.FixedLenFeature([], tf.int64,default_value = 0),\n",
    "            'video': tf.FixedLenFeature([], tf.string),\n",
    "        })\n",
    "    video = tf.decode_raw(features['video'], tf.uint8)\n",
    "    height = features['height']\n",
    "    width = features['width']\n",
    "    length = features['len']\n",
    "    shape = tf.stack([length,height,width])\n",
    "    video = tf.reshape(video,shape)\n",
    "    label = features['label']\n",
    "    features = {'video':video}\n",
    "    return features,label\n",
    "\n",
    "def input_fn_from_dataset(files,batch_size = 1,num_epochs = None,shuffle = True):\n",
    "    data_set = tf.data.TFRecordDataset(files)\n",
    "    if shuffle:\n",
    "        data_set = data_set.shuffle(buffer_size=len(files)) \n",
    "    data_set = data_set.map(decode)\n",
    "    data_set = data_set.padded_batch(batch_size,padded_shapes= ({'video':[212,100,100]},[]))\n",
    "    data_set = data_set.repeat(num_epochs)\n",
    "    data_set = data_set.prefetch(batch_size)\n",
    "    \n",
    "    return data_set\n",
    "\n",
    "def decode_frame(serialized_example):\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'len': tf.FixedLenFeature([], tf.int64),\n",
    "            'height': tf.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.FixedLenFeature([], tf.int64),\n",
    "            'label': tf.FixedLenFeature([], tf.int64,default_value = 0),\n",
    "            'video': tf.FixedLenFeature([], tf.string),\n",
    "        })\n",
    "    video = tf.decode_raw(features['video'], tf.uint8)\n",
    "    height = features['height']\n",
    "    width = features['width']\n",
    "    length = features['len']\n",
    "    shape = tf.stack([length,height,length])\n",
    "    video = tf.reshape(video,shape)\n",
    "    label = features['label']\n",
    "    label = tf.expand_dims(label,axis=-1)\n",
    "    label = tf.tile(label,tf.expand_dims(length,axis=-1))\n",
    "    features = {'frame':video}\n",
    "    return features,label\n",
    "\n",
    "def input_fn_frame_from_dataset(files,batch_size = 1,num_epochs = None):\n",
    "    data_set = tf.data.TFRecordDataset(files)\n",
    "    data_set = data_set.shuffle(buffer_size=len(files)) \n",
    "    data_set = data_set.map(decode_frame)\n",
    "    data_set = data_set.shuffle(buffer_size=batch_size)\n",
    "    data_set = data_set.apply(tf.contrib.data.unbatch())\n",
    "    data_set = data_set.batch(batch_size)\n",
    "    data_set = data_set.repeat(num_epochs)\n",
    "    data_set = data_set.prefetch(batch_size)\n",
    "    \n",
    "    return data_set\n",
    "#Custom support functions\n",
    "\n",
    "#Count for the minimum number of frames video among the list_array of videos\n",
    "def count_min_number_frames(list_array):\n",
    "    min_frames = 999\n",
    "    for sample in list_array:\n",
    "        if sample.shape[0] < min_frames:\n",
    "            min_frames = sample.shape[0]\n",
    "            \n",
    "    return min_frames\n",
    "\n",
    "#Count number of subsamples given the min_frames\n",
    "def count_new_subsamples(list_array, min_frames, train_set=True, labels=[]):\n",
    "    \"\"\"Params:\n",
    "       - list array: list of videos (list of numpy array)\n",
    "       - min_frames: number of frames per subsample (equal to the minimum number fo frames among training and test videos)\n",
    "       - train_set: set to false if the dataset passed is the test set\n",
    "       - labels: pass the labels for the train set\n",
    "       Return:\n",
    "       - n_new_subsamples \n",
    "         if train set == true -> int(video frames/min_frames)\n",
    "         if test set == false -> len(list_array) just pick one subsample of min_frames per video and discard the other frames\n",
    "    \"\"\"\n",
    "    classes = [0,0]\n",
    "    i = 0\n",
    "    n_new_samples = 0\n",
    "    for sample in list_array:\n",
    "        n_new_samples += int(sample.shape[0]/min_frames)\n",
    "        if train_set:\n",
    "            classes[labels[i]] += int(sample.shape[0]/min_frames)\n",
    "        i+=1\n",
    "    \n",
    "    return n_new_samples\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(X_videos):\n",
    "    total_videos = len(X_videos)\n",
    "    total_features = 13205\n",
    "    #Extracting features\n",
    "    X_features = np.zeros((total_videos,total_features))\n",
    "    video_n = 0\n",
    "    for video in tqdm(X_videos):\n",
    "        \n",
    "        video_grad = np.diff(video,axis = 0) #frame-wise gradient\n",
    "        height_im_grad = video_grad.shape[1]\n",
    "        width_im_grad = video_grad.shape[2]\n",
    "\n",
    "        for idx in range(height_im):\n",
    "\n",
    "            X_features[video_n][idx*6] = np.mean(video[:,:,idx]) #entire video video mean_column_pixels            1\n",
    "            X_features[video_n][idx*6+1] = np.mean(video[:,idx,:]) #entire video mean_row_pixels             2\n",
    "            X_features[video_n][idx*6+2] = np.std(video[:,:,idx]) #entire video std_column_pixels            3\n",
    "            X_features[video_n][idx*6+3] = np.std(video[:,idx,:]) #entire video std_row_pixels               4\n",
    "            X_features[video_n][idx*6+4] = np.count_nonzero(video[:,:,idx]) #entire nonzero_column_pixels    5\n",
    "            X_features[video_n][idx*6+5] = np.count_nonzero(video[:,idx,:]) #entire nonzero_row_pixels       6\n",
    "        \n",
    "        # ? TODO ? Taking some of the same previous features but just frame-wise?\n",
    "        idx_next = height_im*6\n",
    "      \n",
    "        X_features[video_n][idx_next+1] = np.mean(video_grad) #mean_video_grad    1\n",
    "        X_features[video_n][idx_next+2] = np.std(video_grad) #std_video_grad      2\n",
    "        X_features[video_n][idx_next+3] = np.mean(video_grad) #mean_frame_grad    3\n",
    "    \n",
    "        idx_next = idx_next+4\n",
    "\n",
    "        idx_frame_grad = 0\n",
    "        \n",
    "        for frame_grad in video_grad:\n",
    "\n",
    "            X_features[video_n][idx_next  + 1 +   idx_frame_grad*6*height_im_grad] = np.std(frame_grad) #Std of the gradient of the single frame std_frame_grad      1\n",
    "\n",
    "            for idx in range(height_im_grad):\n",
    "                \n",
    "                #print(\"Inside inside: \",idx_next  + 1 +   idx*6 + idx_frame_grad*6*height_im_grad)\n",
    "                X_features[video_n][idx_next  + 1 +   idx*6 + idx_frame_grad*6*height_im_grad] = np.mean(frame_grad[:,idx]) #mean_grad_column_pixels   1\n",
    "                X_features[video_n][idx_next  + 2 +   idx*6 + idx_frame_grad*6*height_im_grad] = np.mean(frame_grad[idx,:]) #mean_grad_row_pixels           2\n",
    "                X_features[video_n][idx_next  + 3 +   idx*6 + idx_frame_grad*6*height_im_grad] = np.std(frame_grad[:,idx]) #std_grad_column_pixels          3\n",
    "                X_features[video_n][idx_next  + 4 +   idx*6 + idx_frame_grad*6*height_im_grad] = np.std(frame_grad[idx,:]) #std_grad_row_pixels               4\n",
    "                X_features[video_n][idx_next  + 5 +   idx*6 + idx_frame_grad*6*height_im_grad] = np.count_nonzero(frame_grad[:,idx]) #nonzero_grad_column_pixels  5\n",
    "                X_features[video_n][idx_next  + 6 +   idx*6 + idx_frame_grad*6*height_im_grad] = np.count_nonzero(frame_grad[idx,:]) #nonzero_grad_row_pixels     6\n",
    "                #print(\"Finish Inside inside: \",idx_next  + 6 +   idx*6 + idx_frame_grad*6*height_im_grad)\n",
    "        \n",
    "            idx_frame_grad+=1\n",
    "    \n",
    "        print(video_n)\n",
    "        video_n+=1\n",
    "    return X_features\n",
    "\n",
    "def extract_features_ax(X_videos):\n",
    "    total_videos = len(X_videos)\n",
    "    #Extracting features\n",
    "    \n",
    "    all_features = list()\n",
    "    for video in X_videos:\n",
    "        feature_vec = list()\n",
    "        \n",
    "        video_grad = np.diff(video,axis = 0) #frame-wise gradient\n",
    "        height_im_grad = video_grad.shape[1]\n",
    "        width_im_grad = video_grad.shape[2]\n",
    "        \n",
    "        # compute the sum of pixels per frame\n",
    "        heartBeatApprox = np.sum(np.sum(video, axis=1), axis=1)\n",
    "        feature_vec.append(np.min(heartBeatApprox))\n",
    "        feature_vec.append(np.max(heartBeatApprox))\n",
    "        feature_vec.append(np.mean(heartBeatApprox))\n",
    "        feature_vec.append(np.std(heartBeatApprox))\n",
    "        feature_vec.append(np.std(heartBeatApprox))\n",
    "        feature_vec += heartBeatApprox.tolist()\n",
    "        \n",
    "        \n",
    "        \n",
    "        for idx in range(height_im):\n",
    "            feature_vec.append(np.mean(video[:,:,idx])) #entire video video mean_column_pixels            1\n",
    "            feature_vec.append(np.mean(video[:,idx,:])) #entire video mean_row_pixels             2\n",
    "            feature_vec.append(np.std(video[:,:,idx])) #entire video std_column_pixels            3\n",
    "            feature_vec.append(np.std(video[:,idx,:])) #entire video std_row_pixels               4\n",
    "            feature_vec.append(np.count_nonzero(video[:,:,idx])) #entire nonzero_column_pixels    5\n",
    "            feature_vec.append(np.count_nonzero(video[:,idx,:])) #entire nonzero_row_pixels       6\n",
    "        \n",
    "        # ? TODO ? Taking some of the same previous features but just frame-wise?\n",
    "        feature_vec.append(np.mean(video_grad)) #mean_video_grad    1\n",
    "        feature_vec.append(np.std(video_grad)) #std_video_grad      2\n",
    "        feature_vec.append(np.mean(video_grad)) #mean_frame_grad    3\n",
    "    \n",
    "    \n",
    "        for frame_grad in video_grad:\n",
    "            feature_vec.append(np.mean(frame_grad))\n",
    "            feature_vec.append(np.std(frame_grad)) #Std of the gradient of the single frame std_frame_grad      1\n",
    "\n",
    "            for idx in range(height_im_grad):            \n",
    "                feature_vec.append(np.mean(frame_grad[:,idx])) #mean_grad_column_pixels   1\n",
    "                feature_vec.append(np.mean(frame_grad[idx,:])) #mean_grad_row_pixels           2\n",
    "                feature_vec.append(np.std(frame_grad[:,idx])) #std_grad_column_pixels          3\n",
    "                feature_vec.append(np.std(frame_grad[idx,:])) #std_grad_row_pixels               4\n",
    "                feature_vec.append(np.count_nonzero(frame_grad[:,idx])) #nonzero_grad_column_pixels  5\n",
    "                feature_vec.append(np.count_nonzero(frame_grad[idx,:])) #nonzero_grad_row_pixels     6\n",
    "        \n",
    "        total_features = len(feature_vec)\n",
    "        all_features.append(feature_vec)\n",
    "        \n",
    "    X_features = np.zeros((total_videos,total_features))\n",
    "    \n",
    "    for i in range(total_videos):\n",
    "        X_features[i,:] = all_features[i]\n",
    "        \n",
    "    return X_features\n",
    "\n",
    "def extract_ids(data_folder):\n",
    "    \n",
    "    print(\"Extracting ids from test set videos\")\n",
    "    data_folder = os.path.join(dir_path,data_folder)\n",
    "    x = []\n",
    "    file_names = []\n",
    "    \n",
    "    if os.path.isdir(data_folder):\n",
    "        for dirpath, dirnames, filenames in os.walk(data_folder):\n",
    "            filenames = natsort.natsorted(filenames,reverse=False)\n",
    "            ids = []\n",
    "            for filename in filenames:\n",
    "              ids.append(int(filename.split(\".\")[0]))\n",
    "    return ids\n",
    "\n",
    "def make_submission(filename, predictions):\n",
    "    ids = extract_ids(test_folder)\n",
    "    df = pd.DataFrame({'id':ids, 'y':predictions})\n",
    "    df[[\"id\", \"y\"]].to_csv(\"submissions/\"+filename, index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the train and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "train_folder = os.path.join(dir_path,\"data/train/\")\n",
    "test_folder = os.path.join(dir_path,\"data/test/\")\n",
    "\n",
    "train_target = os.path.join(dir_path,'data/train_target.csv')\n",
    "\n",
    "#Load data from csv file\n",
    "x_train = get_videos_from_folder(train_folder) #List of numpy arrays\n",
    "y_train = get_target_from_csv(train_target) #Numpy array of labels\n",
    "x_test = get_videos_from_folder(test_folder) #List of numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute minimum number of frames per video in train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number of frames among video dataset -> 22\n"
     ]
    }
   ],
   "source": [
    "#Compute minimum number of frames per video in train and test set\n",
    "min_frames_train = count_min_number_frames(x_train)\n",
    "min_frames_test = count_min_number_frames(x_test)\n",
    "\n",
    "#Pick minimum number of frames among all dataset\n",
    "min_frames = min_frames_train if min_frames_train<min_frames_test else min_frames_test\n",
    "print(f\"Minimum number of frames among video dataset -> {min_frames}\")\n",
    "\n",
    "#Count the number of new subsamples per video of min_frames\n",
    "n_train_subsamples = count_new_subsamples(x_train, min_frames, train_set = True, labels=y_train)\n",
    "n_test_subsamples = count_new_subsamples(x_test, min_frames, train_set = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct new datasets from subsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New training samples shape ->  (402, 22, 100, 100)\n",
      "New test samples shape ->  (69, 22, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "height_im = x_train[0].shape[1]\n",
    "width_im = x_train[0].shape[2]\n",
    "X = np.zeros((n_train_subsamples, min_frames, height_im, width_im))\n",
    "Y = np.zeros((n_train_subsamples))\n",
    "\n",
    "\n",
    "#TRAIN\n",
    "\n",
    "sample_idx = 0\n",
    "subsample_idx = 0\n",
    "\n",
    "#Train subsamples\n",
    "for sample_video in x_train:\n",
    "    \n",
    "    subsamples = int(sample_video.shape[0]/min_frames)\n",
    "    \n",
    "    for i in range(subsamples):\n",
    "        X[subsample_idx,:,:,:] = sample_video[i*min_frames : i*min_frames+min_frames, :, :]\n",
    "        Y[subsample_idx] = y_train[sample_idx]\n",
    "        subsample_idx+=1\n",
    "        \n",
    "    sample_idx+=1\n",
    "\n",
    "    \n",
    "#TEST\n",
    "X_test = np.zeros((len(x_test), min_frames, height_im, width_im))\n",
    "\n",
    "\n",
    "sample_idx = 0\n",
    "subsample_idx = 0\n",
    "\n",
    "#Test subsamples\n",
    "# ? TODO ? create test set subsamples and go for a maximum consensus or other tecniques for prediction?\n",
    "for sample_video in x_test:\n",
    "    \n",
    "    X_test[subsample_idx,:,:,:] = sample_video[0 : min_frames, :, :]\n",
    "    \"\"\"    subsamples = int(sample.shape[0]/min_frames)\n",
    "    \n",
    "    for i in range(subsamples):\n",
    "        X[subsample_idx,:,:,:] = sample[i*min_frames : i*min_frames+min_frames, :, :]\n",
    "        Y[subsample_idx] = y_train[sample_idx]\n",
    "        subsample_idx+=1\"\"\"\n",
    "    subsample_idx+=1\n",
    "    #sample_idx+=1\n",
    "\n",
    "    \n",
    "#Reshaping for (n_samples, n_frames, height_frame, width_frame )\n",
    "X = np.reshape(X,(402,22,100,100))\n",
    "X_test = np.reshape(X_test, (len(x_test), min_frames, x_test[0].shape[1], x_test[0].shape[2]))\n",
    "print(\"New training samples shape -> \", X.shape)\n",
    "print(\"New test samples shape -> \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: 2233 features\n"
     ]
    }
   ],
   "source": [
    "X_train_features = extract_features_ax(X)\n",
    "X_train_scaled = preprocessing.scale(X_train_features)\n",
    "\n",
    "X_test_features = extract_features_ax(X_test)\n",
    "X_test_scaled = preprocessing.scale(X_test_features)\n",
    "\n",
    "# Create a feature selctor base on a random forest\n",
    "sfm = SelectFromModel(RandomForestClassifier(n_estimators=10000, random_state=seed, n_jobs=-1), threshold=0.0001)\n",
    "\n",
    "# Train the classifier\n",
    "sfm.fit(X_train_scaled, Y)\n",
    "\n",
    "print(f\"Using: {np.sum(sfm.get_support())} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model, train and make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform features\n",
      "start fitting\n",
      "Extracting ids from test set videos\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=500, random_state=seed, n_jobs=-1, verbose=False)\n",
    "gb = GradientBoostingRegressor(random_state=seed, n_estimators=500, learning_rate=0.01)\n",
    "\n",
    "#Predict with one classifiers\n",
    "print(\"Transform features\")\n",
    "X_train_importance = sfm.transform(X_train_scaled)\n",
    "X_test_importance = sfm.transform(X_test_scaled)\n",
    "\n",
    "print(\"start fitting\")\n",
    "rf.fit(X_train_importance, Y)\n",
    "gb.fit(X_train_importance, Y)\n",
    "\n",
    "p_rf = rf.predict(X_test_importance)\n",
    "p_gb = rf.predict(X_test_importance)\n",
    "final = (p_rf + p_gb) / 2.0\n",
    "make_submission(\"final.csv\", final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aml-3)",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
