{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import natsort\n",
    "import random as rn\n",
    "import skvideo.io\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Keras\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras import optimizers\n",
    "\n",
    "#Sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import (SVC, SVR)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import (AdaBoostRegressor, RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, BaggingRegressor)\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support function from the given repo for the project\n",
    "def get_videos_from_folder(data_folder):\n",
    "    '''\n",
    "    get a list of video x wehre each video is a numpy array in the format [n_frames,width,height] \n",
    "    with uint8 elements.\n",
    "    argument: relative path to the data_folder from the source folder.\n",
    "    '''\n",
    "    data_folder = os.path.join(dir_path,data_folder)\n",
    "    x = []\n",
    "    file_names = []\n",
    "    \n",
    "    if os.path.isdir(data_folder):\n",
    "        for dirpath, dirnames, filenames in os.walk(data_folder):\n",
    "            filenames = natsort.natsorted(filenames,reverse=False)\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                statinfo = os.stat(file_path)\n",
    "                if statinfo.st_size != 0:\n",
    "                    video = skvideo.io.vread(file_path, outputdict={\"-pix_fmt\": \"gray\"})[:, :, :, 0]\n",
    "                    x.append(video)\n",
    "                    file_names.append(int(filename.split(\".\")[0]))\n",
    "\n",
    "    indices = sorted(range(len(file_names)), key=file_names.__getitem__)\n",
    "    x = np.take(x,indices)\n",
    "    return x\n",
    "\n",
    "def get_target_from_csv(csv_file):\n",
    "    '''\n",
    "    get a numpy array y of labels. the order follows the id of video. \n",
    "    argument: relative path to the csv_file from the source folder.\n",
    "    '''\n",
    "    csv_file = os.path.join(dir_path,csv_file)\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        label_reader = pd.read_csv(csvfile)\n",
    "        #print(\"Labels: \", label_reader['id'])\n",
    "        y = label_reader['y']\n",
    "        \n",
    "    y = np.array(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def save_solution(csv_file,prob_positive_class):\n",
    "    with open(csv_file, 'w') as csv:\n",
    "        df = pd.DataFrame.from_dict({'id':range(len(prob_positive_class)),'y': prob_positive_class})\n",
    "        df.to_csv(csv,index = False)\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def save_tf_record(x,file_name,y = None):\n",
    "    writer = tf.python_io.TFRecordWriter(file_name)\n",
    "    if y is None:\n",
    "        for video in x:\n",
    "            sys.stdout.flush()\n",
    "            feature = {'len': _int64_feature(video.shape[0]),\n",
    "                       'height': _int64_feature(video.shape[1]),\n",
    "                       'width': _int64_feature(video.shape[2]),\n",
    "                       'video': _bytes_feature(tf.compat.as_bytes(video.tostring()))}\n",
    "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "            writer.write(example.SerializeToString())\n",
    "    else:\n",
    "        for video,label in zip(x,y):\n",
    "            sys.stdout.flush()\n",
    "            feature = {'len': _int64_feature(video.shape[0]),\n",
    "                       'height': _int64_feature(video.shape[1]),\n",
    "                       'width': _int64_feature(video.shape[2]),\n",
    "                       'video': _bytes_feature(tf.compat.as_bytes(video.tostring())),\n",
    "                       'label': _int64_feature(label)}\n",
    "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "            writer.write(example.SerializeToString())\n",
    "    \n",
    "    writer.close()\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def prob_positive_class_from_prediction(pred):\n",
    "    return np.array([p['probabilities'][1] for p in pred])\n",
    "\n",
    "def decode(serialized_example):\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'len': tf.FixedLenFeature([], tf.int64),\n",
    "            'height': tf.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.FixedLenFeature([], tf.int64),\n",
    "            'label': tf.FixedLenFeature([], tf.int64,default_value = 0),\n",
    "            'video': tf.FixedLenFeature([], tf.string),\n",
    "        })\n",
    "    video = tf.decode_raw(features['video'], tf.uint8)\n",
    "    height = features['height']\n",
    "    width = features['width']\n",
    "    length = features['len']\n",
    "    shape = tf.stack([length,height,width])\n",
    "    video = tf.reshape(video,shape)\n",
    "    label = features['label']\n",
    "    features = {'video':video}\n",
    "    return features,label\n",
    "\n",
    "def input_fn_from_dataset(files,batch_size = 1,num_epochs = None,shuffle = True):\n",
    "    data_set = tf.data.TFRecordDataset(files)\n",
    "    if shuffle:\n",
    "        data_set = data_set.shuffle(buffer_size=len(files)) \n",
    "    data_set = data_set.map(decode)\n",
    "    data_set = data_set.padded_batch(batch_size,padded_shapes= ({'video':[212,100,100]},[]))\n",
    "    data_set = data_set.repeat(num_epochs)\n",
    "    data_set = data_set.prefetch(batch_size)\n",
    "    \n",
    "    return data_set\n",
    "\n",
    "def decode_frame(serialized_example):\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'len': tf.FixedLenFeature([], tf.int64),\n",
    "            'height': tf.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.FixedLenFeature([], tf.int64),\n",
    "            'label': tf.FixedLenFeature([], tf.int64,default_value = 0),\n",
    "            'video': tf.FixedLenFeature([], tf.string),\n",
    "        })\n",
    "    video = tf.decode_raw(features['video'], tf.uint8)\n",
    "    height = features['height']\n",
    "    width = features['width']\n",
    "    length = features['len']\n",
    "    shape = tf.stack([length,height,length])\n",
    "    video = tf.reshape(video,shape)\n",
    "    label = features['label']\n",
    "    label = tf.expand_dims(label,axis=-1)\n",
    "    label = tf.tile(label,tf.expand_dims(length,axis=-1))\n",
    "    features = {'frame':video}\n",
    "    return features,label\n",
    "\n",
    "def input_fn_frame_from_dataset(files,batch_size = 1,num_epochs = None):\n",
    "    data_set = tf.data.TFRecordDataset(files)\n",
    "    data_set = data_set.shuffle(buffer_size=len(files)) \n",
    "    data_set = data_set.map(decode_frame)\n",
    "    data_set = data_set.shuffle(buffer_size=batch_size)\n",
    "    data_set = data_set.apply(tf.contrib.data.unbatch())\n",
    "    data_set = data_set.batch(batch_size)\n",
    "    data_set = data_set.repeat(num_epochs)\n",
    "    data_set = data_set.prefetch(batch_size)\n",
    "    \n",
    "    return data_set\n",
    "#Custom support functions\n",
    "\n",
    "#Count for the minimum number of frames video among the list_array of videos\n",
    "def count_min_number_frames(list_array):\n",
    "    min_frames = 999\n",
    "    for sample in list_array:\n",
    "        if sample.shape[0] < min_frames:\n",
    "            min_frames = sample.shape[0]\n",
    "            \n",
    "    return min_frames\n",
    "\n",
    "#Count number of subsamples given the min_frames\n",
    "def count_new_subsamples(list_array, min_frames, train_set=True, labels=[]):\n",
    "    \"\"\"Params:\n",
    "       - list array: list of videos (list of numpy array)\n",
    "       - min_frames: number of frames per subsample (equal to the minimum number fo frames among training and test videos)\n",
    "       - train_set: set to false if the dataset passed is the test set\n",
    "       - labels: pass the labels for the train set\n",
    "       Return:\n",
    "       - n_new_subsamples \n",
    "         if train set == true -> int(video frames/min_frames)\n",
    "         if test set == false -> len(list_array) just pick one subsample of min_frames per video and discard the other frames\n",
    "    \"\"\"\n",
    "    classes = [0,0]\n",
    "    i = 0\n",
    "    n_new_samples = 0\n",
    "    for sample in list_array:\n",
    "        n_new_samples += int(sample.shape[0]/min_frames)\n",
    "        if train_set:\n",
    "            classes[labels[i]] += int(sample.shape[0]/min_frames)\n",
    "        i+=1\n",
    "    print(\"Train set\" if train_set==True else \"Test set\")\n",
    "    print(n_new_samples)\n",
    "    print(\"Labels per class ->\",classes if train_set==True else \"\")\n",
    "    return n_new_samples\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(X_videos):\n",
    "    total_videos = len(X_videos)\n",
    "    total_features = 13205\n",
    "    #Extracting features\n",
    "    X_features = np.zeros((total_videos,total_features))\n",
    "    video_n = 0\n",
    "    for video in tqdm(X_videos):\n",
    "        \n",
    "        video_grad = np.diff(video,axis = 0) #frame-wise gradient\n",
    "        height_im_grad = video_grad.shape[1]\n",
    "        width_im_grad = video_grad.shape[2]\n",
    "\n",
    "        for idx in range(height_im):\n",
    "\n",
    "            X_features[video_n][idx*6] = np.mean(video[:,:,idx]) #entire video video mean_column_pixels            1\n",
    "            X_features[video_n][idx*6+1] = np.mean(video[:,idx,:]) #entire video mean_row_pixels             2\n",
    "            X_features[video_n][idx*6+2] = np.std(video[:,:,idx]) #entire video std_column_pixels            3\n",
    "            X_features[video_n][idx*6+3] = np.std(video[:,idx,:]) #entire video std_row_pixels               4\n",
    "            X_features[video_n][idx*6+4] = np.count_nonzero(video[:,:,idx]) #entire nonzero_column_pixels    5\n",
    "            X_features[video_n][idx*6+5] = np.count_nonzero(video[:,idx,:]) #entire nonzero_row_pixels       6\n",
    "        \n",
    "        # ? TODO ? Taking some of the same previous features but just frame-wise?\n",
    "        idx_next = height_im*6\n",
    "      \n",
    "        X_features[video_n][idx_next+1] = np.mean(video_grad) #mean_video_grad    1\n",
    "        X_features[video_n][idx_next+2] = np.std(video_grad) #std_video_grad      2\n",
    "        X_features[video_n][idx_next+3] = np.mean(video_grad) #mean_frame_grad    3\n",
    "    \n",
    "        idx_next = idx_next+4\n",
    "\n",
    "        idx_frame_grad = 0\n",
    "        \n",
    "        for frame_grad in video_grad:\n",
    "\n",
    "            X_features[video_n][idx_next  + 1 +   idx_frame_grad*6*height_im_grad] = np.std(frame_grad) #Std of the gradient of the single frame std_frame_grad      1\n",
    "\n",
    "            for idx in range(height_im_grad):\n",
    "                \n",
    "                #print(\"Inside inside: \",idx_next  + 1 +   idx*6 + idx_frame_grad*6*height_im_grad)\n",
    "                X_features[video_n][idx_next  + 1 +   idx*6 + idx_frame_grad*6*height_im_grad] = np.mean(frame_grad[:,idx]) #mean_grad_column_pixels   1\n",
    "                X_features[video_n][idx_next  + 2 +   idx*6 + idx_frame_grad*6*height_im_grad] = np.mean(frame_grad[idx,:]) #mean_grad_row_pixels           2\n",
    "                X_features[video_n][idx_next  + 3 +   idx*6 + idx_frame_grad*6*height_im_grad] = np.std(frame_grad[:,idx]) #std_grad_column_pixels          3\n",
    "                X_features[video_n][idx_next  + 4 +   idx*6 + idx_frame_grad*6*height_im_grad] = np.std(frame_grad[idx,:]) #std_grad_row_pixels               4\n",
    "                X_features[video_n][idx_next  + 5 +   idx*6 + idx_frame_grad*6*height_im_grad] = np.count_nonzero(frame_grad[:,idx]) #nonzero_grad_column_pixels  5\n",
    "                X_features[video_n][idx_next  + 6 +   idx*6 + idx_frame_grad*6*height_im_grad] = np.count_nonzero(frame_grad[idx,:]) #nonzero_grad_row_pixels     6\n",
    "                #print(\"Finish Inside inside: \",idx_next  + 6 +   idx*6 + idx_frame_grad*6*height_im_grad)\n",
    "        \n",
    "            idx_frame_grad+=1\n",
    "    \n",
    "        print(video_n)\n",
    "        video_n+=1\n",
    "    return X_features\n",
    "\n",
    "def extract_features_ax(X_videos):\n",
    "    total_videos = len(X_videos)\n",
    "    #Extracting features\n",
    "    \n",
    "    all_features = list()\n",
    "    for video in X_videos:\n",
    "        feature_vec = list()\n",
    "        \n",
    "        video_grad = np.diff(video,axis = 0) #frame-wise gradient\n",
    "        height_im_grad = video_grad.shape[1]\n",
    "        width_im_grad = video_grad.shape[2]\n",
    "        \n",
    "        # compute the sum of pixels per frame\n",
    "        heartBeatApprox = np.sum(np.sum(video, axis=1), axis=1)\n",
    "        feature_vec.append(np.min(heartBeatApprox))\n",
    "        feature_vec.append(np.max(heartBeatApprox))\n",
    "        feature_vec.append(np.mean(heartBeatApprox))\n",
    "        feature_vec.append(np.std(heartBeatApprox))\n",
    "        feature_vec.append(np.std(heartBeatApprox))\n",
    "        feature_vec += heartBeatApprox.tolist()\n",
    "        \n",
    "        \n",
    "        \n",
    "        for idx in range(height_im):\n",
    "            feature_vec.append(np.mean(video[:,:,idx])) #entire video video mean_column_pixels            1\n",
    "            feature_vec.append(np.mean(video[:,idx,:])) #entire video mean_row_pixels             2\n",
    "            feature_vec.append(np.std(video[:,:,idx])) #entire video std_column_pixels            3\n",
    "            feature_vec.append(np.std(video[:,idx,:])) #entire video std_row_pixels               4\n",
    "            feature_vec.append(np.count_nonzero(video[:,:,idx])) #entire nonzero_column_pixels    5\n",
    "            feature_vec.append(np.count_nonzero(video[:,idx,:])) #entire nonzero_row_pixels       6\n",
    "        \n",
    "        # ? TODO ? Taking some of the same previous features but just frame-wise?\n",
    "        feature_vec.append(np.mean(video_grad)) #mean_video_grad    1\n",
    "        feature_vec.append(np.std(video_grad)) #std_video_grad      2\n",
    "        feature_vec.append(np.mean(video_grad)) #mean_frame_grad    3\n",
    "    \n",
    "    \n",
    "        for frame_grad in video_grad:\n",
    "            feature_vec.append(np.mean(frame_grad))\n",
    "            feature_vec.append(np.std(frame_grad)) #Std of the gradient of the single frame std_frame_grad      1\n",
    "\n",
    "            \"\"\"for idx in range(height_im_grad):            \n",
    "                feature_vec.append(np.mean(frame_grad[:,idx])) #mean_grad_column_pixels   1\n",
    "                feature_vec.append(np.mean(frame_grad[idx,:])) #mean_grad_row_pixels           2\n",
    "                feature_vec.append(np.std(frame_grad[:,idx])) #std_grad_column_pixels          3\n",
    "                feature_vec.append(np.std(frame_grad[idx,:])) #std_grad_row_pixels               4\n",
    "                feature_vec.append(np.count_nonzero(frame_grad[:,idx])) #nonzero_grad_column_pixels  5\n",
    "                feature_vec.append(np.count_nonzero(frame_grad[idx,:])) #nonzero_grad_row_pixels     6\"\"\"\n",
    "        \n",
    "        total_features = len(feature_vec)\n",
    "        all_features.append(feature_vec)\n",
    "        \n",
    "    X_features = np.zeros((total_videos,total_features))\n",
    "    \n",
    "    for i in range(total_videos):\n",
    "        X_features[i,:] = all_features[i]\n",
    "        \n",
    "    return X_features\n",
    "\n",
    "def extract_ids(data_folder):\n",
    "    \n",
    "    print(\"Extracting ids from test set videos\")\n",
    "    data_folder = os.path.join(dir_path,data_folder)\n",
    "    x = []\n",
    "    file_names = []\n",
    "    \n",
    "    if os.path.isdir(data_folder):\n",
    "        for dirpath, dirnames, filenames in os.walk(data_folder):\n",
    "            filenames = natsort.natsorted(filenames,reverse=False)\n",
    "            ids = []\n",
    "            for filename in filenames:\n",
    "              ids.append(int(filename.split(\".\")[0]))\n",
    "    return ids\n",
    "\n",
    "def make_submission(filename, predictions):\n",
    "    ids = extract_ids(test_folder)\n",
    "    df = pd.DataFrame({'id':ids, 'y':predictions})\n",
    "    df[[\"id\", \"y\"]].to_csv(\"submissions/\"+filename, index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dir ->  /home/francesco/Scrivania/AML-18/task4\n",
      "Train folder ->  /home/francesco/Scrivania/AML-18/task4/data/train/\n",
      "Train target ->  /home/francesco/Scrivania/AML-18/task4/data/train_target.csv\n",
      "Test folder ->  /home/francesco/Scrivania/AML-18/task4/data/test/\n",
      "Train Data\n",
      "\n",
      "Test Data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed=42\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "train_folder = os.path.join(dir_path,\"data/train/\")\n",
    "test_folder = os.path.join(dir_path,\"data/test/\")\n",
    "\n",
    "train_target = os.path.join(dir_path,'data/train_target.csv')\n",
    "\n",
    "print(\"Current dir -> \", dir_path)\n",
    "print(\"Train folder -> \",train_folder)\n",
    "print(\"Train target -> \",train_target)\n",
    "print(\"Test folder -> \",test_folder)\n",
    "\n",
    "#Load data from csv file\n",
    "print(\"Train Data\\n\")\n",
    "x_train = get_videos_from_folder(train_folder) #List of numpy arrays\n",
    "y_train = get_target_from_csv(train_target) #Numpy array of labels\n",
    "print(\"Test Data\\n\")\n",
    "x_test = get_videos_from_folder(test_folder) #List of numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number of frames among video dataset ->  22\n",
      "Validation set samples ->  15\n",
      "Train set samples ->  143\n",
      "Train set\n",
      "355\n",
      "Labels per class -> [153, 202]\n",
      "Test set\n",
      "183\n",
      "Labels per class -> \n",
      "Train set\n",
      "47\n",
      "Labels per class -> [29, 18]\n",
      "Train subsamples ->  355\n",
      "Validation subsamples ->  47\n",
      "Test subsamples ->  183\n"
     ]
    }
   ],
   "source": [
    "#Compute minimum number of frames per video in train and test set\n",
    "min_frames_train = count_min_number_frames(x_train)\n",
    "min_frames_test = count_min_number_frames(x_test)\n",
    "\n",
    "\n",
    "validation_split = 0.1\n",
    "\n",
    "#Pick minimum number of frames among all dataset\n",
    "min_frames = min_frames_train if min_frames_train<min_frames_test else min_frames_test\n",
    "print(\"Minimum number of frames among video dataset -> \",min_frames)\n",
    "\n",
    "total_train_samples = len(x_train)\n",
    "valid_samples = int(validation_split*total_train_samples)\n",
    "\n",
    "x_valid = x_train[total_train_samples-valid_samples:]\n",
    "y_valid = y_train[total_train_samples-valid_samples:]\n",
    "\n",
    "x_new_train = x_train[0:total_train_samples-valid_samples]\n",
    "y_train = y_train[0:total_train_samples-valid_samples]\n",
    "\n",
    "print(\"Validation set samples -> \",len(x_valid))\n",
    "print(\"Train set samples -> \", len(x_new_train))\n",
    "\n",
    "\n",
    "#Count the number of new subsamples per video of min_frames\n",
    "n_train_subsamples = count_new_subsamples(x_new_train, min_frames, train_set = True, labels=y_train)\n",
    "n_test_subsamples = count_new_subsamples(x_test, min_frames, train_set = False)\n",
    "n_valid_subsamples = count_new_subsamples(x_valid, min_frames, train_set = True, labels=y_valid)\n",
    "\n",
    "\n",
    "print(\"Train subsamples -> \",n_train_subsamples)\n",
    "print(\"Validation subsamples -> \",n_valid_subsamples)\n",
    "print(\"Test subsamples -> \",n_test_subsamples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New training samples shape ->  (355, 22, 100, 100)\n",
      "New validation samples shape ->  (47, 22, 100, 100)\n",
      "New test samples shape ->  (69, 22, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "#Construct new datasets from subsamples\n",
    "\n",
    "height_im = x_train[0].shape[1]\n",
    "width_im = x_train[0].shape[2]\n",
    "\n",
    "X = np.zeros((n_train_subsamples, min_frames, height_im, width_im))\n",
    "Y = np.zeros((n_train_subsamples))\n",
    "X_valid = np.zeros((n_valid_subsamples, min_frames, height_im, width_im))\n",
    "Y_valid = np.zeros((n_valid_subsamples))\n",
    "\n",
    "#TRAIN\n",
    "\n",
    "sample_idx = 0\n",
    "subsample_idx = 0\n",
    "\n",
    "#Train subsamples\n",
    "for sample_video in x_new_train:\n",
    "    \n",
    "    subsamples = int(sample_video.shape[0]/min_frames)\n",
    "    \n",
    "    for i in range(subsamples):\n",
    "        X[subsample_idx,:,:,:] = sample_video[i*min_frames : i*min_frames+min_frames, :, :]\n",
    "        Y[subsample_idx] = y_train[sample_idx]\n",
    "        subsample_idx+=1\n",
    "        \n",
    "    sample_idx+=1\n",
    "    \n",
    "#VALIDATION\n",
    "\n",
    "sample_idx = 0\n",
    "subsample_idx = 0\n",
    "\n",
    "#Validation subsamples\n",
    "for sample_video in x_valid:\n",
    "    \n",
    "    subsamples = int(sample_video.shape[0]/min_frames)\n",
    "    \n",
    "    for i in range(subsamples):\n",
    "        X_valid[subsample_idx,:,:,:] = sample_video[i*min_frames : i*min_frames+min_frames, :, :]\n",
    "        Y_valid[subsample_idx] = y_valid[sample_idx]\n",
    "        subsample_idx+=1\n",
    "        \n",
    "    sample_idx+=1\n",
    "\n",
    "    \n",
    "#TEST\n",
    "X_test = np.zeros((len(x_test), min_frames, height_im, width_im))\n",
    "\n",
    "\n",
    "sample_idx = 0\n",
    "subsample_idx = 0\n",
    "\n",
    "#Test subsamples\n",
    "# ? TODO ? create test set subsamples and go for a maximum consensus or other tecniques for prediction?\n",
    "for sample_video in x_test:\n",
    "    \n",
    "    X_test[subsample_idx,:,:,:] = sample_video[0 : min_frames, :, :]\n",
    "    \"\"\"    subsamples = int(sample.shape[0]/min_frames)\n",
    "    \n",
    "    for i in range(subsamples):\n",
    "        X[subsample_idx,:,:,:] = sample[i*min_frames : i*min_frames+min_frames, :, :]\n",
    "        Y[subsample_idx] = y_train[sample_idx]\n",
    "        subsample_idx+=1\"\"\"\n",
    "    subsample_idx+=1\n",
    "    #sample_idx+=1\n",
    "\n",
    "    \n",
    "#Reshaping for (n_samples, n_frames, height_frame, width_frame )\n",
    "X = np.reshape(X,(355,22,100,100))\n",
    "X_valid = np.reshape(X_valid,(47,22,100,100))\n",
    "X_test = np.reshape(X_test, (len(x_test), min_frames, x_test[0].shape[1], x_test[0].shape[2]))\n",
    "print(\"New training samples shape -> \", X.shape)\n",
    "print(\"New validation samples shape -> \", X_valid.shape)\n",
    "print(\"New test samples shape -> \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset shape ->  (471, 672)\n",
      "Test dataset shape ->  (69, 672)\n",
      "Train dataset shape ->  (355, 672)\n",
      "Validation dataset shape ->  (47, 672)\n"
     ]
    }
   ],
   "source": [
    "X_train_features = extract_features_ax(X)\n",
    "X_valid_features = extract_features_ax(X_valid)\n",
    "X_test_features = extract_features_ax(X_test)\n",
    "\n",
    "\n",
    "X_total = np.concatenate((X_train_features,X_valid_features, X_test_features), axis=0)\n",
    "print(\"Total dataset shape -> \", X_total.shape)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_total)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test_features)\n",
    "print(\"Test dataset shape -> \",X_test_scaled.shape)\n",
    "X_train_scaled = scaler.transform(X_train_features)\n",
    "print(\"Train dataset shape -> \",X_train_scaled.shape)\n",
    "X_valid_scaled = scaler.transform(X_valid_features)\n",
    "print(\"Validation dataset shape -> \",X_valid_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: 672 features\n"
     ]
    }
   ],
   "source": [
    "# Create a feature selctor base on a random forest\n",
    "sfm = SelectFromModel(RandomForestClassifier(n_estimators=10000, random_state=seed, n_jobs=-1), threshold=0.0001)\n",
    "\n",
    "# Train the classifier\n",
    "sfm.fit(X_train_scaled, Y)\n",
    "print(f\"Using: {np.sum(sfm.get_support())} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=200,\n",
    "                                       random_state=seed,\n",
    "                                       n_jobs=-1,\n",
    "                                       verbose=False)\n",
    "gb = GradientBoostingRegressor(random_state=seed,\n",
    "                                       n_estimators=100,\n",
    "                                       max_depth=5,\n",
    "                                       learning_rate=0.01)\n",
    "\n",
    "classifiers = [rf, gb]\n",
    "classifiers_names = [\"RandomForestRegressor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fitting\n"
     ]
    }
   ],
   "source": [
    "#Validate the model\n",
    "#X_train_importance = sfm.transform(X_train_scaled)\n",
    "#X_valid_importance = sfm.transform(X_valid_scaled)\n",
    "#print(\"Transformed train set shape -> \",X_train_importance.shape)\n",
    "#print(\"Transformed validation set shape -> \",X_valid_importance.shape)\n",
    "\n",
    "print(\"start fitting\")\n",
    "rf.fit(X_train_scaled, Y)\n",
    "p = rf.predict(X_valid_scaled)\n",
    "\n",
    "gb.fit(X_train_scaled, Y)\n",
    "p2 = gb.predict(X_valid_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39  0.17  0.52  0.61  0.635 0.72  0.715 0.66  0.825 0.805 0.83  0.35\n",
      " 0.58  0.465 0.585 0.46  0.515 0.57  0.35  0.57  0.745 0.775 0.75  0.57\n",
      " 0.67  0.755 0.77  0.77  0.77  0.635 0.49  0.435 0.5   0.42  0.455 0.335\n",
      " 0.385 0.28  0.295 0.475 0.155 0.21  0.18  0.23  0.15  0.615 0.605]\n",
      "[0.4515653  0.21234479 0.32907333 0.58903194 0.59594421 0.72257813\n",
      " 0.68266489 0.67974114 0.82580525 0.82580525 0.74065696 0.21234479\n",
      " 0.68581685 0.58858062 0.63324197 0.59166513 0.59855177 0.68581685\n",
      " 0.21234479 0.32949042 0.83811973 0.83140143 0.8444053  0.8444053\n",
      " 0.8444053  0.8444053  0.8444053  0.8444053  0.8444053  0.8444053\n",
      " 0.46985229 0.47810706 0.45137898 0.56371542 0.68223853 0.27407741\n",
      " 0.24953438 0.2192392  0.29532866 0.32617967 0.29778637 0.28661661\n",
      " 0.41304372 0.30289356 0.22741127 0.75319613 0.75523996]\n",
      "[0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(p)\n",
    "print(p2)\n",
    "p3 = (p+p2)/2\n",
    "print(Y_valid)\n",
    "roc_auc = roc_auc_score(Y_valid, p)\n",
    "roc_auc2 = roc_auc_score(Y_valid, p2)\n",
    "roc_auc3 = roc_auc_score(Y_valid, p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6676245210727969\n",
      "0.5229885057471264\n",
      "0.6264367816091954\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc)\n",
    "print(roc_auc2)\n",
    "print(roc_auc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "0: current roc_auc score: 0.6483253588516747\n",
      "1: current roc_auc score: 0.6698564593301436\n",
      "2: current roc_auc score: 0.648989898989899\n",
      "3: current roc_auc score: 0.6578282828282829\n",
      "4: current roc_auc score: 0.5101010101010102\n",
      "5: current roc_auc score: 0.8106060606060607\n",
      "6: current roc_auc score: 0.6363636363636364\n",
      "7: current roc_auc score: 0.7070707070707072\n",
      "8: current roc_auc score: 0.8181818181818182\n",
      "9: current roc_auc score: 0.7083333333333334\n",
      "========================================\n",
      "RandomForestRegressor roc_auc avg score 0.6815656565656566 +/- 0.08425700612627685\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "clf_scores_avg = []\n",
    "clf_scores_std = []\n",
    "\n",
    "\n",
    "print(\"Start\")\n",
    "for clf in classifiers:\n",
    "    roc_auc_scores = []\n",
    "    for train, test in kfold.split(X_train_scaled, Y):\n",
    "        X_fold = X_train_scaled[train]\n",
    "        y_fold = Y[train]\n",
    "        X_fold_test = X_train_scaled[test]\n",
    "        y_valid = Y[test]\n",
    "        # use only important features\n",
    "        X_important_train = sfm.transform(X_fold)\n",
    "        X_important_test = sfm.transform(X_fold_test)\n",
    "        \n",
    "        # fit classifier\n",
    "        clf.fit(X_important_train, y_fold)\n",
    "        \n",
    "        y_pred = clf.predict(X_important_test)\n",
    "        y_true = Y[test]\n",
    "        \n",
    "        roc_auc = roc_auc_score(y_true, y_pred)\n",
    "        print(f\"{len(roc_auc_scores)}: current roc_auc score: {roc_auc}\")\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "        \n",
    "    clf_scores_avg.append(np.mean(roc_auc_scores))\n",
    "    clf_scores_std.append(np.std(roc_auc_scores))\n",
    "    print(\"========================================\")\n",
    "for i in range(len(classifiers)):\n",
    "    print(f\"{classifiers_names[i]} roc_auc avg score {clf_scores_avg[i]} +/- {clf_scores_std[i]}\" )\n",
    "    \n",
    "# RandomForestRegressor roc_auc avg score 0.9351475279106859 +/- 0.0373456711654917 with 2000 estimators\n",
    "# RandomForestRegressor roc_auc avg score 0.9357589048378522 +/- 0.038307405172097714 with 5000 estimators\n",
    "# RandomForestRegressor roc_auc avg score 0.941208133971292 +/- 0.03181291905225851 with min max features 2000 estimators\n",
    "# RandomForestRegressor roc_auc avg score 0.9365297713981926 +/- 0.0370342614578307 with pixel sum feature 2000 estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fitting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=-1,\n",
       "           oob_score=False, random_state=42, verbose=False,\n",
       "           warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict with one classifiers\n",
    "X_train_importance = sfm.transform(X_train_scaled)\n",
    "X_test_importance = sfm.transform(X_test_scaled)\n",
    "print(\"start fitting\")\n",
    "rf.fit(X_train_importance, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.421  0.6045 0.798  0.6605 0.4745 0.39   0.6235 0.725  0.784  0.806\n",
      " 0.187  0.0935 0.4935 0.347  0.1665 0.823  0.516  0.7005 0.803  0.544\n",
      " 0.517  0.687  0.7335 0.3085 0.6805 0.5605 0.2265 0.675  0.4035 0.31\n",
      " 0.4145 0.5035 0.4345 0.416  0.44   0.6675 0.6805 0.608  0.8005 0.697\n",
      " 0.215  0.41   0.325  0.431  0.362  0.787  0.258  0.5635 0.662  0.655\n",
      " 0.281  0.4345 0.307  0.447  0.278  0.372  0.6435 0.792  0.7915 0.4625\n",
      " 0.215  0.658  0.8095 0.765  0.7625 0.915  0.5565 0.785  0.6365]\n",
      "Extracting ids from test set videos\n"
     ]
    }
   ],
   "source": [
    "p_rf = rf.predict(X_test_importance)\n",
    "print(p_rf)\n",
    "make_submission(\"ax_rf_pixelSum_only_feature_2000.csv\", p_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
