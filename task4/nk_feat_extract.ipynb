{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import natsort\n",
    "import random as rn\n",
    "import skvideo.io\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import (SVC, SVR)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import (AdaBoostRegressor, RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, BaggingRegressor)\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videos_from_folder(data_folder):\n",
    "    '''\n",
    "    get a list of video x wehre each video is a numpy array in the format [n_frames,width,height] \n",
    "    with uint8 elements.\n",
    "    argument: relative path to the data_folder from the source folder.\n",
    "    '''\n",
    "    data_folder = os.path.join(dir_path,data_folder)\n",
    "    x = []\n",
    "    file_names = []\n",
    "    \n",
    "    if os.path.isdir(data_folder):\n",
    "        for dirpath, dirnames, filenames in os.walk(data_folder):\n",
    "            filenames = natsort.natsorted(filenames,reverse=False)\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                statinfo = os.stat(file_path)\n",
    "                if statinfo.st_size != 0:\n",
    "                    video = skvideo.io.vread(file_path, outputdict={\"-pix_fmt\": \"gray\"})[:, :, :, 0]\n",
    "                    x.append(video)\n",
    "                    file_names.append(int(filename.split(\".\")[0]))\n",
    "\n",
    "    indices = sorted(range(len(file_names)), key=file_names.__getitem__)\n",
    "    x = np.take(x,indices)\n",
    "    return x\n",
    "\n",
    "def get_target_from_csv(csv_file):\n",
    "    '''\n",
    "    get a numpy array y of labels. the order follows the id of video. \n",
    "    argument: relative path to the csv_file from the source folder.\n",
    "    '''\n",
    "    csv_file = os.path.join(dir_path,csv_file)\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        label_reader = pd.read_csv(csvfile)\n",
    "        #print(\"Labels: \", label_reader['id'])\n",
    "        y = label_reader['y']\n",
    "        \n",
    "    y = np.array(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(videos):\n",
    "\n",
    "    #Extracting features\n",
    "    height_im = videos[0][0].shape[0]\n",
    "    \n",
    "    X_video_features = []\n",
    "    \n",
    "    for video in tqdm(videos):\n",
    "        all_features = []\n",
    "        n_parts = len(video)\n",
    "        for part in video:\n",
    "            feature_vec = []\n",
    "            part_grad = np.diff(part,axis = 0) #frame-wise gradient\n",
    "            height_im_grad = part_grad.shape[1]\n",
    "            width_im_grad = part_grad.shape[2]\n",
    "\n",
    "            # compute the sum of pixels per frame\n",
    "            heartBeatApprox = np.sum(np.sum(part, axis=1), axis=1)\n",
    "            feature_vec.append(np.min(heartBeatApprox))\n",
    "            feature_vec.append(np.max(heartBeatApprox))\n",
    "            feature_vec.append(np.mean(heartBeatApprox))\n",
    "            feature_vec.append(np.std(heartBeatApprox))\n",
    "            feature_vec.append(np.std(heartBeatApprox))\n",
    "            feature_vec += heartBeatApprox.tolist()\n",
    "\n",
    "\n",
    "            for idx in range(height_im):\n",
    "                feature_vec.append(np.mean(part[:,:,idx])) #entire video video mean_column_pixels            1\n",
    "                feature_vec.append(np.mean(part[:,idx,:])) #entire video mean_row_pixels             2\n",
    "                feature_vec.append(np.std(part[:,:,idx])) #entire video std_column_pixels            3\n",
    "                feature_vec.append(np.std(part[:,idx,:])) #entire video std_row_pixels               4\n",
    "                feature_vec.append(np.count_nonzero(part[:,:,idx])) #entire nonzero_column_pixels    5\n",
    "                feature_vec.append(np.count_nonzero(part[:,idx,:])) #entire nonzero_row_pixels       6\n",
    "\n",
    "            # ? TODO ? Taking some of the same previous features but just frame-wise?\n",
    "            feature_vec.append(np.mean(part_grad)) #mean_video_grad    1\n",
    "            feature_vec.append(np.std(part_grad)) #std_video_grad      2\n",
    "            feature_vec.append(np.mean(part_grad)) #mean_frame_grad    3\n",
    "\n",
    "\n",
    "            for frame_grad in part_grad:\n",
    "                feature_vec.append(np.mean(frame_grad))\n",
    "                feature_vec.append(np.std(frame_grad)) #Std of the gradient of the single frame std_frame_grad      1\n",
    "\n",
    "                for idx in range(height_im_grad):            \n",
    "                    feature_vec.append(np.mean(frame_grad[:,idx])) #mean_grad_column_pixels   1\n",
    "                    feature_vec.append(np.mean(frame_grad[idx,:])) #mean_grad_row_pixels           2\n",
    "                    feature_vec.append(np.std(frame_grad[:,idx])) #std_grad_column_pixels          3\n",
    "                    feature_vec.append(np.std(frame_grad[idx,:])) #std_grad_row_pixels               4\n",
    "                    feature_vec.append(np.count_nonzero(frame_grad[:,idx])) #nonzero_grad_column_pixels  5\n",
    "                    feature_vec.append(np.count_nonzero(frame_grad[idx,:])) #nonzero_grad_row_pixels     6\n",
    "\n",
    "            total_features = len(feature_vec)\n",
    "            all_features.append(feature_vec)\n",
    "\n",
    "        X_features = np.zeros((n_parts,total_features))\n",
    "\n",
    "        for i in range(n_parts):\n",
    "            X_features[i,:] = all_features[i]\n",
    "        \n",
    "        X_video_features.append(X_features)\n",
    "    return np.concatenate([X_video_features])\n",
    "\n",
    "def make_submission(filename, predictions):\n",
    "    ids = extract_ids(test_folder)\n",
    "    df = pd.DataFrame({'id':ids, 'y':predictions})\n",
    "    df[[\"id\", \"y\"]].to_csv(\"submissions/\"+filename, index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dir ->  C:\\Development\\AML-18\\task4\n",
      "Train folder ->  C:\\Development\\AML-18\\task4\\data/train/\n",
      "Train target ->  C:\\Development\\AML-18\\task4\\data/train_target.csv\n",
      "Test folder ->  C:\\Development\\AML-18\\task4\\data/test/\n",
      "Train Data\n",
      "\n",
      "Test Data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed=42\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "train_folder = os.path.join(dir_path,\"data/train/\")\n",
    "test_folder = os.path.join(dir_path,\"data/test/\")\n",
    "\n",
    "train_target = os.path.join(dir_path,'data/train_target.csv')\n",
    "\n",
    "print(\"Current dir -> \", dir_path)\n",
    "print(\"Train folder -> \",train_folder)\n",
    "print(\"Train target -> \",train_target)\n",
    "print(\"Test folder -> \",test_folder)\n",
    "\n",
    "#Load data from csv file\n",
    "print(\"Train Data\\n\")\n",
    "x_train = get_videos_from_folder(train_folder) #List of numpy arrays\n",
    "y_train = get_target_from_csv(train_target) #Numpy array of labels\n",
    "print(\"Test Data\\n\")\n",
    "x_test = get_videos_from_folder(test_folder) #List of numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def split_into_parts(x_data, n_frames, y_data=None):\n",
    "\n",
    "    height_im = x_data[0].shape[1]\n",
    "    width_im = x_data[0].shape[2]\n",
    "    \n",
    "    videos = []\n",
    "    video_ids = []\n",
    "    video_labels = []\n",
    "    \n",
    "    n_videos = x_data.shape[0]\n",
    "\n",
    "    for v_id in range(n_videos):\n",
    "        video = x_data[v_id]\n",
    "        if y_data is not None:\n",
    "            label = y_data[v_id]\n",
    "            \n",
    "        n_subsamples = int(video.shape[0]/n_frames)\n",
    "        parts = []\n",
    "        parts_v_ids = []\n",
    "        parts_labels = []\n",
    "        \n",
    "        for i in range(n_subsamples):\n",
    "            from_frame = i*n_frames\n",
    "            to_frame = from_frame + n_frames\n",
    "            parts.append(video[from_frame:to_frame,:,:])\n",
    "            parts_v_ids.append(v_id)\n",
    "            \n",
    "            if y_data is not None:\n",
    "                parts_labels.append(label)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        videos.append(np.concatenate([parts]))\n",
    "        video_ids.append(np.concatenate([parts_v_ids]))\n",
    "        \n",
    "        if y_data is not None:\n",
    "            video_labels.append(np.concatenate([parts_labels]))\n",
    "            \n",
    "    X = np.concatenate([videos])\n",
    "    v_idx = np.concatenate([video_ids])\n",
    "    \n",
    "    if y_data is not None:\n",
    "        y = np.concatenate([video_labels])\n",
    "        \n",
    "        return X, v_idx, y\n",
    "        \n",
    "    else:\n",
    "        return X, v_idx\n",
    "    \n",
    "\n",
    "def combine_parts_pred(y_pred, v_idx):\n",
    "    d = {}\n",
    "    for v_id, pred in zip(v_idx, y_pred):\n",
    "        if v_id not in d:\n",
    "            d[v_id] = []\n",
    "        \n",
    "        d[v_id].append(pred)\n",
    "        \n",
    "    \n",
    "    results = []\n",
    "    for v_id, preds in d.items():\n",
    "        results.append({\"id\":v_id,\"y\":sum(preds) / float(len(preds))})\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature selctor base on a random forest\n",
    "sfm = SelectFromModel(RandomForestClassifier(n_estimators=10000, random_state=seed, n_jobs=-1), threshold=0.0001)\n",
    "\n",
    "# Train the classifier\n",
    "sfm.fit(X_train_scaled, Y)\n",
    "print(f\"Using: {np.sum(sfm.get_support())} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=2000,\n",
    "                                       random_state=seed,\n",
    "                                       n_jobs=-1,\n",
    "                                       verbose=False)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=2000, \n",
    "                             criterion='gini', \n",
    "                             max_depth=None, \n",
    "                             min_samples_split=2, \n",
    "                             min_samples_leaf=1,\n",
    "                             min_weight_fraction_leaf=0.0, \n",
    "                             max_features='auto', \n",
    "                             max_leaf_nodes=None, \n",
    "                             min_impurity_decrease=0.0, \n",
    "                             min_impurity_split=None, \n",
    "                             bootstrap=True, \n",
    "                             oob_score=False, n_jobs=-1, \n",
    "                             random_state=seed, \n",
    "                             verbose=0, \n",
    "                             warm_start=False, \n",
    "                             class_weight=None)\n",
    "\n",
    "classifiers = [rf]\n",
    "classifiers_names = [\"RandomForestRegressor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (158,)\n",
      "y_train: (158,)\n",
      "x_test: (69,)\n",
      "Splitting videos into parts...\n",
      "Extracting features from parts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8870ccac10f54b978f5f74ac47ed8bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=158), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "xtrain_parts_features: (158,)\n",
      "xtrain_parts_features[0]: (3, 12804)\n",
      "Unrolled: (402, 12804)\n",
      "Fitting Standard Scalar...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_frames = 22\n",
    "\n",
    "\n",
    "print(f\"x_train: {x_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"x_test: {x_test.shape}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print(\"Splitting videos into parts...\")\n",
    "x_train_parts, train_idx_parts, y_train_parts = split_into_parts(x_data=x_train, n_frames=n_frames, y_data=y_train)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Extracting features from parts...\")\n",
    "x_train_parts_features = extract_features(x_train_parts)\n",
    "\n",
    "print(f\"xtrain_parts_features: {x_train_parts_features.shape}\")\n",
    "print(f\"xtrain_parts_features[0]: {x_train_parts_features[0].shape}\")\n",
    "\n",
    "x_train_parts_features_unrolled = np.concatenate(x_train_parts_features)\n",
    "print(f\"Unrolled: {x_train_parts_features_unrolled.shape}\")\n",
    "\n",
    "print(\"Fitting Standard Scalar...\")\n",
    "scaler.fit(x_train_parts_features_unrolled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9242d3cc2df34ac6bf0e9f604e2b8301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Classifier: ', max=1, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b17ffc289f40eb8399efa34841e2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc parts: 0.6940104166666667\n",
      "roc auc video: 0.640625\n",
      "roc auc parts: 0.4624505928853755\n",
      "roc auc video: 0.5625\n",
      "roc auc parts: 0.8530020703933748\n",
      "roc auc video: 0.921875\n",
      "roc auc parts: 0.48639455782312924\n",
      "roc auc video: 0.5625\n",
      "roc auc parts: 0.7946428571428571\n",
      "roc auc video: 0.765625\n",
      "roc auc parts: 0.81203007518797\n",
      "roc auc video: 0.8125\n",
      "roc auc parts: 0.5051652892561984\n",
      "roc auc video: 0.5\n",
      "roc auc parts: 0.4935897435897436\n",
      "roc auc video: 0.5390625\n",
      "roc auc parts: 0.825925925925926\n",
      "roc auc video: 0.859375\n",
      "roc auc parts: 0.782051282051282\n",
      "roc auc video: 0.7755102040816326\n",
      "========================================\n",
      "\n",
      "RandomForestRegressor roc_auc parts avg score 0.6709262810922524 +/- 0.15547129040864743 roc_auc video avg score 0.6939572704081632 +/- 0.14288709232029995\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "clf_scores_parts_avg = []\n",
    "clf_scores_parts_std = []\n",
    "\n",
    "clf_scores_video_avg = []\n",
    "clf_scores_video_std = []\n",
    "\n",
    "verbose = False\n",
    "\n",
    "\n",
    "print(\"Start\")\n",
    "for clf in tqdm(classifiers, desc=\"Classifier: \"):\n",
    "    roc_auc_parts_scores = []\n",
    "    roc_auc_video_scores = []\n",
    "    \n",
    "    for train, valid in tqdm(kfold.split(x_train_parts_features , y_train)):\n",
    "        \n",
    "        # split training set into parts\n",
    "        x_train_fold = x_train_parts_features[train]\n",
    "        y_train_fold = y_train_parts[train] \n",
    "        \n",
    "        # split validation set into parts\n",
    "        x_valid_fold = x_train_parts_features[valid]\n",
    "        y_valid_fold = y_train_parts[valid]\n",
    "        y_valid_fold_videos = y_train[valid]\n",
    "        idx_valid_fold = train_idx_parts[valid]\n",
    "        \n",
    "        # unrolling from [video id, part idx, feature] to [part idx, feature]\n",
    "        x_train_fold_unrolled = np.concatenate(x_train_fold)\n",
    "        y_train_fold_unrolled = np.concatenate(y_train_fold)\n",
    "        \n",
    "        x_valid_fold_unrolled = np.concatenate(x_valid_fold)\n",
    "        y_valid_fold_unrolled = np.concatenate(y_valid_fold)\n",
    "        idx_valid_fold_unrolled = np.concatenate(idx_valid_fold)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"x_train_fold: {x_train_fold.shape}\")\n",
    "            print(f\"y_train_fold: {y_train_fold.shape}\")\n",
    "            print(f\"x_train_fold_unrolled: {x_train_fold_unrolled.shape}\")\n",
    "            print(f\"y_train_fold_unrolled: {y_train_fold_unrolled.shape}\")\n",
    "            \n",
    "            print(f\"x_valid_fold: {x_valid_fold.shape}\")\n",
    "            print(f\"y_valid_fold: {y_valid_fold.shape}\")\n",
    "            print(f\"idx_valid_fold: {idx_valid_fold.shape}\")\n",
    "            print(f\"x_valid_fold_unrolled: {x_valid_fold_unrolled.shape}\")\n",
    "            print(f\"y_valid_fold_unrolled: {y_valid_fold_unrolled.shape}\")\n",
    "            print(f\"idx_valid_fold_unrolled: {idx_valid_fold_unrolled.shape}\")\n",
    "        \n",
    "        # scale the extracted features to zero mean and unit variance \n",
    "        x_train_fold_scaled = scaler.transform(x_train_fold_unrolled)\n",
    "        x_valid_fold_scaled = scaler.transform(x_valid_fold_unrolled)\n",
    "        \n",
    "        \n",
    "        # Shuffle the training data\n",
    "        indices = np.arange(x_train_fold_scaled.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        x_train_fold_scaled = x_train_fold_scaled[indices]\n",
    "        y_train_fold_unrolled = y_train_fold_unrolled[indices]\n",
    "    \n",
    "        # use only important features\n",
    "        #X_important_train = sfm.transform(X_fold)\n",
    "        #X_important_test = sfm.transform(X_fold_test)\n",
    "        \n",
    "        # fit classifier\n",
    "        clf.fit(x_train_fold_scaled, y_train_fold_unrolled)\n",
    "        \n",
    "        y_valid_fold_pred = clf.predict(x_valid_fold_scaled)\n",
    "\n",
    "        # calculate the roc auc based on per part predictions\n",
    "        roc_auc_parts = roc_auc_score(y_true=y_valid_fold_unrolled, y_score=y_valid_fold_pred)\n",
    "        roc_auc_parts_scores.append(roc_auc_parts)\n",
    "        print(f\"roc auc parts: {roc_auc_parts}\")\n",
    "        \n",
    "        # calculate the roc auc based on per video predictions\n",
    "        df = combine_parts_pred(y_pred=y_valid_fold_pred, v_idx=idx_valid_fold_unrolled)\n",
    "        if verbose:\n",
    "            display(df)\n",
    "        roc_auc_video = roc_auc_score(y_true=y_valid_fold_videos, y_score=df['y'].values)\n",
    "        roc_auc_video_scores.append(roc_auc_video)\n",
    "        print(f\"roc auc video: {roc_auc_video}\")\n",
    "        \n",
    "    \n",
    "    clf_scores_parts_avg.append(np.mean(roc_auc_parts_scores))\n",
    "    clf_scores_parts_std.append(np.std(roc_auc_parts_scores))\n",
    "\n",
    "    clf_scores_video_avg.append(np.mean(roc_auc_video_scores))\n",
    "    clf_scores_video_std.append(np.std(roc_auc_video_scores))\n",
    "        \n",
    "    print(\"========================================\")\n",
    "for i in range(len(classifiers)):\n",
    "    print(f\"{classifiers_names[i]} roc_auc parts avg score {clf_scores_parts_avg[i]} +/- {clf_scores_parts_std[i]} roc_auc video avg score {clf_scores_video_avg[i]} +/- {clf_scores_video_std[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict with one classifiers\n",
    "X_train_importance = sfm.transform(X_train_scaled)\n",
    "X_test_importance = sfm.transform(X_test_scaled)\n",
    "print(\"start fitting\")\n",
    "rf.fit(X_train_importance, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.465000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.468333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.567500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.697500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.623333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.412500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.452500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.502500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.631250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.455000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.398333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.498333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.748750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.432500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.743333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.626667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.643333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.607500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.736250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>0.717000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>0.431250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>0.277500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>0.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>0.582000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>0.563750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>0.741667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>0.526667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>0.245000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>0.405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>0.672500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>0.637500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>0.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>60</td>\n",
       "      <td>0.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>61</td>\n",
       "      <td>0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>0.676667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>0.727500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>0.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>0.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>0.655000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>68</td>\n",
       "      <td>0.536667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id         y\n",
       "0    0  0.465000\n",
       "1    1  0.295000\n",
       "2    2  0.468333\n",
       "3    3  0.630000\n",
       "4    4  0.567500\n",
       "5    5  0.520000\n",
       "6    6  0.697500\n",
       "7    7  0.623333\n",
       "8    8  0.520000\n",
       "9    9  0.412500\n",
       "10  10  0.452500\n",
       "11  11  0.502500\n",
       "12  12  0.631250\n",
       "13  13  0.455000\n",
       "14  14  0.398333\n",
       "15  15  0.542500\n",
       "16  16  0.498333\n",
       "17  17  0.748750\n",
       "18  18  0.685000\n",
       "19  19  0.432500\n",
       "20  20  0.445000\n",
       "21  21  0.743333\n",
       "22  22  0.516667\n",
       "23  23  0.480000\n",
       "24  24  0.626667\n",
       "25  25  0.643333\n",
       "26  26  0.607500\n",
       "27  27  0.736250\n",
       "28  28  0.565000\n",
       "29  29  0.580000\n",
       "..  ..       ...\n",
       "39  39  0.390000\n",
       "40  40  0.717000\n",
       "41  41  0.431250\n",
       "42  42  0.277500\n",
       "43  43  0.540000\n",
       "44  44  0.425000\n",
       "45  45  0.582000\n",
       "46  46  0.665000\n",
       "47  47  0.675000\n",
       "48  48  0.563750\n",
       "49  49  0.540000\n",
       "50  50  0.741667\n",
       "51  51  0.526667\n",
       "52  52  0.450000\n",
       "53  53  0.245000\n",
       "54  54  0.405000\n",
       "55  55  0.366667\n",
       "56  56  0.672500\n",
       "57  57  0.700000\n",
       "58  58  0.637500\n",
       "59  59  0.710000\n",
       "60  60  0.635000\n",
       "61  61  0.440000\n",
       "62  62  0.676667\n",
       "63  63  0.727500\n",
       "64  64  0.650000\n",
       "65  65  0.535000\n",
       "66  66  0.505000\n",
       "67  67  0.655000\n",
       "68  68  0.536667\n",
       "\n",
       "[69 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split videos in parts of 22 frames \n",
    "x_test_parts, test_idx_parts = split_into_parts(x_data=x_test, n_frames=n_frames)\n",
    "\n",
    "# extract features for each part\n",
    "x_test_parts_features = extract_features(x_test_parts)\n",
    "\n",
    "# unroll [video, part_id, features] to [part_id, features]\n",
    "x_test_parts_features_unrolled = np.concatenate(x_test_parts_features)\n",
    "test_idx_parts_unrolled = np.concatenate(test_idx_parts)\n",
    "\n",
    "# scale data\n",
    "x_test_scaled = scaler.transform(x_test_parts_features_unrolled)\n",
    "\n",
    "# predict \n",
    "y_test_pred = clf.predict(x_test_scaled)\n",
    "\n",
    "# combine preidctions of multiple parts per video into single prediction by averaging\n",
    "df = combine_parts_pred(y_pred=y_test_pred, v_idx=test_idx_parts_unrolled)\n",
    "\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
