{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import natsort\n",
    "import random as rn\n",
    "import skvideo.io\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import (SVC, SVR)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import (AdaBoostRegressor, RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, BaggingRegressor)\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videos_from_folder(data_folder):\n",
    "    '''\n",
    "    get a list of video x wehre each video is a numpy array in the format [n_frames,width,height] \n",
    "    with uint8 elements.\n",
    "    argument: relative path to the data_folder from the source folder.\n",
    "    '''\n",
    "    data_folder = os.path.join(dir_path,data_folder)\n",
    "    x = []\n",
    "    file_names = []\n",
    "    \n",
    "    if os.path.isdir(data_folder):\n",
    "        for dirpath, dirnames, filenames in os.walk(data_folder):\n",
    "            filenames = natsort.natsorted(filenames,reverse=False)\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                statinfo = os.stat(file_path)\n",
    "                if statinfo.st_size != 0:\n",
    "                    video = skvideo.io.vread(file_path, outputdict={\"-pix_fmt\": \"gray\"})[:, :, :, 0]\n",
    "                    x.append(video)\n",
    "                    file_names.append(int(filename.split(\".\")[0]))\n",
    "\n",
    "    indices = sorted(range(len(file_names)), key=file_names.__getitem__)\n",
    "    x = np.take(x,indices)\n",
    "    return x\n",
    "\n",
    "def get_target_from_csv(csv_file):\n",
    "    '''\n",
    "    get a numpy array y of labels. the order follows the id of video. \n",
    "    argument: relative path to the csv_file from the source folder.\n",
    "    '''\n",
    "    csv_file = os.path.join(dir_path,csv_file)\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        label_reader = pd.read_csv(csvfile)\n",
    "        #print(\"Labels: \", label_reader['id'])\n",
    "        y = label_reader['y']\n",
    "        \n",
    "    y = np.array(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def extract_features_fv(videos):\n",
    "    #Extracting features\n",
    "    height_im = videos[0][0].shape[0]\n",
    "    \n",
    "    X_video_features = []\n",
    "    \n",
    "    for video in tqdm(videos):\n",
    "        all_features = []\n",
    "        n_parts = len(video)\n",
    "        for part in video:\n",
    "            feature_vec = []\n",
    "            part_grad = np.diff(part,axis = 0) #frame-wise gradient\n",
    "            height_im_grad = part_grad.shape[1]\n",
    "            width_im_grad = part_grad.shape[2]\n",
    "\n",
    "            # compute the sum of pixels per frame\n",
    "            heartBeatApprox = np.sum(np.sum(part, axis=1), axis=1)\n",
    "            feature_vec.append(np.min(heartBeatApprox))\n",
    "            feature_vec.append(np.max(heartBeatApprox))\n",
    "            feature_vec.append(np.mean(heartBeatApprox))\n",
    "            feature_vec.append(np.std(heartBeatApprox))\n",
    "            feature_vec += heartBeatApprox.tolist()\n",
    "\n",
    "\n",
    "            for idx in range(height_im):\n",
    "                feature_vec.append(np.mean(part[:,:,idx])) #entire video video mean_column_pixels            1\n",
    "                feature_vec.append(np.mean(part[:,idx,:])) #entire video mean_row_pixels             2\n",
    "                feature_vec.append(np.std(part[:,:,idx])) #entire video std_column_pixels            3\n",
    "                feature_vec.append(np.std(part[:,idx,:])) #entire video std_row_pixels               4\n",
    "                feature_vec.append(np.count_nonzero(part[:,:,idx])) #entire nonzero_column_pixels    5\n",
    "                feature_vec.append(np.count_nonzero(part[:,idx,:])) #entire nonzero_row_pixels       6\n",
    "\n",
    "            # ? TODO ? Taking some of the same previous features but just frame-wise?\n",
    "            feature_vec.append(np.mean(part_grad)) #mean_video_grad    1\n",
    "            feature_vec.append(np.std(part_grad)) #std_video_grad      2\n",
    "            feature_vec.append(np.mean(part_grad)) #mean_frame_grad    3\n",
    "\n",
    "\n",
    "            for frame_grad in part_grad:\n",
    "                feature_vec.append(np.mean(frame_grad))\n",
    "                feature_vec.append(np.std(frame_grad)) #Std of the gradient of the single frame std_frame_grad      1\n",
    "\n",
    "\n",
    "            total_features = len(feature_vec)\n",
    "            all_features.append(feature_vec)\n",
    "\n",
    "        X_features = np.zeros((n_parts,total_features))\n",
    "\n",
    "        for i in range(n_parts):\n",
    "            X_features[i,:] = all_features[i]\n",
    "        \n",
    "        X_video_features.append(X_features)\n",
    "    return np.concatenate([X_video_features])\n",
    "    \n",
    "\n",
    "def extract_features(videos):\n",
    "\n",
    "    #Extracting features\n",
    "    height_im = videos[0][0].shape[0]\n",
    "    \n",
    "    X_video_features = []\n",
    "    \n",
    "    for video in tqdm(videos):\n",
    "        all_features = []\n",
    "        n_parts = len(video)\n",
    "        for part in video:\n",
    "            feature_vec = []\n",
    "            part_grad = np.diff(part,axis = 0) #frame-wise gradient\n",
    "            height_im_grad = part_grad.shape[1]\n",
    "            width_im_grad = part_grad.shape[2]\n",
    "\n",
    "            # compute the sum of pixels per frame\n",
    "            heartBeatApprox = np.sum(np.sum(part, axis=1), axis=1)\n",
    "            feature_vec.append(np.min(heartBeatApprox))\n",
    "            feature_vec.append(np.max(heartBeatApprox))\n",
    "            feature_vec.append(np.mean(heartBeatApprox))\n",
    "            feature_vec.append(np.std(heartBeatApprox))\n",
    "            feature_vec += heartBeatApprox.tolist()\n",
    "\n",
    "\n",
    "            for idx in range(height_im):\n",
    "                feature_vec.append(np.mean(part[:,:,idx])) #entire video video mean_column_pixels            1\n",
    "                feature_vec.append(np.mean(part[:,idx,:])) #entire video mean_row_pixels             2\n",
    "                feature_vec.append(np.std(part[:,:,idx])) #entire video std_column_pixels            3\n",
    "                feature_vec.append(np.std(part[:,idx,:])) #entire video std_row_pixels               4\n",
    "                feature_vec.append(np.count_nonzero(part[:,:,idx])) #entire nonzero_column_pixels    5\n",
    "                feature_vec.append(np.count_nonzero(part[:,idx,:])) #entire nonzero_row_pixels       6\n",
    "\n",
    "            # ? TODO ? Taking some of the same previous features but just frame-wise?\n",
    "            feature_vec.append(np.mean(part_grad)) #mean_video_grad    1\n",
    "            feature_vec.append(np.std(part_grad)) #std_video_grad      2\n",
    "            feature_vec.append(np.mean(part_grad)) #mean_frame_grad    3\n",
    "\n",
    "\n",
    "            for frame_grad in part_grad:\n",
    "                feature_vec.append(np.mean(frame_grad))\n",
    "                feature_vec.append(np.std(frame_grad)) #Std of the gradient of the single frame std_frame_grad      1\n",
    "\n",
    "                for idx in range(height_im_grad):            \n",
    "                    feature_vec.append(np.mean(frame_grad[:,idx])) #mean_grad_column_pixels   1\n",
    "                    feature_vec.append(np.mean(frame_grad[idx,:])) #mean_grad_row_pixels           2\n",
    "                    feature_vec.append(np.std(frame_grad[:,idx])) #std_grad_column_pixels          3\n",
    "                    feature_vec.append(np.std(frame_grad[idx,:])) #std_grad_row_pixels               4\n",
    "                    feature_vec.append(np.count_nonzero(frame_grad[:,idx])) #nonzero_grad_column_pixels  5\n",
    "                    feature_vec.append(np.count_nonzero(frame_grad[idx,:])) #nonzero_grad_row_pixels     6\n",
    "\n",
    "            total_features = len(feature_vec)\n",
    "            all_features.append(feature_vec)\n",
    "\n",
    "        X_features = np.zeros((n_parts,total_features))\n",
    "\n",
    "        for i in range(n_parts):\n",
    "            X_features[i,:] = all_features[i]\n",
    "        \n",
    "        X_video_features.append(X_features)\n",
    "    return np.concatenate([X_video_features])\n",
    "\n",
    "def make_submission(filename, predictions):\n",
    "    ids = extract_ids(test_folder)\n",
    "    df = pd.DataFrame({'id':ids, 'y':predictions})\n",
    "    df[[\"id\", \"y\"]].to_csv(\"submissions/\"+filename, index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dir ->  /home/ax/master/2018_02/ml/task4\n",
      "Train folder ->  /home/ax/master/2018_02/ml/task4/data/train/\n",
      "Train target ->  /home/ax/master/2018_02/ml/task4/data/train_target.csv\n",
      "Test folder ->  /home/ax/master/2018_02/ml/task4/data/test/\n",
      "Train Data\n",
      "\n",
      "Test Data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed=42\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "train_folder = os.path.join(dir_path,\"data/train/\")\n",
    "test_folder = os.path.join(dir_path,\"data/test/\")\n",
    "\n",
    "train_target = os.path.join(dir_path,'data/train_target.csv')\n",
    "\n",
    "print(\"Current dir -> \", dir_path)\n",
    "print(\"Train folder -> \",train_folder)\n",
    "print(\"Train target -> \",train_target)\n",
    "print(\"Test folder -> \",test_folder)\n",
    "\n",
    "#Load data from csv file\n",
    "print(\"Train Data\\n\")\n",
    "x_train = get_videos_from_folder(train_folder) #List of numpy arrays\n",
    "y_train = get_target_from_csv(train_target) #Numpy array of labels\n",
    "print(\"Test Data\\n\")\n",
    "x_test = get_videos_from_folder(test_folder) #List of numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_parts(x_data, n_frames, y_data=None):\n",
    "\n",
    "    height_im = x_data[0].shape[1]\n",
    "    width_im = x_data[0].shape[2]\n",
    "    \n",
    "    videos = []\n",
    "    video_ids = []\n",
    "    video_labels = []\n",
    "    \n",
    "    n_videos = x_data.shape[0]\n",
    "\n",
    "    for v_id in range(n_videos):\n",
    "        video = x_data[v_id]\n",
    "        if y_data is not None:\n",
    "            label = y_data[v_id]\n",
    "            \n",
    "        n_subsamples = int(video.shape[0]/n_frames)\n",
    "        parts = []\n",
    "        parts_v_ids = []\n",
    "        parts_labels = []\n",
    "        \n",
    "        for i in range(n_subsamples):\n",
    "            from_frame = i*n_frames\n",
    "            to_frame = from_frame + n_frames\n",
    "            parts.append(video[from_frame:to_frame,:,:])\n",
    "            parts_v_ids.append(v_id)\n",
    "            \n",
    "            if y_data is not None:\n",
    "                parts_labels.append(label)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        videos.append(np.concatenate([parts]))\n",
    "        video_ids.append(np.concatenate([parts_v_ids]))\n",
    "        \n",
    "        if y_data is not None:\n",
    "            video_labels.append(np.concatenate([parts_labels]))\n",
    "            \n",
    "    X = np.concatenate([videos])\n",
    "    v_idx = np.concatenate([video_ids])\n",
    "    \n",
    "    if y_data is not None:\n",
    "        y = np.concatenate([video_labels])\n",
    "        \n",
    "        return X, v_idx, y\n",
    "        \n",
    "    else:\n",
    "        return X, v_idx\n",
    "    \n",
    "\n",
    "def combine_parts_pred(y_pred, v_idx):\n",
    "    d = {}\n",
    "    for v_id, pred in zip(v_idx, y_pred):\n",
    "        if v_id not in d:\n",
    "            d[v_id] = []\n",
    "        \n",
    "        d[v_id].append(pred)\n",
    "        \n",
    "    \n",
    "    results = []\n",
    "    for v_id, preds in d.items():\n",
    "        results.append({\"id\":v_id,\"y\":sum(preds) / float(len(preds))})\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=2000,\n",
    "                                       random_state=seed,\n",
    "                                       n_jobs=-1,\n",
    "                                       verbose=False)\n",
    "\n",
    "gb = GradientBoostingRegressor(random_state=seed,\n",
    "                                       n_estimators=500,\n",
    "                                       max_depth=10,\n",
    "                                       learning_rate=0.01)\n",
    "\n",
    "classifiers = [rf, gb]\n",
    "classifiers_names = [\"RandomForestRegressor\", \"GradientBoostingRegressor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (158,)\n",
      "y_train: (158,)\n",
      "x_test: (69,)\n",
      "Splitting videos into parts...\n",
      "Extracting features from parts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e329141e3d43b99258054cb460ce65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=158), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d22d98872764bc3880c1432edca8aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=158), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "xtrain_parts_features: (158,)\n",
      "xtrain_parts_features[0]: (3, 12803)\n",
      "Unrolled: (402, 12803)\n",
      "Unrolled GB: (402, 203)\n",
      "Fitting Standard Scalar...\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train_parts_features\n",
    "x_test = x_test\n",
    "    \n",
    "    # split validation set into parts\n",
    "x_valid_fold = x_train_parts_features[valid]\n",
    "    x_valid_fold_gb = x_train_parts_features_gb[valid]\n",
    "    y_valid_fold = y_train_parts[valid]\n",
    "    y_valid_fold_videos = y_train[valid]\n",
    "    idx_valid_fold = train_idx_parts[valid]\n",
    "\n",
    "    # unrolling from [video id, part idx, feature] to [part idx, feature]\n",
    "    x_train_fold_unrolled = np.concatenate(x_train_fold)\n",
    "    x_train_fold_unrolled_gb = np.concatenate(x_train_fold_gb)\n",
    "    y_train_fold_unrolled = np.concatenate(y_train_fold)\n",
    "\n",
    "    x_valid_fold_unrolled = np.concatenate(x_valid_fold)\n",
    "    x_valid_fold_unrolled_gb = np.concatenate(x_valid_fold_gb)\n",
    "    y_valid_fold_unrolled = np.concatenate(y_valid_fold)\n",
    "    idx_valid_fold_unrolled = np.concatenate(idx_valid_fold)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"x_train_fold: {x_train_fold.shape}\")\n",
    "        print(f\"y_train_fold: {y_train_fold.shape}\")\n",
    "        print(f\"x_train_fold_unrolled: {x_train_fold_unrolled.shape}\")\n",
    "        print(f\"y_train_fold_unrolled: {y_train_fold_unrolled.shape}\")\n",
    "\n",
    "        print(f\"x_valid_fold: {x_valid_fold.shape}\")\n",
    "        print(f\"y_valid_fold: {y_valid_fold.shape}\")\n",
    "        print(f\"idx_valid_fold: {idx_valid_fold.shape}\")\n",
    "        print(f\"x_valid_fold_unrolled: {x_valid_fold_unrolled.shape}\")\n",
    "        print(f\"y_valid_fold_unrolled: {y_valid_fold_unrolled.shape}\")\n",
    "        print(f\"idx_valid_fold_unrolled: {idx_valid_fold_unrolled.shape}\")\n",
    "\n",
    "    # scale the extracted features to zero mean and unit variance \n",
    "    x_train_fold_scaled = scaler.transform(x_train_fold_unrolled)\n",
    "    x_valid_fold_scaled = scaler.transform(x_valid_fold_unrolled)\n",
    "    x_train_fold_scaled_gb = scaler_gb.transform(x_train_fold_unrolled_gb)\n",
    "    x_valid_fold_scaled_gb = scaler_gb.transform(x_valid_fold_unrolled_gb)\n",
    "\n",
    "\n",
    "    # Shuffle the training data\n",
    "    indices = np.arange(x_train_fold_scaled.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    x_train_fold_scaled = x_train_fold_scaled[indices]\n",
    "    y_train_fold_unrolled = y_train_fold_unrolled[indices]\n",
    "    \n",
    "    indices_gb = np.arange(x_train_fold_scaled_gb.shape[0])\n",
    "    np.random.shuffle(indices_gb)\n",
    "    x_train_fold_scaled_gb = x_train_fold_scaled_gb[indices_gb]\n",
    "    y_train_fold_unrolled_gb = y_train_fold_unrolled[indices_gb]\n",
    "\n",
    "    # use only important features only RF\n",
    "    X_important_train = sfm.transform(x_train_fold_scaled)\n",
    "    X_important_valid = sfm.transform(x_valid_fold_scaled)\n",
    "\n",
    "    # fit classifier\n",
    "    rf.fit(X_important_train, y_train_fold_unrolled)\n",
    "    #gb.fit(x_train_fold_scaled_gb, y_train_fold_unrolled_gb)\n",
    "    gb.fit(X_important_train, y_train_fold_unrolled)\n",
    "\n",
    "    y_valid_fold_pred = rf.predict(X_important_valid)\n",
    "    #y_valid_fold_pred = rf.predict(x_valid_fold_scaled_gb)\n",
    "    #y_valid_fold_pred_gb = gb.predict(x_valid_fold_scaled_gb)\n",
    "    y_valid_fold_pred_gb = gb.predict(X_important_valid)\n",
    "\n",
    "    # calculate the roc auc based on per part predictions\n",
    "    roc_auc_parts = roc_auc_score(y_true=y_valid_fold_unrolled, y_score=y_valid_fold_pred)\n",
    "    roc_auc_parts_scores.append(roc_auc_parts)\n",
    "    print(f\"roc auc parts: {roc_auc_parts}\")\n",
    "\n",
    "    # calculate the roc auc based on per video predictions\n",
    "    df = combine_parts_pred(y_pred=y_valid_fold_pred, v_idx=idx_valid_fold_unrolled)\n",
    "    roc_auc_video = roc_auc_score(y_true=y_valid_fold_videos, y_score=df['y'].values)\n",
    "    roc_auc_video_scores.append(roc_auc_video)\n",
    "    print(f\"roc auc video: {roc_auc_video}\")\n",
    "        \n",
    "    # calculate the roc auc based on per part predictions\n",
    "    roc_auc_parts_gb = roc_auc_score(y_true=y_valid_fold_unrolled, y_score=y_valid_fold_pred_gb)\n",
    "    roc_auc_parts_scores_gb.append(roc_auc_parts_gb)\n",
    "    print(f\"roc auc part GBs: {roc_auc_parts_gb}\")\n",
    "\n",
    "    # calculate the roc auc based on per video predictions\n",
    "    df = combine_parts_pred(y_pred=y_valid_fold_pred_gb, v_idx=idx_valid_fold_unrolled)\n",
    "    roc_auc_video_gb = roc_auc_score(y_true=y_valid_fold_videos, y_score=df['y'].values)\n",
    "    roc_auc_video_scores_gb.append(roc_auc_video_gb)\n",
    "    print(f\"roc auc video GB: {roc_auc_video_gb}\")\n",
    "    \n",
    "    # Combined approach\n",
    "    y_combine_pred = (y_valid_fold_pred + y_valid_fold_pred_gb) / 2.0\n",
    "    # calculate the roc auc based on per part predictions\n",
    "    roc_auc_parts_avg = roc_auc_score(y_true=y_valid_fold_unrolled, y_score=y_combine_pred)\n",
    "    roc_auc_parts_scores_avg.append(roc_auc_parts_avg)\n",
    "    print(f\"roc auc part AVG: {roc_auc_parts_avg}\")\n",
    "\n",
    "    # calculate the roc auc based on per video predictions\n",
    "    df = combine_parts_pred(y_pred=y_combine_pred, v_idx=idx_valid_fold_unrolled)\n",
    "    roc_auc_video_avg = roc_auc_score(y_true=y_valid_fold_videos, y_score=df['y'].values)\n",
    "    roc_auc_video_scores_avg.append(roc_auc_video_avg)\n",
    "    print(f\"roc auc video AVG: {roc_auc_video_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: 2493 features\n"
     ]
    }
   ],
   "source": [
    "# Create a feature selctor base on a random forest\n",
    "sfm = SelectFromModel(RandomForestClassifier(n_estimators=10000, random_state=seed, n_jobs=-1), threshold=0.0001)\n",
    "\n",
    "# Train the classifier\n",
    "\n",
    "sfm.fit(x_train_parts_features_unrolled, np.concatenate(y_train_parts))\n",
    "print(f\"Using: {np.sum(sfm.get_support())} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2f1627333049faa9d5a36c71f4fef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc parts: 0.76953125\n",
      "roc auc video: 0.765625\n",
      "roc auc part GBs: 0.7265625\n",
      "roc auc video GB: 0.8125\n",
      "roc auc part AVG: 0.7604166666666667\n",
      "roc auc video AVG: 0.8125\n",
      "========================================\n",
      "roc auc parts: 0.5177865612648221\n",
      "roc auc video: 0.625\n",
      "roc auc part GBs: 0.5573122529644269\n",
      "roc auc video GB: 0.65625\n",
      "roc auc part AVG: 0.507905138339921\n",
      "roc auc video AVG: 0.640625\n",
      "========================================\n",
      "roc auc parts: 0.9151138716356108\n",
      "roc auc video: 0.9375\n",
      "roc auc part GBs: 0.7256728778467909\n",
      "roc auc video GB: 0.796875\n",
      "roc auc part AVG: 0.7846790890269151\n",
      "roc auc video AVG: 0.84375\n",
      "========================================\n",
      "roc auc parts: 0.5629251700680272\n",
      "roc auc video: 0.640625\n",
      "roc auc part GBs: 0.33673469387755095\n",
      "roc auc video GB: 0.359375\n",
      "roc auc part AVG: 0.391156462585034\n",
      "roc auc video AVG: 0.453125\n",
      "========================================\n",
      "roc auc parts: 0.8184523809523809\n",
      "roc auc video: 0.765625\n",
      "roc auc part GBs: 0.5997023809523808\n",
      "roc auc video GB: 0.609375\n",
      "roc auc part AVG: 0.6726190476190477\n",
      "roc auc video AVG: 0.671875\n",
      "========================================\n",
      "roc auc parts: 0.9022556390977443\n",
      "roc auc video: 0.890625\n",
      "roc auc part GBs: 0.7406015037593985\n",
      "roc auc video GB: 0.7265625\n",
      "roc auc part AVG: 0.8170426065162907\n",
      "roc auc video AVG: 0.8125\n",
      "========================================\n",
      "roc auc parts: 0.628099173553719\n",
      "roc auc video: 0.5625\n",
      "roc auc part GBs: 0.5495867768595042\n",
      "roc auc video GB: 0.4375\n",
      "roc auc part AVG: 0.5702479338842975\n",
      "roc auc video AVG: 0.4375\n",
      "========================================\n",
      "roc auc parts: 0.5921474358974359\n",
      "roc auc video: 0.671875\n",
      "roc auc part GBs: 0.6490384615384616\n",
      "roc auc video GB: 0.6875\n",
      "roc auc part AVG: 0.592948717948718\n",
      "roc auc video AVG: 0.71875\n",
      "========================================\n",
      "roc auc parts: 0.8481481481481481\n",
      "roc auc video: 0.890625\n",
      "roc auc part GBs: 0.5351851851851852\n",
      "roc auc video GB: 0.515625\n",
      "roc auc part AVG: 0.6592592592592593\n",
      "roc auc video AVG: 0.65625\n",
      "========================================\n",
      "roc auc parts: 0.8315018315018315\n",
      "roc auc video: 0.8571428571428571\n",
      "roc auc part GBs: 0.532967032967033\n",
      "roc auc video GB: 0.5612244897959183\n",
      "roc auc part AVG: 0.6556776556776557\n",
      "roc auc video AVG: 0.6938775510204082\n",
      "========================================\n",
      "Random forest roc_auc parts avg score 0.7385961462119719 +/- 0.14113256666849988 roc_auc video avg score 0.7607142857142857 +/- 0.12424757599368337\n",
      "Gradient boosting roc_auc parts avg score 0.5953363665950733 +/- 0.11667674280875547 roc_auc video avg score 0.6162786989795919 +/- 0.1418173636030353\n",
      "Combined roc_auc parts avg score 0.6411952577523806 +/- 0.12445643945031885 roc_auc video avg score 0.6740752551020408 +/- 0.13256269074527158\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "clf_scores_parts_avg = []\n",
    "clf_scores_parts_std = []\n",
    "\n",
    "clf_scores_video_avg = []\n",
    "clf_scores_video_std = []\n",
    "\n",
    "verbose = False\n",
    "\n",
    "\n",
    "roc_auc_parts_scores = []\n",
    "roc_auc_video_scores = []\n",
    "roc_auc_parts_scores_gb = []\n",
    "roc_auc_video_scores_gb = []\n",
    "roc_auc_parts_scores_avg = []\n",
    "roc_auc_video_scores_avg = []\n",
    "\n",
    "\n",
    "print(\"Start\")\n",
    "for train, valid in tqdm(kfold.split(x_train_parts_features , y_train)):\n",
    "    #for clf in tqdm(classifiers, desc=\"Classifier: \"):\n",
    "    # split training set into parts\n",
    "    x_train_fold = x_train_parts_features[train]\n",
    "    x_train_fold_gb = x_train_parts_features_gb[train]\n",
    "    y_train_fold = y_train_parts[train] \n",
    "\n",
    "    # split validation set into parts\n",
    "    x_valid_fold = x_train_parts_features[valid]\n",
    "    x_valid_fold_gb = x_train_parts_features_gb[valid]\n",
    "    y_valid_fold = y_train_parts[valid]\n",
    "    y_valid_fold_videos = y_train[valid]\n",
    "    idx_valid_fold = train_idx_parts[valid]\n",
    "\n",
    "    # unrolling from [video id, part idx, feature] to [part idx, feature]\n",
    "    x_train_fold_unrolled = np.concatenate(x_train_fold)\n",
    "    x_train_fold_unrolled_gb = np.concatenate(x_train_fold_gb)\n",
    "    y_train_fold_unrolled = np.concatenate(y_train_fold)\n",
    "\n",
    "    x_valid_fold_unrolled = np.concatenate(x_valid_fold)\n",
    "    x_valid_fold_unrolled_gb = np.concatenate(x_valid_fold_gb)\n",
    "    y_valid_fold_unrolled = np.concatenate(y_valid_fold)\n",
    "    idx_valid_fold_unrolled = np.concatenate(idx_valid_fold)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"x_train_fold: {x_train_fold.shape}\")\n",
    "        print(f\"y_train_fold: {y_train_fold.shape}\")\n",
    "        print(f\"x_train_fold_unrolled: {x_train_fold_unrolled.shape}\")\n",
    "        print(f\"y_train_fold_unrolled: {y_train_fold_unrolled.shape}\")\n",
    "\n",
    "        print(f\"x_valid_fold: {x_valid_fold.shape}\")\n",
    "        print(f\"y_valid_fold: {y_valid_fold.shape}\")\n",
    "        print(f\"idx_valid_fold: {idx_valid_fold.shape}\")\n",
    "        print(f\"x_valid_fold_unrolled: {x_valid_fold_unrolled.shape}\")\n",
    "        print(f\"y_valid_fold_unrolled: {y_valid_fold_unrolled.shape}\")\n",
    "        print(f\"idx_valid_fold_unrolled: {idx_valid_fold_unrolled.shape}\")\n",
    "\n",
    "    # scale the extracted features to zero mean and unit variance \n",
    "    x_train_fold_scaled = scaler.transform(x_train_fold_unrolled)\n",
    "    x_valid_fold_scaled = scaler.transform(x_valid_fold_unrolled)\n",
    "    x_train_fold_scaled_gb = scaler_gb.transform(x_train_fold_unrolled_gb)\n",
    "    x_valid_fold_scaled_gb = scaler_gb.transform(x_valid_fold_unrolled_gb)\n",
    "\n",
    "\n",
    "    # Shuffle the training data\n",
    "    indices = np.arange(x_train_fold_scaled.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    x_train_fold_scaled = x_train_fold_scaled[indices]\n",
    "    y_train_fold_unrolled = y_train_fold_unrolled[indices]\n",
    "    \n",
    "    indices_gb = np.arange(x_train_fold_scaled_gb.shape[0])\n",
    "    np.random.shuffle(indices_gb)\n",
    "    x_train_fold_scaled_gb = x_train_fold_scaled_gb[indices_gb]\n",
    "    y_train_fold_unrolled_gb = y_train_fold_unrolled[indices_gb]\n",
    "\n",
    "    # use only important features only RF\n",
    "    X_important_train = sfm.transform(x_train_fold_scaled)\n",
    "    X_important_valid = sfm.transform(x_valid_fold_scaled)\n",
    "\n",
    "    # fit classifier\n",
    "    rf.fit(X_important_train, y_train_fold_unrolled)\n",
    "    #gb.fit(x_train_fold_scaled_gb, y_train_fold_unrolled_gb)\n",
    "    gb.fit(X_important_train, y_train_fold_unrolled)\n",
    "\n",
    "    y_valid_fold_pred = rf.predict(X_important_valid)\n",
    "    #y_valid_fold_pred = rf.predict(x_valid_fold_scaled_gb)\n",
    "    #y_valid_fold_pred_gb = gb.predict(x_valid_fold_scaled_gb)\n",
    "    y_valid_fold_pred_gb = gb.predict(X_important_valid)\n",
    "\n",
    "    # calculate the roc auc based on per part predictions\n",
    "    roc_auc_parts = roc_auc_score(y_true=y_valid_fold_unrolled, y_score=y_valid_fold_pred)\n",
    "    roc_auc_parts_scores.append(roc_auc_parts)\n",
    "    print(f\"roc auc parts: {roc_auc_parts}\")\n",
    "\n",
    "    # calculate the roc auc based on per video predictions\n",
    "    df = combine_parts_pred(y_pred=y_valid_fold_pred, v_idx=idx_valid_fold_unrolled)\n",
    "    roc_auc_video = roc_auc_score(y_true=y_valid_fold_videos, y_score=df['y'].values)\n",
    "    roc_auc_video_scores.append(roc_auc_video)\n",
    "    print(f\"roc auc video: {roc_auc_video}\")\n",
    "        \n",
    "    # calculate the roc auc based on per part predictions\n",
    "    roc_auc_parts_gb = roc_auc_score(y_true=y_valid_fold_unrolled, y_score=y_valid_fold_pred_gb)\n",
    "    roc_auc_parts_scores_gb.append(roc_auc_parts_gb)\n",
    "    print(f\"roc auc part GBs: {roc_auc_parts_gb}\")\n",
    "\n",
    "    # calculate the roc auc based on per video predictions\n",
    "    df = combine_parts_pred(y_pred=y_valid_fold_pred_gb, v_idx=idx_valid_fold_unrolled)\n",
    "    roc_auc_video_gb = roc_auc_score(y_true=y_valid_fold_videos, y_score=df['y'].values)\n",
    "    roc_auc_video_scores_gb.append(roc_auc_video_gb)\n",
    "    print(f\"roc auc video GB: {roc_auc_video_gb}\")\n",
    "    \n",
    "    # Combined approach\n",
    "    y_combine_pred = (y_valid_fold_pred + y_valid_fold_pred_gb) / 2.0\n",
    "    # calculate the roc auc based on per part predictions\n",
    "    roc_auc_parts_avg = roc_auc_score(y_true=y_valid_fold_unrolled, y_score=y_combine_pred)\n",
    "    roc_auc_parts_scores_avg.append(roc_auc_parts_avg)\n",
    "    print(f\"roc auc part AVG: {roc_auc_parts_avg}\")\n",
    "\n",
    "    # calculate the roc auc based on per video predictions\n",
    "    df = combine_parts_pred(y_pred=y_combine_pred, v_idx=idx_valid_fold_unrolled)\n",
    "    roc_auc_video_avg = roc_auc_score(y_true=y_valid_fold_videos, y_score=df['y'].values)\n",
    "    roc_auc_video_scores_avg.append(roc_auc_video_avg)\n",
    "    print(f\"roc auc video AVG: {roc_auc_video_avg}\")\n",
    "    \n",
    "        \n",
    "    print(\"========================================\")\n",
    "\n",
    "print(f\"Random forest roc_auc parts avg score {np.mean(roc_auc_parts_scores)} +/- {np.std(roc_auc_parts_scores)} roc_auc video avg score {np.mean(roc_auc_video_scores)} +/- {np.std(roc_auc_video_scores)}\")\n",
    "print(f\"Gradient boosting roc_auc parts avg score {np.mean(roc_auc_parts_scores_gb)} +/- {np.std(roc_auc_parts_scores_gb)} roc_auc video avg score {np.mean(roc_auc_video_scores_gb)} +/- {np.std(roc_auc_video_scores_gb)}\")\n",
    "print(f\"Combined roc_auc parts avg score {np.mean(roc_auc_parts_scores_avg)} +/- {np.std(roc_auc_parts_scores_avg)} roc_auc video avg score {np.mean(roc_auc_video_scores_avg)} +/- {np.std(roc_auc_video_scores_avg)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=-1,\n",
       "           oob_score=False, random_state=42, verbose=False,\n",
       "           warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unrolling from [video id, part idx, feature] to [part idx, feature]\n",
    "x_train_unrolled = np.concatenate(x_train_parts_features)    \n",
    "y_train_unrolled = np.concatenate(y_train_parts)\n",
    "\n",
    "\n",
    "# scale the extracted features to zero mean and unit variance \n",
    "x_train_scaled = scaler.transform(x_train_unrolled)\n",
    "\n",
    "# use only important features only RF\n",
    "X_important_train = sfm.transform(x_train_scaled)\n",
    "    \n",
    "# fit classifier\n",
    "rf.fit(X_important_train, y_train_unrolled)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2d474772e84cf6aab32aa5741e0d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=69), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.424500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.284500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.549667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.682250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.536750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.510250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.695000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.721833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.419750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.402500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.439750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.655625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.461500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.463833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.617875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.474000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.801375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.703250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.484063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.500500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.775167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.601000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.465000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.665667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.628333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.616250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.742375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>0.736000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>0.473375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>0.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>0.414500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>0.572400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0.722750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>0.658250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>0.622125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>0.589167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>0.763167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>0.595500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>0.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>0.234250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>0.322833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>0.677000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>0.684000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>0.703250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>0.757833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>60</td>\n",
       "      <td>0.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>61</td>\n",
       "      <td>0.429250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>0.661333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>0.702250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>0.667500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>0.561500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>0.511333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>0.619750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>68</td>\n",
       "      <td>0.563667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id         y\n",
       "0    0  0.424500\n",
       "1    1  0.284500\n",
       "2    2  0.549667\n",
       "3    3  0.682250\n",
       "4    4  0.536750\n",
       "5    5  0.510250\n",
       "6    6  0.695000\n",
       "7    7  0.721833\n",
       "8    8  0.435000\n",
       "9    9  0.419750\n",
       "10  10  0.402500\n",
       "11  11  0.439750\n",
       "12  12  0.655625\n",
       "13  13  0.461500\n",
       "14  14  0.463833\n",
       "15  15  0.617875\n",
       "16  16  0.474000\n",
       "17  17  0.801375\n",
       "18  18  0.703250\n",
       "19  19  0.484063\n",
       "20  20  0.500500\n",
       "21  21  0.775167\n",
       "22  22  0.601000\n",
       "23  23  0.465000\n",
       "24  24  0.665667\n",
       "25  25  0.628333\n",
       "26  26  0.616250\n",
       "27  27  0.742375\n",
       "28  28  0.680000\n",
       "29  29  0.556000\n",
       "..  ..       ...\n",
       "39  39  0.400000\n",
       "40  40  0.736000\n",
       "41  41  0.473375\n",
       "42  42  0.275000\n",
       "43  43  0.565000\n",
       "44  44  0.414500\n",
       "45  45  0.572400\n",
       "46  46  0.722750\n",
       "47  47  0.658250\n",
       "48  48  0.622125\n",
       "49  49  0.589167\n",
       "50  50  0.763167\n",
       "51  51  0.595500\n",
       "52  52  0.370000\n",
       "53  53  0.234250\n",
       "54  54  0.430000\n",
       "55  55  0.322833\n",
       "56  56  0.677000\n",
       "57  57  0.684000\n",
       "58  58  0.703250\n",
       "59  59  0.757833\n",
       "60  60  0.632000\n",
       "61  61  0.429250\n",
       "62  62  0.661333\n",
       "63  63  0.702250\n",
       "64  64  0.667500\n",
       "65  65  0.561500\n",
       "66  66  0.511333\n",
       "67  67  0.619750\n",
       "68  68  0.563667\n",
       "\n",
       "[69 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAADBJJREFUeJzt3W+IZfddx/H3p1lDTUxsZKdSk0wnlTQYilAdpFoQbVqJWdkULLILKYlUF8UmtQR1BaWiT1YRa8E82cbaoDVB1tLGpvR/Q6mkwd0k2iZrTJuu6dpqtn+0CmqS8vXB3Afjdnfn3nvO3LvzzfsFw8ydOTPn++PuvDl77r1nUlVIkna+Fyx7AEnSOAy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6Qmdi1yZ7t37661tbVF7lKSdrxjx459tapWttpuoUFfW1vj6NGji9ylJO14Sf55mu085SJJTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNLPSVotoZ1g7et7R9nzi0Z2n7fr7xfu7HI3RJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNbFl0JO8K8nTST636XPfk+SjSZ6YvL9se8eUJG1lmiP0dwPXn/a5g8DHq+pq4OOT25KkJdoy6FX1KeDrp336RuCuycd3Aa8feS5J0ozmPYf+vVX1FYDJ+xePN5IkaR7b/qBokgNJjiY5eurUqe3enSQ9b80b9H9L8hKAyfunz7ZhVR2uqvWqWl9ZWZlzd5Kkrcwb9HuBmycf3wy8f5xxJEnzmuZpi3cDDwDXJDmZ5E3AIeB1SZ4AXje5LUlaol1bbVBV+8/ypetGnkWSNICvFJWkJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWpiy6stSou0dvC+pez3xKE9S9mvNCaP0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYGBT3JW5M8muRzSe5O8sKxBpMkzWbuoCe5HLgNWK+qVwAXAPvGGkySNJuhp1x2Ad+ZZBdwEfDl4SNJkuYx95+gq6p/SfKHwFPAfwMfqaqPnL5dkgPAAYDV1dV5dye1taw/u7dM/qnB7THklMtlwI3AVcD3ARcnuen07arqcFWtV9X6ysrK/JNKks5pyCmX1wJfrKpTVfUs8F7gx8YZS5I0qyFBfwp4VZKLkgS4Djg+zliSpFnNHfSqehA4AjwEfHbysw6PNJckaUZzPygKUFVvA9420iySpAF8pagkNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITg662KHXxfPwzcOrHI3RJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNTEo6ElelORIkn9McjzJj441mCRpNkOvh/4O4ENV9YYkFwIXjTCTJGkOcwc9yaXAjwO3AFTVM8Az44wlSZrVkFMuLwNOAX+W5OEkdya5eKS5JEkzGnLKZRfwQ8CtVfVgkncAB4Hf3rxRkgPAAYDV1dUBu5OkYZb1pwZPHNqzkP0MOUI/CZysqgcnt4+wEfj/p6oOV9V6Va2vrKwM2J0k6VzmDnpV/SvwpSTXTD51HfDYKFNJkmY29FkutwLvmTzD5Ung54ePJEmax6CgV9UjwPpIs0iSBvCVopLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNDA56kguSPJzkA2MMJEmazxhH6G8Bjo/wcyRJAwwKepIrgD3AneOMI0ma166B3//HwK8Dl5xtgyQHgAMAq6urc+9o7eB9c3/vECcO7VnKfmF5a5a0M819hJ7kZ4Cnq+rYubarqsNVtV5V6ysrK/PuTpK0hSGnXF4N7E1yArgHeE2SvxhlKknSzOYOelX9ZlVdUVVrwD7gE1V102iTSZJm4vPQJamJoQ+KAlBV9wP3j/GzJEnz8Qhdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1MXfQk1yZ5JNJjid5NMlbxhxMkjSbXQO+9zng9qp6KMklwLEkH62qx0aaTZI0g7mP0KvqK1X10OTj/wSOA5ePNZgkaTajnENPsga8EnhwjJ8nSZrd4KAn+S7gr4FfrapvnuHrB5IcTXL01KlTQ3cnSTqLQUFP8h1sxPw9VfXeM21TVYerar2q1ldWVobsTpJ0DkOe5RLgT4HjVfVH440kSZrHkCP0VwNvBF6T5JHJ2w0jzSVJmtHcT1usqk8DGXEWSdIAvlJUkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU3Mffnc54u1g/ctewRJmopH6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqYlDQk1yf5PEkn09ycKyhJEmzmzvoSS4A7gB+GrgW2J/k2rEGkyTNZsgR+o8An6+qJ6vqGeAe4MZxxpIkzWpI0C8HvrTp9snJ5yRJSzDkT9DlDJ+rb9soOQAcmNz8rySPD9jn+WI38NVlD7ENXNfO0XFN0HRd+f3B63rpNBsNCfpJ4MpNt68Avnz6RlV1GDg8YD/nnSRHq2p92XOMzXXtHB3XBK5rqCGnXP4OuDrJVUkuBPYB944zliRpVnMfoVfVc0neDHwYuAB4V1U9OtpkkqSZDDnlQlV9EPjgSLPsJK1OIW3iunaOjmsC1zVIqr7tcUxJ0g7kS/8lqQmDfg5bXdogyS8l+WySR5J8eqe8UnbaSzYkeUOSSnLeP+tgivvqliSnJvfVI0l+YRlzzmqa+yrJzyV5LMmjSf5y0TPOY4r76+2b7qt/SvLvy5hzFlOsaTXJJ5M8nOQfktww+hBV5dsZ3th4oPcLwMuAC4G/B649bZtLN328F/jQsuceY12T7S4BPgV8Blhf9twj3Fe3AH+y7Fm3YV1XAw8Dl01uv3jZc4+xrtO2v5WNJ10sffaB99Vh4JcnH18LnBh7Do/Qz27LSxtU1Tc33byYM7yw6jw07SUbfg/4A+B/FjncnLpehmKadf0icEdVfQOgqp5e8IzzmPX+2g/cvZDJ5jfNmgq4dPLxd3OG1+0MZdDPbqpLGyT5lSRfYCN+ty1otiG2XFeSVwJXVtUHFjnYANNehuJnJ//VPZLkyjN8/XwzzbpeDrw8yd8m+UyS6xc23fymvmxIkpcCVwGfWMBcQ0yzpt8Bbkpyko1nB9469hAG/eymurRBVd1RVd8P/AbwW9s+1XDnXFeSFwBvB25f2ETDTXNf/Q2wVlU/CHwMuGvbpxpumnXtYuO0y0+wcSR7Z5IXbfNcQ031uzWxDzhSVd/axnnGMM2a9gPvrqorgBuAP5/8vo3GoJ/dVJc22OQe4PXbOtE4tlrXJcArgPuTnABeBdx7nj8wuuV9VVVfq6r/ndx8J/DDC5ptiGn+DZ4E3l9Vz1bVF4HH2Qj8+WyW3619nP+nW2C6Nb0J+CuAqnoAeCEb164ZjUE/uy0vbZBk8y/OHuCJBc43r3Ouq6r+o6p2V9VaVa2x8aDo3qo6upxxpzLNffWSTTf3AscXON+8prm8xvuAnwRIspuNUzBPLnTK2U112ZAk1wCXAQ8seL55TLOmp4DrAJL8ABtBPzXmEINeKdpZneXSBkl+FzhaVfcCb07yWuBZ4BvAzcubeDpTrmtHmXJNtyXZCzwHfJ2NZ72c16Zc14eBn0ryGPAt4Neq6mvLm3prM/wb3A/cU5OnhZzPplzT7cA7k7yVjdMxt4y9Nl8pKklNeMpFkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1IT/welAbXoZ83mWQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# split videos in parts of 22 frames \n",
    "x_test_parts, test_idx_parts = split_into_parts(x_data=x_test, n_frames=n_frames)\n",
    "\n",
    "# extract features for each part\n",
    "x_test_parts_features = extract_features(x_test_parts)\n",
    "\n",
    "# unroll [video, part_id, features] to [part_id, features]\n",
    "x_test_parts_features_unrolled = np.concatenate(x_test_parts_features)\n",
    "test_idx_parts_unrolled = np.concatenate(test_idx_parts)\n",
    "\n",
    "# scale data\n",
    "x_test_scaled = scaler.transform(x_test_parts_features_unrolled)\n",
    "\n",
    "# pick importanted features\n",
    "# use only important features only RF\n",
    "X_important_test = sfm.transform(x_test_scaled)\n",
    "    \n",
    "    \n",
    "# predict \n",
    "y_test_pred = rf.predict(X_important_test)\n",
    "\n",
    "# combine preidctions of multiple parts per video into single prediction by averaging\n",
    "df = combine_parts_pred(y_pred=y_test_pred, v_idx=test_idx_parts_unrolled)\n",
    "\n",
    "display(df)\n",
    "plt.hist(df['y'], bins=25)\n",
    "plt.show()\n",
    "df.to_csv(\"submissions/rf_combined.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
