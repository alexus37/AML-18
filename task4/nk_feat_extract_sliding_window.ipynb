{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import natsort\n",
    "import random as rn\n",
    "import skvideo.io\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import (SVC, SVR)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import (AdaBoostRegressor, RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, BaggingRegressor)\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videos_from_folder(data_folder):\n",
    "    '''\n",
    "    get a list of video x wehre each video is a numpy array in the format [n_frames,width,height] \n",
    "    with uint8 elements.\n",
    "    argument: relative path to the data_folder from the source folder.\n",
    "    '''\n",
    "    data_folder = os.path.join(dir_path,data_folder)\n",
    "    x = []\n",
    "    file_names = []\n",
    "    \n",
    "    if os.path.isdir(data_folder):\n",
    "        for dirpath, dirnames, filenames in os.walk(data_folder):\n",
    "            filenames = natsort.natsorted(filenames,reverse=False)\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                statinfo = os.stat(file_path)\n",
    "                if statinfo.st_size != 0:\n",
    "                    video = skvideo.io.vread(file_path, outputdict={\"-pix_fmt\": \"gray\"})[:, :, :, 0]\n",
    "                    x.append(video)\n",
    "                    file_names.append(int(filename.split(\".\")[0]))\n",
    "\n",
    "    indices = sorted(range(len(file_names)), key=file_names.__getitem__)\n",
    "    x = np.take(x,indices)\n",
    "    return x\n",
    "\n",
    "def get_target_from_csv(csv_file):\n",
    "    '''\n",
    "    get a numpy array y of labels. the order follows the id of video. \n",
    "    argument: relative path to the csv_file from the source folder.\n",
    "    '''\n",
    "    csv_file = os.path.join(dir_path,csv_file)\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        label_reader = pd.read_csv(csvfile)\n",
    "        #print(\"Labels: \", label_reader['id'])\n",
    "        y = label_reader['y']\n",
    "        \n",
    "    y = np.array(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(videos):\n",
    "\n",
    "    #Extracting features\n",
    "    height_im = videos[0][0].shape[0]\n",
    "    \n",
    "    X_video_features = []\n",
    "    \n",
    "    for video in tqdm(videos):\n",
    "        all_features = []\n",
    "        n_parts = len(video)\n",
    "        for part in video:\n",
    "            feature_vec = []\n",
    "            part_grad = np.diff(part,axis = 0) #frame-wise gradient\n",
    "            height_im_grad = part_grad.shape[1]\n",
    "            width_im_grad = part_grad.shape[2]\n",
    "\n",
    "            # compute the sum of pixels per frame\n",
    "            heartBeatApprox = np.sum(np.sum(part, axis=1), axis=1)\n",
    "            feature_vec.append(np.min(heartBeatApprox))\n",
    "            feature_vec.append(np.max(heartBeatApprox))\n",
    "            feature_vec.append(np.mean(heartBeatApprox))\n",
    "            feature_vec.append(np.std(heartBeatApprox))\n",
    "            feature_vec += heartBeatApprox.tolist()\n",
    "\n",
    "\n",
    "            for idx in range(height_im):\n",
    "                feature_vec.append(np.mean(part[:,:,idx])) #entire video video mean_column_pixels            1\n",
    "                feature_vec.append(np.mean(part[:,idx,:])) #entire video mean_row_pixels             2\n",
    "                feature_vec.append(np.std(part[:,:,idx])) #entire video std_column_pixels            3\n",
    "                feature_vec.append(np.std(part[:,idx,:])) #entire video std_row_pixels               4\n",
    "                feature_vec.append(np.count_nonzero(part[:,:,idx])) #entire nonzero_column_pixels    5\n",
    "                feature_vec.append(np.count_nonzero(part[:,idx,:])) #entire nonzero_row_pixels       6\n",
    "\n",
    "            # ? TODO ? Taking some of the same previous features but just frame-wise?\n",
    "            feature_vec.append(np.mean(part_grad)) #mean_video_grad    1\n",
    "            feature_vec.append(np.std(part_grad)) #std_video_grad      2\n",
    "            feature_vec.append(np.mean(part_grad)) #mean_frame_grad    3\n",
    "\n",
    "\n",
    "            for frame_grad in part_grad:\n",
    "                feature_vec.append(np.mean(frame_grad))\n",
    "                feature_vec.append(np.std(frame_grad)) #Std of the gradient of the single frame std_frame_grad      1\n",
    "\n",
    "                for idx in range(height_im_grad):            \n",
    "                    feature_vec.append(np.mean(frame_grad[:,idx])) #mean_grad_column_pixels   1\n",
    "                    feature_vec.append(np.mean(frame_grad[idx,:])) #mean_grad_row_pixels           2\n",
    "                    feature_vec.append(np.std(frame_grad[:,idx])) #std_grad_column_pixels          3\n",
    "                    feature_vec.append(np.std(frame_grad[idx,:])) #std_grad_row_pixels               4\n",
    "                    feature_vec.append(np.count_nonzero(frame_grad[:,idx])) #nonzero_grad_column_pixels  5\n",
    "                    feature_vec.append(np.count_nonzero(frame_grad[idx,:])) #nonzero_grad_row_pixels     6\n",
    "\n",
    "            total_features = len(feature_vec)\n",
    "            all_features.append(feature_vec)\n",
    "\n",
    "        X_features = np.zeros((n_parts,total_features))\n",
    "\n",
    "        for i in range(n_parts):\n",
    "            X_features[i,:] = all_features[i]\n",
    "        \n",
    "        X_video_features.append(X_features)\n",
    "    return np.concatenate([X_video_features])\n",
    "\n",
    "def make_submission(filename, predictions):\n",
    "    ids = extract_ids(test_folder)\n",
    "    df = pd.DataFrame({'id':ids, 'y':predictions})\n",
    "    df[[\"id\", \"y\"]].to_csv(\"submissions/\"+filename, index= False)\n",
    "def split_into_parts(x_data, n_frames, y_data=None):\n",
    "\n",
    "    height_im = x_data[0].shape[1]\n",
    "    width_im = x_data[0].shape[2]\n",
    "    \n",
    "    videos = []\n",
    "    video_ids = []\n",
    "    video_labels = []\n",
    "    \n",
    "    n_videos = x_data.shape[0]\n",
    "\n",
    "    for v_id in range(n_videos):\n",
    "        video = x_data[v_id]\n",
    "        if y_data is not None:\n",
    "            label = y_data[v_id]\n",
    "            \n",
    "        n_subsamples = int(video.shape[0]/n_frames)\n",
    "        parts = []\n",
    "        parts_v_ids = []\n",
    "        parts_labels = []\n",
    "        \n",
    "        for i in range(n_subsamples):\n",
    "            from_frame = i*n_frames\n",
    "            to_frame = from_frame + n_frames\n",
    "            parts.append(video[from_frame:to_frame,:,:])\n",
    "            parts_v_ids.append(v_id)\n",
    "            \n",
    "            if y_data is not None:\n",
    "                parts_labels.append(label)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        videos.append(np.concatenate([parts]))\n",
    "        video_ids.append(np.concatenate([parts_v_ids]))\n",
    "        \n",
    "        if y_data is not None:\n",
    "            video_labels.append(np.concatenate([parts_labels]))\n",
    "            \n",
    "    X = np.concatenate([videos])\n",
    "    v_idx = np.concatenate([video_ids])\n",
    "    \n",
    "    if y_data is not None:\n",
    "        y = np.concatenate([video_labels])\n",
    "        \n",
    "        return X, v_idx, y\n",
    "        \n",
    "    else:\n",
    "        return X, v_idx\n",
    "    \n",
    "def split_into_parts_with_sliding_window(x_data, n_frames, y_data=None):\n",
    "\n",
    "    height_im = x_data[0].shape[1]\n",
    "    width_im = x_data[0].shape[2]\n",
    "    \n",
    "    videos = []\n",
    "    video_ids = []\n",
    "    video_labels = []\n",
    "    \n",
    "    n_videos = x_data.shape[0]\n",
    "\n",
    "    for v_id in range(n_videos):\n",
    "        video = x_data[v_id]\n",
    "        if y_data is not None:\n",
    "            label = y_data[v_id]\n",
    "            \n",
    "        n_subsamples = int(video.shape[0] - n_frames)\n",
    "        parts = []\n",
    "        parts_v_ids = []\n",
    "        parts_labels = []\n",
    "        \n",
    "        for i in range(n_subsamples):\n",
    "            from_frame = i\n",
    "            to_frame = from_frame + n_frames\n",
    "            parts.append(video[from_frame:to_frame,:,:])\n",
    "            parts_v_ids.append(v_id)\n",
    "            \n",
    "            if y_data is not None:\n",
    "                parts_labels.append(label)\n",
    "        \n",
    "        videos.append(np.concatenate([parts]))\n",
    "        video_ids.append(np.concatenate([parts_v_ids]))\n",
    "        \n",
    "        if y_data is not None:\n",
    "            video_labels.append(np.concatenate([parts_labels]))\n",
    "            \n",
    "    X = np.concatenate([videos])\n",
    "    v_idx = np.concatenate([video_ids])\n",
    "    \n",
    "    if y_data is not None:\n",
    "        y = np.concatenate([video_labels])\n",
    "        \n",
    "        return X, v_idx, y\n",
    "        \n",
    "    else:\n",
    "        return X, v_idx\n",
    "    \n",
    "\n",
    "\n",
    "def combine_parts_pred(y_pred, v_idx):\n",
    "    d = {}\n",
    "    for v_id, pred in zip(v_idx, y_pred):\n",
    "        if v_id not in d:\n",
    "            d[v_id] = []\n",
    "        \n",
    "        d[v_id].append(pred)\n",
    "        \n",
    "    \n",
    "    results = []\n",
    "    for v_id, preds in d.items():\n",
    "        results.append({\"id\":v_id,\"y\":sum(preds) / float(len(preds))})\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dir ->  /home/ax/master/2018_02/ml/task4\n",
      "Train folder ->  /home/ax/master/2018_02/ml/task4/data/train/\n",
      "Train target ->  /home/ax/master/2018_02/ml/task4/data/train_target.csv\n",
      "Test folder ->  /home/ax/master/2018_02/ml/task4/data/test/\n",
      "Train Data\n",
      "\n",
      "Test Data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed=42\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "train_folder = os.path.join(dir_path,\"data/train/\")\n",
    "test_folder = os.path.join(dir_path,\"data/test/\")\n",
    "\n",
    "train_target = os.path.join(dir_path,'data/train_target.csv')\n",
    "\n",
    "print(\"Current dir -> \", dir_path)\n",
    "print(\"Train folder -> \",train_folder)\n",
    "print(\"Train target -> \",train_target)\n",
    "print(\"Test folder -> \",test_folder)\n",
    "\n",
    "#Load data from csv file\n",
    "print(\"Train Data\\n\")\n",
    "x_train = get_videos_from_folder(train_folder) #List of numpy arrays\n",
    "y_train = get_target_from_csv(train_target) #Numpy array of labels\n",
    "print(\"Test Data\\n\")\n",
    "x_test = get_videos_from_folder(test_folder) #List of numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=500,\n",
    "                                       random_state=seed,\n",
    "                                       n_jobs=-1,\n",
    "                                       verbose=1)\n",
    "\n",
    "gb = GradientBoostingRegressor(random_state=seed,\n",
    "                                       n_estimators=100,\n",
    "                                       max_depth=5,\n",
    "                                       verbose=1,\n",
    "                                       learning_rate=0.1)\n",
    "\n",
    "classifiers = [rf, gb]\n",
    "classifiers_names = [\"RandomForestRegressor\", \"GradientBoostingRegressor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (158,)\n",
      "y_train: (158,)\n",
      "x_test: (69,)\n",
      "Splitting videos into parts...\n",
      "Extracting features from parts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec05cb1e87ab48e9834f240af490be92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=158), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain_parts_features: (158,)\n",
      "xtrain_parts_features[0]: (57, 11585)\n",
      "Unrolled: (7150, 11585)\n",
      "Fitting Standard Scalar...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_frames = 20\n",
    "\n",
    "\n",
    "print(f\"x_train: {x_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"x_test: {x_test.shape}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print(\"Splitting videos into parts...\")\n",
    "x_train_parts, train_idx_parts, y_train_parts = split_into_parts_with_sliding_window(x_data=x_train, n_frames=n_frames, y_data=y_train)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Extracting features from parts...\")\n",
    "x_train_parts_features = extract_features(x_train_parts)\n",
    "\n",
    "print(f\"xtrain_parts_features: {x_train_parts_features.shape}\")\n",
    "print(f\"xtrain_parts_features[0]: {x_train_parts_features[0].shape}\")\n",
    "\n",
    "x_train_parts_features_unrolled = np.concatenate(x_train_parts_features)\n",
    "print(f\"Unrolled: {x_train_parts_features_unrolled.shape}\")\n",
    "\n",
    "print(\"Fitting Standard Scalar...\")\n",
    "scaler.fit(x_train_parts_features_unrolled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   37.7s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2000 out of 2000 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: 1984 features\n"
     ]
    }
   ],
   "source": [
    "# Create a feature selctor base on a random forest\n",
    "sfm = SelectFromModel(RandomForestClassifier(n_estimators=2000, random_state=seed, n_jobs=-1, verbose=1), threshold=0.0001)\n",
    "\n",
    "# Train the classifier\n",
    "\n",
    "sfm.fit(x_train_parts_features_unrolled, np.concatenate(y_train_parts))\n",
    "print(f\"Using: {np.sum(sfm.get_support())} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 3010\n",
      "Class 1: 3824\n"
     ]
    }
   ],
   "source": [
    "print(f\"Class 0: {np.sum(np.concatenate(y_train_parts) == 0)}\")\n",
    "print(f\"Class 1: {np.sum(np.concatenate(y_train_parts) == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847f41d8bba646679ff62a2e9ce36a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  5.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2138            1.90m\n",
      "         2           0.1863            1.90m\n",
      "         3           0.1649            1.94m\n",
      "         4           0.1470            1.91m\n",
      "         5           0.1341            1.87m\n",
      "         6           0.1214            1.88m\n",
      "         7           0.1114            1.87m\n",
      "         8           0.1024            1.86m\n",
      "         9           0.0900            1.87m\n",
      "        10           0.0799            1.87m\n",
      "        20           0.0305            1.73m\n",
      "        30           0.0169            1.48m\n",
      "        40           0.0099            1.27m\n",
      "        50           0.0055            1.07m\n",
      "        60           0.0036           51.35s\n",
      "        70           0.0026           37.94s\n",
      "        80           0.0020           25.07s\n",
      "        90           0.0015           12.45s\n",
      "       100           0.0012            0.00s\n",
      "roc auc parts: 0.5043175863299646\n",
      "roc auc video: 0.484375\n",
      "roc auc part GBs: 0.7037665717842251\n",
      "roc auc video GB: 0.59375\n",
      "roc auc part AVG: 0.6407698534980283\n",
      "roc auc video AVG: 0.5625\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   30.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  7.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2177            1.65m\n",
      "         2           0.1947            1.64m\n",
      "         3           0.1721            1.76m\n",
      "         4           0.1532            1.78m\n",
      "         5           0.1351            1.81m\n",
      "         6           0.1218            1.84m\n",
      "         7           0.1057            1.88m\n",
      "         8           0.0941            1.87m\n",
      "         9           0.0847            1.88m\n",
      "        10           0.0734            1.88m\n",
      "        20           0.0273            1.73m\n",
      "        30           0.0128            1.51m\n",
      "        40           0.0060            1.32m\n",
      "        50           0.0039            1.08m\n",
      "        60           0.0025           52.00s\n",
      "        70           0.0018           38.60s\n",
      "        80           0.0014           25.44s\n",
      "        90           0.0011           12.40s\n",
      "       100           0.0008            0.00s\n",
      "roc auc parts: 0.37483012728490767\n",
      "roc auc video: 0.4375\n",
      "roc auc part GBs: 0.2915239097840303\n",
      "roc auc video GB: 0.40625\n",
      "roc auc part AVG: 0.35338628895907104\n",
      "roc auc video AVG: 0.453125\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   22.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  5.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2219            1.57m\n",
      "         2           0.1916            1.85m\n",
      "         3           0.1619            2.00m\n",
      "         4           0.1438            2.02m\n",
      "         5           0.1232            2.05m\n",
      "         6           0.1061            2.06m\n",
      "         7           0.0955            2.05m\n",
      "         8           0.0828            2.05m\n",
      "         9           0.0752            2.02m\n",
      "        10           0.0661            2.01m\n",
      "        20           0.0228            1.80m\n",
      "        30           0.0111            1.53m\n",
      "        40           0.0054            1.32m\n",
      "        50           0.0035            1.08m\n",
      "        60           0.0025           51.49s\n",
      "        70           0.0018           39.10s\n",
      "        80           0.0013           25.79s\n",
      "        90           0.0010           12.93s\n",
      "       100           0.0008            0.00s\n",
      "roc auc parts: 0.37835224005436774\n",
      "roc auc video: 0.625\n",
      "roc auc part GBs: 0.47414919755345286\n",
      "roc auc video GB: 0.71875\n",
      "roc auc part AVG: 0.4066992524439333\n",
      "roc auc video AVG: 0.6875\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   33.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  6.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2182            2.28m\n",
      "         2           0.1955            2.02m\n",
      "         3           0.1750            1.95m\n",
      "         4           0.1558            2.05m\n",
      "         5           0.1395            2.15m\n",
      "         6           0.1222            2.23m\n",
      "         7           0.1118            2.16m\n",
      "         8           0.0960            2.17m\n",
      "         9           0.0828            2.17m\n",
      "        10           0.0722            2.16m\n",
      "        20           0.0325            1.88m\n",
      "        30           0.0151            1.69m\n",
      "        40           0.0085            1.45m\n",
      "        50           0.0053            1.19m\n",
      "        60           0.0038           55.51s\n",
      "        70           0.0028           40.55s\n",
      "        80           0.0021           26.68s\n",
      "        90           0.0016           13.21s\n",
      "       100           0.0013            0.00s\n",
      "roc auc parts: 0.8796931889334663\n",
      "roc auc video: 0.828125\n",
      "roc auc part GBs: 0.801593191252522\n",
      "roc auc video GB: 0.75\n",
      "roc auc part AVG: 0.8564156675401775\n",
      "roc auc video AVG: 0.796875\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   21.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2114            1.95m\n",
      "         2           0.1814            1.92m\n",
      "         3           0.1584            1.90m\n",
      "         4           0.1362            1.89m\n",
      "         5           0.1211            1.87m\n",
      "         6           0.1053            1.87m\n",
      "         7           0.0924            1.88m\n",
      "         8           0.0841            1.84m\n",
      "         9           0.0763            1.81m\n",
      "        10           0.0663            1.82m\n",
      "        20           0.0216            1.78m\n",
      "        30           0.0108            1.59m\n",
      "        40           0.0061            1.35m\n",
      "        50           0.0038            1.12m\n",
      "        60           0.0025           53.56s\n",
      "        70           0.0018           39.58s\n",
      "        80           0.0014           25.98s\n",
      "        90           0.0011           12.85s\n",
      "       100           0.0009            0.00s\n",
      "roc auc parts: 0.657409381663113\n",
      "roc auc video: 0.609375\n",
      "roc auc part GBs: 0.7060573754603605\n",
      "roc auc video GB: 0.703125\n",
      "roc auc part AVG: 0.6600988563675131\n",
      "roc auc video AVG: 0.671875\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   31.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  7.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2142            1.81m\n",
      "         2           0.1916            1.72m\n",
      "         3           0.1676            1.79m\n",
      "         4           0.1436            1.88m\n",
      "         5           0.1252            1.91m\n",
      "         6           0.1080            1.94m\n",
      "         7           0.0964            1.94m\n",
      "         8           0.0837            1.94m\n",
      "         9           0.0741            1.94m\n",
      "        10           0.0647            1.94m\n",
      "        20           0.0214            1.80m\n",
      "        30           0.0099            1.59m\n",
      "        40           0.0052            1.37m\n",
      "        50           0.0034            1.12m\n",
      "        60           0.0022           53.75s\n",
      "        70           0.0016           39.58s\n",
      "        80           0.0012           26.11s\n",
      "        90           0.0010           12.84s\n",
      "       100           0.0007            0.00s\n",
      "roc auc parts: 0.6552474757157465\n",
      "roc auc video: 0.546875\n",
      "roc auc part GBs: 0.610804895194274\n",
      "roc auc video GB: 0.484375\n",
      "roc auc part AVG: 0.6561062116564417\n",
      "roc auc video AVG: 0.546875\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   21.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2147            1.61m\n",
      "         2           0.1831            1.69m\n",
      "         3           0.1586            1.71m\n",
      "         4           0.1397            1.71m\n",
      "         5           0.1218            1.73m\n",
      "         6           0.1074            1.71m\n",
      "         7           0.0945            1.70m\n",
      "         8           0.0832            1.68m\n",
      "         9           0.0744            1.66m\n",
      "        10           0.0657            1.66m\n",
      "        20           0.0260            1.51m\n",
      "        30           0.0133            1.33m\n",
      "        40           0.0080            1.11m\n",
      "        50           0.0052           53.98s\n",
      "        60           0.0035           42.44s\n",
      "        70           0.0023           31.74s\n",
      "        80           0.0018           20.63s\n",
      "        90           0.0014           10.17s\n",
      "       100           0.0011            0.00s\n",
      "roc auc parts: 0.697046664027796\n",
      "roc auc video: 0.703125\n",
      "roc auc part GBs: 0.648628886836434\n",
      "roc auc video GB: 0.765625\n",
      "roc auc part AVG: 0.6941329111140431\n",
      "roc auc video AVG: 0.734375\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2153            1.68m\n",
      "         2           0.1843            1.79m\n",
      "         3           0.1599            1.81m\n",
      "         4           0.1386            1.83m\n",
      "         5           0.1209            1.88m\n",
      "         6           0.1070            1.90m\n",
      "         7           0.0942            1.91m\n",
      "         8           0.0847            1.91m\n",
      "         9           0.0749            1.91m\n",
      "        10           0.0662            1.90m\n",
      "        20           0.0214            1.66m\n",
      "        30           0.0100            1.43m\n",
      "        40           0.0055            1.19m\n",
      "        50           0.0034           58.28s\n",
      "        60           0.0024           45.43s\n",
      "        70           0.0018           33.21s\n",
      "        80           0.0014           21.74s\n",
      "        90           0.0010           10.81s\n",
      "       100           0.0008            0.00s\n",
      "roc auc parts: 0.37629430218977333\n",
      "roc auc video: 0.46875\n",
      "roc auc part GBs: 0.45973718529536534\n",
      "roc auc video GB: 0.546875\n",
      "roc auc part AVG: 0.4205192397333096\n",
      "roc auc video AVG: 0.484375\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  5.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2152            1.99m\n",
      "         2           0.1856            2.13m\n",
      "         3           0.1617            2.15m\n",
      "         4           0.1403            2.17m\n",
      "         5           0.1230            2.24m\n",
      "         6           0.1082            2.24m\n",
      "         7           0.0952            2.26m\n",
      "         8           0.0867            2.24m\n",
      "         9           0.0759            2.22m\n",
      "        10           0.0668            2.23m\n",
      "        20           0.0236            2.07m\n",
      "        30           0.0101            1.83m\n",
      "        40           0.0058            1.53m\n",
      "        50           0.0038            1.24m\n",
      "        60           0.0028           57.58s\n",
      "        70           0.0020           42.09s\n",
      "        80           0.0016           27.63s\n",
      "        90           0.0012           13.67s\n",
      "       100           0.0010            0.00s\n",
      "roc auc parts: 0.5209544670118147\n",
      "roc auc video: 0.59375\n",
      "roc auc part GBs: 0.6497809637594585\n",
      "roc auc video GB: 0.65625\n",
      "roc auc part AVG: 0.5711535908668526\n",
      "roc auc video AVG: 0.609375\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  7.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.2201            1.67m\n",
      "         2           0.1973            1.72m\n",
      "         3           0.1728            1.85m\n",
      "         4           0.1572            1.84m\n",
      "         5           0.1418            1.84m\n",
      "         6           0.1214            1.92m\n",
      "         7           0.1045            1.97m\n",
      "         8           0.0968            1.93m\n",
      "         9           0.0873            1.93m\n",
      "        10           0.0763            1.95m\n",
      "        20           0.0277            1.83m\n",
      "        30           0.0120            1.60m\n",
      "        40           0.0071            1.33m\n",
      "        50           0.0041            1.11m\n",
      "        60           0.0031           52.28s\n",
      "        70           0.0022           38.75s\n",
      "        80           0.0017           25.52s\n",
      "        90           0.0014           12.62s\n",
      "       100           0.0011            0.00s\n",
      "roc auc parts: 0.5891364421416234\n",
      "roc auc video: 0.5102040816326531\n",
      "roc auc part GBs: 0.5770984455958549\n",
      "roc auc video GB: 0.5306122448979592\n",
      "roc auc part AVG: 0.5893379389752447\n",
      "roc auc video AVG: 0.5510204081632653\n",
      "========================================\n",
      "Random forest roc_auc parts avg score 0.5633281875352574 +/- 0.15682506657953832 roc_auc video avg score 0.5807079081632653 +/- 0.11295725386448856\n",
      "Gradient boosting roc_auc parts avg score 0.5923140622515978 +/- 0.14079008009101263 roc_auc video avg score 0.6155612244897959 +/- 0.11575608233776856\n",
      "Combined roc_auc parts avg score 0.5848619811154615 +/- 0.14563069850994967 roc_auc video avg score 0.6097895408163265 +/- 0.10504148404204308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "clf_scores_parts_avg = []\n",
    "clf_scores_parts_std = []\n",
    "\n",
    "clf_scores_video_avg = []\n",
    "clf_scores_video_std = []\n",
    "\n",
    "verbose = False\n",
    "\n",
    "\n",
    "roc_auc_parts_scores = []\n",
    "roc_auc_video_scores = []\n",
    "roc_auc_parts_scores_gb = []\n",
    "roc_auc_video_scores_gb = []\n",
    "roc_auc_parts_scores_avg = []\n",
    "roc_auc_video_scores_avg = []\n",
    "\n",
    "\n",
    "print(\"Start\")\n",
    "for train, valid in tqdm(kfold.split(x_train_parts_features , y_train)):\n",
    "    #for clf in tqdm(classifiers, desc=\"Classifier: \"):\n",
    "    # split training set into parts\n",
    "    x_train_fold = x_train_parts_features[train]\n",
    "    y_train_fold = y_train_parts[train] \n",
    "\n",
    "    # split validation set into parts\n",
    "    x_valid_fold = x_train_parts_features[valid]\n",
    "    y_valid_fold = y_train_parts[valid]\n",
    "    y_valid_fold_videos = y_train[valid]\n",
    "    idx_valid_fold = train_idx_parts[valid]\n",
    "\n",
    "    # unrolling from [video id, part idx, feature] to [part idx, feature]\n",
    "    x_train_fold_unrolled = np.concatenate(x_train_fold)\n",
    "    y_train_fold_unrolled = np.concatenate(y_train_fold)\n",
    "\n",
    "    x_valid_fold_unrolled = np.concatenate(x_valid_fold)\n",
    "    y_valid_fold_unrolled = np.concatenate(y_valid_fold)\n",
    "    idx_valid_fold_unrolled = np.concatenate(idx_valid_fold)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"x_train_fold: {x_train_fold.shape}\")\n",
    "        print(f\"y_train_fold: {y_train_fold.shape}\")\n",
    "        print(f\"x_train_fold_unrolled: {x_train_fold_unrolled.shape}\")\n",
    "        print(f\"y_train_fold_unrolled: {y_train_fold_unrolled.shape}\")\n",
    "\n",
    "        print(f\"x_valid_fold: {x_valid_fold.shape}\")\n",
    "        print(f\"y_valid_fold: {y_valid_fold.shape}\")\n",
    "        print(f\"idx_valid_fold: {idx_valid_fold.shape}\")\n",
    "        print(f\"x_valid_fold_unrolled: {x_valid_fold_unrolled.shape}\")\n",
    "        print(f\"y_valid_fold_unrolled: {y_valid_fold_unrolled.shape}\")\n",
    "        print(f\"idx_valid_fold_unrolled: {idx_valid_fold_unrolled.shape}\")\n",
    "\n",
    "    # scale the extracted features to zero mean and unit variance \n",
    "    x_train_fold_scaled = scaler.transform(x_train_fold_unrolled)\n",
    "    x_valid_fold_scaled = scaler.transform(x_valid_fold_unrolled)\n",
    "\n",
    "    # Shuffle the training data\n",
    "    indices = np.arange(x_train_fold_scaled.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    x_train_fold_scaled = x_train_fold_scaled[indices]\n",
    "    y_train_fold_unrolled = y_train_fold_unrolled[indices]\n",
    "    \n",
    "    # use only important features only RF\n",
    "    X_important_train = sfm.transform(x_train_fold_scaled)\n",
    "    X_important_valid = sfm.transform(x_valid_fold_scaled)\n",
    "\n",
    "    # fit classifier\n",
    "    rf.fit(X_important_train, y_train_fold_unrolled)\n",
    "    #gb.fit(x_train_fold_scaled_gb, y_train_fold_unrolled_gb)\n",
    "    gb.fit(X_important_train, y_train_fold_unrolled)\n",
    "\n",
    "    y_valid_fold_pred = rf.predict(X_important_valid)\n",
    "    #y_valid_fold_pred = rf.predict(x_valid_fold_scaled_gb)\n",
    "    #y_valid_fold_pred_gb = gb.predict(x_valid_fold_scaled_gb)\n",
    "    y_valid_fold_pred_gb = gb.predict(X_important_valid)\n",
    "\n",
    "    # calculate the roc auc based on per part predictions\n",
    "    roc_auc_parts = roc_auc_score(y_true=y_valid_fold_unrolled, y_score=y_valid_fold_pred)\n",
    "    roc_auc_parts_scores.append(roc_auc_parts)\n",
    "    print(f\"roc auc parts: {roc_auc_parts}\")\n",
    "\n",
    "    # calculate the roc auc based on per video predictions\n",
    "    df = combine_parts_pred(y_pred=y_valid_fold_pred, v_idx=idx_valid_fold_unrolled)\n",
    "    roc_auc_video = roc_auc_score(y_true=y_valid_fold_videos, y_score=df['y'].values)\n",
    "    roc_auc_video_scores.append(roc_auc_video)\n",
    "    print(f\"roc auc video: {roc_auc_video}\")\n",
    "        \n",
    "    # calculate the roc auc based on per part predictions\n",
    "    roc_auc_parts_gb = roc_auc_score(y_true=y_valid_fold_unrolled, y_score=y_valid_fold_pred_gb)\n",
    "    roc_auc_parts_scores_gb.append(roc_auc_parts_gb)\n",
    "    print(f\"roc auc part GBs: {roc_auc_parts_gb}\")\n",
    "\n",
    "    # calculate the roc auc based on per video predictions\n",
    "    df = combine_parts_pred(y_pred=y_valid_fold_pred_gb, v_idx=idx_valid_fold_unrolled)\n",
    "    roc_auc_video_gb = roc_auc_score(y_true=y_valid_fold_videos, y_score=df['y'].values)\n",
    "    roc_auc_video_scores_gb.append(roc_auc_video_gb)\n",
    "    print(f\"roc auc video GB: {roc_auc_video_gb}\")\n",
    "    \n",
    "    # Combined approach\n",
    "    y_combine_pred = (y_valid_fold_pred + y_valid_fold_pred_gb) / 2.0\n",
    "    # calculate the roc auc based on per part predictions\n",
    "    roc_auc_parts_avg = roc_auc_score(y_true=y_valid_fold_unrolled, y_score=y_combine_pred)\n",
    "    roc_auc_parts_scores_avg.append(roc_auc_parts_avg)\n",
    "    print(f\"roc auc part AVG: {roc_auc_parts_avg}\")\n",
    "\n",
    "    # calculate the roc auc based on per video predictions\n",
    "    df = combine_parts_pred(y_pred=y_combine_pred, v_idx=idx_valid_fold_unrolled)\n",
    "    roc_auc_video_avg = roc_auc_score(y_true=y_valid_fold_videos, y_score=df['y'].values)\n",
    "    roc_auc_video_scores_avg.append(roc_auc_video_avg)\n",
    "    print(f\"roc auc video AVG: {roc_auc_video_avg}\")\n",
    "    \n",
    "        \n",
    "    print(\"========================================\")\n",
    "\n",
    "print(f\"Random forest roc_auc parts avg score {np.mean(roc_auc_parts_scores)} +/- {np.std(roc_auc_parts_scores)} roc_auc video avg score {np.mean(roc_auc_video_scores)} +/- {np.std(roc_auc_video_scores)}\")\n",
    "print(f\"Gradient boosting roc_auc parts avg score {np.mean(roc_auc_parts_scores_gb)} +/- {np.std(roc_auc_parts_scores_gb)} roc_auc video avg score {np.mean(roc_auc_video_scores_gb)} +/- {np.std(roc_auc_video_scores_gb)}\")\n",
    "print(f\"Combined roc_auc parts avg score {np.mean(roc_auc_parts_scores_avg)} +/- {np.std(roc_auc_parts_scores_avg)} roc_auc video avg score {np.mean(roc_auc_video_scores_avg)} +/- {np.std(roc_auc_video_scores_avg)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=-1,\n",
       "           oob_score=False, random_state=42, verbose=False,\n",
       "           warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unrolling from [video id, part idx, feature] to [part idx, feature]\n",
    "x_train_unrolled = np.concatenate(x_train_parts_features)    \n",
    "y_train_unrolled = np.concatenate(y_train_parts)\n",
    "\n",
    "\n",
    "# scale the extracted features to zero mean and unit variance \n",
    "x_train_scaled = scaler.transform(x_train_unrolled)\n",
    "\n",
    "# use only important features only RF\n",
    "X_important_train = sfm.transform(x_train_scaled)\n",
    "    \n",
    "# fit classifier\n",
    "rf.fit(X_important_train, y_train_unrolled)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2d474772e84cf6aab32aa5741e0d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=69), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.424500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.284500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.549667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.682250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.536750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.510250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.695000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.721833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.419750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.402500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.439750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.655625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.461500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.463833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.617875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.474000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.801375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.703250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.484063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.500500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.775167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.601000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.465000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.665667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.628333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.616250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.742375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>0.736000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>0.473375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>0.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>0.414500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>0.572400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0.722750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>0.658250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>0.622125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>0.589167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>0.763167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>0.595500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>0.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>0.234250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>0.322833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>0.677000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>0.684000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>0.703250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>0.757833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>60</td>\n",
       "      <td>0.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>61</td>\n",
       "      <td>0.429250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>0.661333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>0.702250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>0.667500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>0.561500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>0.511333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>0.619750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>68</td>\n",
       "      <td>0.563667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id         y\n",
       "0    0  0.424500\n",
       "1    1  0.284500\n",
       "2    2  0.549667\n",
       "3    3  0.682250\n",
       "4    4  0.536750\n",
       "5    5  0.510250\n",
       "6    6  0.695000\n",
       "7    7  0.721833\n",
       "8    8  0.435000\n",
       "9    9  0.419750\n",
       "10  10  0.402500\n",
       "11  11  0.439750\n",
       "12  12  0.655625\n",
       "13  13  0.461500\n",
       "14  14  0.463833\n",
       "15  15  0.617875\n",
       "16  16  0.474000\n",
       "17  17  0.801375\n",
       "18  18  0.703250\n",
       "19  19  0.484063\n",
       "20  20  0.500500\n",
       "21  21  0.775167\n",
       "22  22  0.601000\n",
       "23  23  0.465000\n",
       "24  24  0.665667\n",
       "25  25  0.628333\n",
       "26  26  0.616250\n",
       "27  27  0.742375\n",
       "28  28  0.680000\n",
       "29  29  0.556000\n",
       "..  ..       ...\n",
       "39  39  0.400000\n",
       "40  40  0.736000\n",
       "41  41  0.473375\n",
       "42  42  0.275000\n",
       "43  43  0.565000\n",
       "44  44  0.414500\n",
       "45  45  0.572400\n",
       "46  46  0.722750\n",
       "47  47  0.658250\n",
       "48  48  0.622125\n",
       "49  49  0.589167\n",
       "50  50  0.763167\n",
       "51  51  0.595500\n",
       "52  52  0.370000\n",
       "53  53  0.234250\n",
       "54  54  0.430000\n",
       "55  55  0.322833\n",
       "56  56  0.677000\n",
       "57  57  0.684000\n",
       "58  58  0.703250\n",
       "59  59  0.757833\n",
       "60  60  0.632000\n",
       "61  61  0.429250\n",
       "62  62  0.661333\n",
       "63  63  0.702250\n",
       "64  64  0.667500\n",
       "65  65  0.561500\n",
       "66  66  0.511333\n",
       "67  67  0.619750\n",
       "68  68  0.563667\n",
       "\n",
       "[69 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# split videos in parts of 22 frames \n",
    "x_test_parts, test_idx_parts = split_into_parts(x_data=x_test, n_frames=n_frames)\n",
    "\n",
    "# extract features for each part\n",
    "x_test_parts_features = extract_features(x_test_parts)\n",
    "\n",
    "# unroll [video, part_id, features] to [part_id, features]\n",
    "x_test_parts_features_unrolled = np.concatenate(x_test_parts_features)\n",
    "test_idx_parts_unrolled = np.concatenate(test_idx_parts)\n",
    "\n",
    "# scale data\n",
    "x_test_scaled = scaler.transform(x_test_parts_features_unrolled)\n",
    "\n",
    "# pick importanted features\n",
    "# use only important features only RF\n",
    "X_important_test = sfm.transform(x_test_scaled)\n",
    "    \n",
    "    \n",
    "# predict \n",
    "y_test_pred = rf.predict(X_important_test)\n",
    "\n",
    "# combine preidctions of multiple parts per video into single prediction by averaging\n",
    "df = combine_parts_pred(y_pred=y_test_pred, v_idx=test_idx_parts_unrolled)\n",
    "\n",
    "display(df)\n",
    "plt.hist(df['y'], bins=25)\n",
    "plt.show()\n",
    "df.to_csv(\"submissions/rf_combined.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aml-3)",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
