{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 17:14:51) \n",
      "[GCC 7.2.0]\n",
      "scipy: 1.1.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# packages to load \n",
    "# Check the versions of libraries\n",
    "# Python version\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "print('Python: {}'.format(sys.version))\n",
    "\n",
    "import scipy\n",
    "print('scipy: {}'.format(scipy.__version__))\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.decomposition import (PCA, LatentDirichletAllocation)\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.feature_selection import f_regression\n",
    "# Importing metrics for evaluation\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor, GradientBoostingClassifier )\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "import random as rn\n",
    "from biosppy.signals import (ecg, tools)\n",
    "import pywt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import math\n",
    "from itertools import product\n",
    "# ============= CONSTS =============\n",
    "TRAIN_FILE_PATH = \"data/X_train.csv\"\n",
    "TARGET_FILE_PATH =  \"data/y_train.csv\"\n",
    "TEST_FILE_PATH = \"data/X_test.csv\"\n",
    "\n",
    "seed=42\n",
    "NUM_MAX_POINTS = 18154\n",
    "SAMPLING_RATE=300\n",
    "USE_WAVE_LETS = False\n",
    "my_cols = [\"id\"] + [\"x\" + str(i) for i in range(NUM_MAX_POINTS)]\n",
    "# ============= CONSTS =============\n",
    "\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(TRAIN_FILE_PATH, names=my_cols)[1:]\n",
    "train_data.drop(\"id\", axis=1, inplace=True)\n",
    "\n",
    "Y_train = pd.read_csv(TARGET_FILE_PATH)\n",
    "Y_train.drop(\"id\", axis=1, inplace = True)\n",
    "\n",
    "test_data =  pd.read_csv(TEST_FILE_PATH, names=my_cols)[1:]\n",
    "id_test = test_data.columns[0]\n",
    "test_data.drop(\"id\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function defintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(filename, predictions):\n",
    "    test_data =  pd.read_csv(TEST_FILE_PATH, names=my_cols)[1:]\n",
    "    test_data[\"y\"] = predictions\n",
    "    test_data[[\"id\", \"y\"]].to_csv(\"submissions/\"+filename, index= False)\n",
    "\n",
    "def get_features_from_raw_qrs(signal, sampling_rate):\n",
    "    X = list()\n",
    "    ts, filtered, rpeaks, templates_ts, templates, heart_rate_ts, heart_rate = ecg.ecg(signal, sampling_rate, show=False)\n",
    "    rpeaks = ecg.correct_rpeaks(signal=signal, rpeaks=rpeaks, sampling_rate=sampling_rate, tol=0.1)\n",
    "    \n",
    "    peaks = signal[rpeaks]\n",
    "    if len(heart_rate) < 2:\n",
    "        heart_rate = [0, 1]\n",
    "    if len(heart_rate_ts) < 2:\n",
    "        heart_rate_ts = [0, 1]\n",
    "    \n",
    "    X.append(np.mean(peaks))\n",
    "    X.append(np.min(peaks))\n",
    "    X.append(np.max(peaks))\n",
    "    X.append(np.mean(np.diff(rpeaks)))\n",
    "    X.append(np.min(np.diff(rpeaks)))\n",
    "    X.append(np.max(np.diff(rpeaks)))\n",
    "    X.append(np.mean(heart_rate))\n",
    "    X.append(np.min(heart_rate))\n",
    "    X.append(np.max(heart_rate))\n",
    "    X.append(np.mean(np.diff(heart_rate)))\n",
    "    X.append(np.min(np.diff(heart_rate)))\n",
    "    X.append(np.max(np.diff(heart_rate)))\n",
    "    X.append(np.mean(np.diff(heart_rate_ts)))\n",
    "    X.append(np.min(np.diff(heart_rate_ts)))\n",
    "    X.append(np.min(np.diff(heart_rate_ts)))\n",
    "    X.append(np.max(np.diff(heart_rate_ts)))\n",
    "    X.append(np.sum(filtered-signal))\n",
    "    \n",
    "    X += list(np.mean(templates, axis=0))\n",
    "    X += list(np.min(templates, axis=0))\n",
    "    X += list(np.max(templates, axis=0))\n",
    "    X = np.array(X)\n",
    "    \n",
    "    X[np.isnan(X)] = 0\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute features from raw signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22b8d8dc1df4bf7983e87d0bb57271e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5117), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ebea8e700c4006bf91faf6e0382dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3411), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "features = list()\n",
    "sampling_rate = float(SAMPLING_RATE)\n",
    "for id in tqdm(range(train_data.shape[0])):\n",
    "    signal = np.array(pd.to_numeric(train_data.iloc[id].dropna()))\n",
    "    features.append(get_features_from_raw_qrs(signal, sampling_rate))\n",
    "    \n",
    "    \n",
    "X = np.array(features)\n",
    "y = np.ravel(np.array(Y_train.values))\n",
    "\n",
    "features_test = list()\n",
    "for id in tqdm(range(test_data.shape[0])):\n",
    "    signal = np.array(pd.to_numeric(test_data.iloc[id].dropna()))\n",
    "    features_test.append(get_features_from_raw_qrs(signal, sampling_rate))\n",
    "    \n",
    "X_test = np.array(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model and make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fitting\n",
      "GBC done\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler() \n",
    "scaler.fit(X)\n",
    "x_train_scaled = scaler.transform(X)\n",
    "x_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Start fitting\")\n",
    "gb = GradientBoostingClassifier(random_state=seed,\n",
    "                                       n_estimators=100,\n",
    "                                       max_depth=5,\n",
    "                                       learning_rate=0.1)\n",
    "\n",
    "gb.fit(x_train_scaled, y)\n",
    "prediction = gb.predict(x_test_scaled)\n",
    "make_submission(\"final.csv\", prediction)\n",
    "print(\"GBC done\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aml-3)",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
